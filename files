import boto3
import logging
import pyspark.sql.functions as f
import sys
import time
import uuid
from builtins import min
from math import *
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.sql.window import Window
from time import sleep
import pytz
import random
import datetime
from multiprocessing.pool import ThreadPool
from src.common_utils.custom_logging import CustomLogging
from src.common_utils.read_config import GetConfig
from src.common_utils.read_write_db import ReadWriteDB
from src.streaming_apps.config.pds.pds_oracle_config import PdsConfig
from src.streaming_apps.schema.pds.all_schema import AllSchema
from src.streaming_framework.utility.common_utility.driver import Driver
from src.streaming_framework.utility.common_utility.hudi_utility import HudiUtility
from src.streaming_framework.utility.kinesis_utility.kinesis_producer import KinesisProducer
from pyspark import StorageLevel
from src.streaming_apps.stream_to_datalake.driver_methods.pds_oracle_operations import PdsDbOperations


class PdsDataLoad(ReadWriteDB, KinesisProducer):
    def __init__(self, spark_session=None):
        ReadWriteDB.__init__(self)
        KinesisProducer.__init__(self)
        if spark_session:
            self.spark_session = spark_session
        self._partitioncolumn = ''
        self._table_name = ''
        self._env = ''
        self._producer = None
        self._loadType = ''
        self._temp_database = ''
        self._deltaLoad=''
        self._secret_id=''
        self._appType=''

    # Removed perform method - not needed for delete operation

    @property
    def secret_id(self):
        return self._secret_id

    @secret_id.setter
    def secret_id(self, value):
        self._secret_id = value

    @property
    def appType(self):
        return self._appType

    @appType.setter
    def appType(self, value):
        self._appType = value

    @property
    def deltaLoad(self):
        return self._deltaLoad

    @deltaLoad.setter
    def deltaLoad(self, value):
        self._deltaLoad = value

    @property
    def temp_database(self):
        return self._temp_database

    @temp_database.setter
    def temp_database(self, value):
        self._temp_database = value

    @property
    def loadType(self):
        return self._loadType

    @loadType.setter
    def loadType(self, value):
        self._loadType = value

    @property
    def producer(self):
        return self._producer

    @producer.setter
    def producer(self, value):
        self._producer = value

    @property
    def env(self):
        return self._env

    @env.setter
    def env(self, value):
        self._env = value

    @property
    def table_name(self):
        return self._table_name

    @table_name.setter
    def table_name(self, value):
        self._table_name = value

    @property
    def partitioncolumn(self):
        return self._partitioncolumn

    @partitioncolumn.setter
    def partitioncolumn(self, value):
        self._partitioncolumn = value

    # ------------------------------------------------------------------------------------------------------------------------
    # **************************** CODE FOR PERFORMING INGESTION BELOW STAGES **************************************
    #                1. PERFORM INGESTION FROM ORACLE DB TO LANDING AND CURATED ZONES USING HUDI
    #                2. FINAL RESULT AFTER COMPLETION OF CURATED NEED TO PUBLISH IN KINESIS
    # -----------------------------------------------------------------------------------------------------------------------

    def pds_load_main(self):
        # Debug all possible loadType sources
        logging.info(f"üîç pds_load_main START: loadType='{self.loadType}', deltaLoad='{self.deltaLoad}', appType='{self.appType}', table_name='{self.table_name}'")
        
        # Initialize dependencies
        dboperations = PdsDbOperations()
        dboperations.generate_spark_context()
        dboperations.secret_id = self.secret_id
        dboperations.env = self.env
        
        # Force DELETE path - check multiple loadType sources
        if (self.loadType and self.loadType.upper() == 'DELETE') or \
           (self.deltaLoad and self.deltaLoad.upper() == 'DELETE') or \
           (hasattr(self, 'load_type_datalake_delete') and self.load_type_datalake_delete == 'DELETE'):
            
            logging.info("üî•üî•üî• DELETE OPERATION CONFIRMED - PROCESSING PLCY_CLOB ONLY üî•üî•üî•")
            
            # Strict table selection - bypass audit table completely
            table_list = ['PLCY_CLOB']
            
            def perform(table):
                logging.info(f'Thread Started for table: {table}')
                conf = PdsConfig(self.env)
                properties = conf.get_pds_conf()
                audit_table_check = properties.get(table).get('hudiTableName')
                logging.info(f'properties for table: {table}')
                
                if table == 'PLCY_CLOB':
                    logging.info(f'Calling DELETE operation for table: {table}')
                    dboperations.pdsDeleteOperation()
                else:
                    logging.warning(f"Skipping table {table}: DELETE operation only allowed for PLCY_CLOB")
            
            perform('PLCY_CLOB')
            return
        
        print(f"DEBUG: pds_load_main - self.loadType = '{self.loadType}'")
        print(f"DEBUG: pds_load_main - self.deltaLoad = '{self.deltaLoad}'")
        print(f"DEBUG: pds_load_main - self.appType = '{self.appType}'")
        table_Schema = ''
        df = DataFrame
        dboperations = PdsDbOperations()
        dboperations.generate_spark_context()
        db_name=''
        table=''
        mode = ''
        write_type = ''
        load_type=None
        source_Schema_audit=''
        time_udf = udf(lambda: int(time.time()), IntegerType())
        audit_schema = ''
        audit_table_check=''
        dboperations.secret_id = self.secret_id
        # Non-DELETE operations
        logging.error(f"Invalid or missing DELETE loadType: {self.loadType}. Only DELETE operations supported.")
        raise ValueError(f"Invalid loadType: {self.loadType}. Expected 'DELETE' for PLCY_CLOB processing.")
        
        try:
            # This code should not be reached for current DELETE-only implementation
            logging.info("pds_load_main :: Load PDS to Redshift for table " + self.table_name + "  Start")
            boto3_producer = boto3.client("kinesis", region_name='us-east-1')
            conf = PdsConfig(self.env)
            properties = conf.get_pds_conf()
            db_name = properties.get('HIVE_DATABASE_NAME')
            audit_schema = properties.get('audit_schema')
            dboperations.databaseCreation(db_name)
            # Force empty table_list to prevent processing other tables
            table_list=[]
            hudi = HudiUtility()
            env=self.env
            count=0
            # PERFORM LOADS BASED ON LOADTYPES FROM AIRFLOW
            def perform(table):
                try:

                    mode = ''
                    load_type = ''
                    write_type = ''
                    df = DataFrame
                    count = 0
                    logging.info('Thread Started for table: '+table)
                    audit_table = ''
                    if table == 'PLCY_CLOB_INF':
                        audit_table = 'pds_inforce_job_audit'
                        drop_query = """
                                    drop table if exists {0}.afp_poll_plcy_clob_inf_tmp_nrt_load;
                                    """
                        try:


                            drop_tbl_df = self.spark_session.sql(
                                drop_query.format(properties.get('HIVE_DATABASE_NAME')))
                            drop_tbl_df.count()
                            logging.info(f"Drop query executed successfully for {table}")
                        except Exception as drop_exception:
                            logging.error(f"error during drop table queryfor {table}")
                            raise drop_exception


                    elif table == 'PLCY_CLOB':
                        audit_table = 'pds_clob_job_audit'
                        drop_query = """
                            drop table if exists {0}.afp_poll_plcy_clob_tmp_nrt_load;
                            """
                        try:

                            drop_tbl_df = self.spark_session.sql(
                                drop_query.format(properties.get('HIVE_DATABASE_NAME')))
                            drop_tbl_df.count()
                            logging.info(f"Drop query executed successfully for {table}")
                        except Exception as drop_exception:
                            logging.error(f"error during drop table queryfor {table}")
                            raise drop_exception
                    else:
                        audit_table = 'job_audit'

                    if table not in table_list:
                        # table=table

                        hudi.table_name = properties.get(table).get('hudiTableName')
                        source_Schema = properties.get(table).get('src_Schema')
                        hudi.record_key = properties.get(table).get('hudiTableKeyField')
                        audit_table_check= properties.get(table).get('hudiTableName')
                        logging.info('properties for table: ' + table)
                        
                        # Only call delete for AFP_POLL_PLCY_CLOB table
                        if table == 'AFP_POLL.PLCY_CLOB':
                            dboperations.pdsDeleteOperation()
                        
                        partitionColumn =''
                        source_Schema_audit = source_Schema

                        #deltaLoad = properties.get(table).get('deltaLoad')
                        try:
                            print(f"DEBUG: Checking loadType = '{self.loadType}'")
                            print(f"DEBUG: loadType.upper() = '{self.loadType.upper()}'")
                            
                            # Only DELETE operation supported
                            print("DEBUG: Going to DELETE path")
                            logging.info(f"Running DELETE load for table: {table}")
                            load_type=self.loadType
                            mode = 'append'
                            write_type = 'upsert'
                            print('DEBUG: entered into delete function')
                            logging.info('print entered into delete function')
                            
                            # Skip fullload for DELETE - call delete operation directly
                            print("DEBUG: About to call pdsDeleteOperation directly")
                            dboperations.env = env
                            dboperations.delete_records_from_datalake()
                            print("DEBUG: Called delete_records_from_datalake successfully")
                            
                            # Create empty dataframe to satisfy downstream logic
                            from pyspark.sql.types import StructType
                            df = self.spark_session.createDataFrame([], StructType([]))
                            count = 0
                        except Exception as load_exception:
                            logging.error(f"Error during {self.loadType.upper()}")
                            raise load_exception


                        # READING HUDI PROPERTIES FROM PDS_ORACLE_CONFIG
                        if df.rdd.isEmpty() :
                            logging.info(f"{table} -> No Data ")
                        else:
                            logging.info(f"Data fetched successfully for table {table}. Number of records: {count}")
                            logging.info(f"Schema of DataFrame: {df.printSchema()}")
                            #logging.info(f"Sample data from DataFrame: {df.show(5)}")
                            try:
                                hudi.hive_database = properties.get('HIVE_DATABASE_NAME')
                                logging.info(f"Hudi hive database: {hudi.hive_database}")
                                hudi.write_table_type = properties.get(table).get('hudiTableType')
                                logging.info(f"Hudi write table type: {hudi.write_table_type}")
                                hudi.hudi_s3_location = properties.get(table).get('s3HudiLocation')
                                logging.info(f"Hudi S3 location: {hudi.hudi_s3_location}")
                                hudi.landing_zone = properties.get(table).get('landingzone')
                                logging.info(f"Hudi landing zone: {hudi.landing_zone}")
                                hudi.combine_key = properties.get(table).get('hudiTablePartitionField')
                                logging.info(f"Hudi combine key: {hudi.combine_key}")
                                hudi.partition_field = properties.get(table).get('hudiTablePartitionField')
                                logging.info(f"Hudi partition field: {hudi.partition_field}")
                                hudi.record_key = properties.get(table).get('hudiTableKeyField')
                                logging.info(f"Hudi record key: {hudi.record_key}")
                                hudi.key_list = properties.get(table).get('producer_keys')
                                logging.info(f"Hudi producer keys: {hudi.key_list}")
                                producer_keys = properties.get(table).get('producer_keys')
                            except Exception as e:
                                logging.error(f"Error retrieving Hudi properties for table {table}: {str(e)}")
                                raise e

                            logging.info("Seetting up Spark session")
                            try:
                                hudi.spark_session = self.spark_session
                                hudi.boto3client = boto3_producer
                                hudi.producer = KinesisProducer()
                                logging.info("Successfully up Spark session")
                            except Exception as e:
                                logging.error(f"Error setting up spark kinesis{str(e)}")
                                raise e
                            hudi.enable_publish = bool(properties.get(table).get('dynamo_table_list'))
                            hudi.producer.stream_name = properties.get("PRODUCER").get("stream_name")



                            s3_checkpoint_location = properties.get(table).get('s3HudiCheckpointLocation')
                            drop_na_col = properties.get(table).get('naDropField')
                            logging.info(f"Dropping NA values for columns specified in the configuration for table {table}")
                            for col in drop_na_col:
                                if col in df.columns:
                                    logging.info(f"Dropping NA values in column: {col}")
                                    df = df.na.drop(subset=col)
                            logging.info(f"Number of records after dropping NAs for table {table}: {df.count()}")

                            #CHECKING DATATYPES IF ANY TYPE NOT SUPPORTED , CAST IT

                            logging.info(f"Starting data type check for table {table}")
                            df1= dboperations.data_type_check(df,hudi.table_name)
                            logging.info(f"Data type check completed for table {table}")
                            logging.info(f"Adding batch timestamp column for table {table}")
                            final_df= df1.withColumn("_batch_timestamp", time_udf())
                            logging.info(f"Batch timestamp added for table {table}")


                    # WRITING TO LANDING , CURATED AND PUBLISHING RECORDS TO KINESIS
                    logging.info(' LOAD STARTED WRITING INTO DATALAKE : ' + table)
                    #logging.info(mode)
                    #logging.info(write_type)
                    #hudi.load_to_landing_zone_full_load(final_df, mode)
                    try:
                        logging.info(f"About to start writing data to curated zone for table  {table}")
                        hudi.load_to_curated_zone_full_load(final_df, mode, write_type)
                        logging.info(f"Finished writing data to cutated zone for table  {table}")
                    except Exception as write_exception:
                        logging.error(f"error curated zone nfor ntable {table}:{write_exception}")
                        raise write_exception



                    if self.loadType.upper() == 'DELETE':
                        if table == 'PLCY_CLOB':
                            landing_s3_location = "s3://ta-individual-datalake-ida-{0}-landing/pds-ida/afp_poll_plcy_clob_tmp_nrt/".format(
                                env)
                            landing_table = "afp_poll_plcy_clob_tmp_nrt_load"
                            final_df.repartition(1).write.mode("OVERWRITE").option("path",
                                                                                   landing_s3_location).saveAsTable(
                                "{0}.{1}".format(properties.get('HIVE_DATABASE_NAME'), landing_table))
                        elif table == 'PLCY_CLOB_INF':
                            landing_s3_location = "s3://ta-individual-datalake-ida-{0}-landing/pds-ida/afp_poll_plcy_clob_inf_tmp_nrt/".format(
                                env)
                            landing_table = "afp_poll_plcy_clob_inf_tmp_nrt_load"
                            final_df.repartition(1).write.mode("OVERWRITE").option("path",
                                                                                   landing_s3_location).saveAsTable(
                                "{0}.{1}".format(properties.get('HIVE_DATABASE_NAME'), landing_table))
                        else:
                            final_df= final_df.select(*producer_keys)
                            hudi.publish_to_kinesis_streaming_full_load(final_df)
                    logging.info(' LOAD COMPLETED WRITING INTO DATALAKE : ' + table)
                    # CREATING AUDITING INFORMATION INCLUDES TABLENAME AND TIMESTAMP FOR REFERNCE WHICH TABLES ARE LOADED
                    # IF ANY TABLES ARE NOT LOADED CODE WILL PICKUP TABLES WHICH ARE NOT IN THIS AUDIT
                    logging.info('PERFORMING AUDIT CHECK : ' + table)
                    source_date = dboperations.source_date_time
                    print(source_date)
                    print(count)
                    if source_date is not None:
                        dboperations.auditInsert(audit_schema, final_df, audit_table, self.env, str(source_date),
                                                 str(count), 'completed', '', load_type, source_Schema_audit, table,
                                                 audit_table_check)
                    else:
                        print('{0} table are processed'.format(table))

                except Exception as perform_exception:
                    logging.error(f"error in perform function for ntable {table}")
                    raise perform_exception


        # print(str(datetime.datetime.now().strftime("%I:%M %P")))
            # Strict table selection for DELETE operations
            if self.loadType and self.loadType.upper() == 'DELETE':
                logging.info("üî•üî•üî• DEPLOYMENT VERIFICATION: DELETE CODE UPDATED - VERSION 2025-10-15 üî•üî•üî•")
                logging.info("Processing DELETE operation for PLCY_CLOB table only")
                table_list = ['PLCY_CLOB']  # Strict table selection
                dboperations.env = self.env
                
                def perform(table):
                    logging.info(f'Thread Started for table: {table}')
                    audit_table_check = properties.get(table).get('hudiTableName')
                    logging.info(f'properties for table: {table}')
                    
                    # Only call delete for PLCY_CLOB table
                    if table == 'PLCY_CLOB':
                        dboperations.pdsDeleteOperation()
                
                perform('PLCY_CLOB')
                return
            else:
                logging.error(f"Invalid or missing loadType: {self.loadType}. Expected 'DELETE' for PLCY_CLOB processing.")
                raise ValueError(f"Invalid or missing loadType: {self.loadType}. Expected 'DELETE' for PLCY_CLOB processing.")


        except Exception as pds_load_exception:
            logging.error(f"pds_load_main :: Failed to load PDS to Redshift for table {self.table_name}: {pds_load_exception}")
            # Option 2 - Force container refresh note
            logging.error("üîÑ If this error persists, use timestamp-based container tag: :emergency-fix-" + str(int(time.time())))
            raise pds_load_exception


pds_dataload.py

+++

####################################### DEVELOPMENT LOG ################################################################
# DESCRIPTION - Streaming main is used for calling the drivers for call streaming task
# USAGE 1: This Job Is used to call all streaming task
# 08/26/2025 - Ravikant Petkar - TLP-7685- added new policy_40_main_eft_nrt task and notification index
########################################################################################################################
import argparse
import logging
from src.streaming_apps.stream_to_datalake.driver_methods.horizon_driver import HorizonDriver
from src.streaming_apps.stream_to_datalake.driver_methods.pplus_driver import PplusDriver
from src.streaming_apps.stream_to_datalake.driver_methods.pds_driver import PdsDriver
from src.common_utils.custom_logging import CustomLogging
from src.streaming_apps.stream_to_datalake.driver_methods.phd_driver import PhdDriver
from src.streaming_apps.stream_to_datalake.driver_methods.dssp7_driver import Dssp7Driver
from src.streaming_apps.stream_to_datalake.driver_methods.mras_driver import MrasDriver
from builtins import str, Exception
import sys
from src.streaming_apps.stream_to_datalake.driver_methods.pds_dataload import PdsDataLoad
from src.streaming_apps.stream_to_datalake.driver_methods.pplus_dataload import PplusDataLoad
from src.streaming_apps.stream_to_datalake.driver_methods.hzn_dataload import HznDataLoad
from src.streaming_apps.stream_to_datalake.driver_methods.phd_batch_dataload import PhdDataLoadCustom

from src.streaming_apps.stream_to_datalake.driver_methods.awd_dataload import AwdDataLoad
from src.streaming_apps.stream_to_dynamodb.driver_function.pds_policy_customer_driver import PDSCustDriver
from src.streaming_apps.stream_to_dynamodb.driver_function.pds_policy_requirement_driver import PDSReqDriver
from src.streaming_apps.stream_to_dynamodb.driver_function.pds_policy_main_pending_driver import PDSMainPendingDriver
from src.streaming_apps.stream_to_dynamodb.driver_function.pds_policy_status_tracker_driver import AgentPolicyStatusTracker
from src.streaming_apps.stream_to_dynamodb.driver_function.pds_policy_inforce_60_benficiary_nrt_model_driver import AgentPolicyInforce60Benficiary
from src.streaming_apps.stream_to_dynamodb.driver_function.policy_40_main_nrt_driver import Policy40MainNrtDriver
from src.streaming_apps.stream_to_dynamodb.driver_function.policy_45_customer_nrt_driver import AgentPolicyInforce45Customer
from src.streaming_apps.stream_to_dynamodb.driver_function.policy_80_riders_nrt_driver import Policy80nrtDriver
from src.streaming_apps.stream_to_dynamodb.driver_function.policy_85_financial_nrt_driver import Policy85NRTDriver
from src.streaming_apps.stream_to_dynamodb.driver_function.policy_90_activity_nrt_driver import Policy90NRTDriver
from src.streaming_apps.stream_to_dynamodb.driver_function.policy_70_driver import Policy70Driver

class SdkClientException(Exception):
    """Raised when SdkClientException occurs"""
    pass


class ConcurrentModificationException(Exception):
    """Raised when ConcurrentModificationException occurs"""
    pass


class ResourceNotFoundException(Exception):
    """Raised when ResourceNotFoundException occurs"""
    pass


class UnknownHostException(Exception):
    """Raised when UnknownHostException occurs"""
    pass


class StreamingQueryException(Exception):
    """Raised when StreamingQueryException occurs"""
    pass


if __name__ == "__main__":
    CustomLogging.init()
    parser = argparse.ArgumentParser()
    parser.add_argument('--model', help="Provide model to run task. Required Field")
    parser.add_argument('--env', required=False,
                        help="Provide Provide secret ID. Required Field, possible values ['dev', 'tst', 'mdl', 'prd'] ")
    parser.add_argument('--load_type', required=False,
                        help="Provide Provide secret ID. Required Field, possible values ['FULL','DELTA','DELETE'] ")
    parser.add_argument('--es_vpc', required=False, help="Needed for OpenSearch Job.")
    parser.add_argument('--opensearch_index_name', required=False, help="Needed for BoB Jobs.")
    parser.add_argument('--opensearch_index_notification_name', required=False, help="Needed for BoB Jobs.")
    args = parser.parse_args()
    attempts = 1
    load_type = args.load_type
    print(f"üîç VALIDATION: args.model = '{args.model}'")
    print(f"üîç VALIDATION: args.model.upper() = '{args.model.upper()}'")
    print(f"üîç VALIDATION: load_type = '{load_type}'")
    logging.info(f"üîç VALIDATION: args.model = '{args.model}', load_type = '{load_type}'")
    print(args.model)
    print(load_type)
    job_run_flag = True
    # Handle PDS_CUSTOM_DELETE operation
    if args.model.upper() == "PDS_CUSTOM_DELETE":
        print("üö® EMERGENCY DELETE ENTRY POINT ACTIVATED üö®")
        print(f"Model: {args.model}")
        print(f"Load Type: {load_type}")
        logging.info("üö® EMERGENCY DELETE ENTRY POINT ACTIVATED üö®")
        logging.info(f"üîç DEBUG: Model={args.model}, Load_Type={load_type}, Env={args.env}")
        
        try:
            print("üîÑ ATTEMPTING IMPORT OF PdsOracleDeleteOperations")
            logging.info("üîÑ ATTEMPTING IMPORT OF PdsOracleDeleteOperations")
            from src.streaming_apps.stream_to_datalake.driver_methods.pds_oracle_operations import PdsOracleDeleteOperations
            from pyspark.sql import SparkSession
            print("‚úÖ IMPORT SUCCESSFUL")
            logging.info("‚úÖ IMPORT SUCCESSFUL")
            
            print("üîÑ CREATING SPARK SESSION")
            logging.info("üîÑ CREATING SPARK SESSION")
            spark = SparkSession.builder \
                .appName("PDS_DELETE_OPERATION") \
                .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
                .config("spark.sql.hive.convertMetastoreParquet", "false") \
                .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.hudi.catalog.HoodieCatalog") \
                .config("spark.sql.extensions", "org.apache.spark.sql.hudi.HoodieSparkSessionExtension") \
                .enableHiveSupport() \
                .getOrCreate()
            print("‚úÖ SPARK SESSION CREATED")
            logging.info("‚úÖ SPARK SESSION CREATED")
            
            print("üîÑ INITIALIZING DELETE OPERATIONS")
            logging.info("üîÑ INITIALIZING DELETE OPERATIONS")
            dboperations = PdsOracleDeleteOperations(spark, args.env.lower())
            print("‚úÖ DELETE OPERATIONS INITIALIZED")
            logging.info("‚úÖ DELETE OPERATIONS INITIALIZED")
            
            print("üî• CALLING DELETE OPERATION FOR PLCY_CLOB ONLY üî•")
            logging.info("üî• CALLING DELETE OPERATION FOR PLCY_CLOB ONLY üî•")
            dboperations.delete_records_from_datalake()
            print("‚úÖ DELETE COMPLETED SUCCESSFULLY ‚úÖ")
            logging.info("‚úÖ DELETE COMPLETED SUCCESSFULLY ‚úÖ")
            
        except Exception as e:
            print(f"‚ùå PDS_CUSTOM_DELETE FAILED: {str(e)}")
            logging.error(f"‚ùå PDS_CUSTOM_DELETE FAILED: {str(e)}")
            import traceback
            print(f"‚ùå TRACEBACK: {traceback.format_exc()}")
            logging.error(f"‚ùå TRACEBACK: {traceback.format_exc()}")
            raise e
            
        sys.exit(0)
    
    while job_run_flag:
        try:
            if args.model.upper() == "HORIZON":
                logging.info("Horizon kinesis to lake job starting.......")
                hzn = HorizonDriver(args.env.lower(), args.model.upper())
                hzn.driver_function()
            if args.model.upper() == "MRAS":
                logging.info("Mras kinesis to lake job starting.......")
                mras = MrasDriver(args.env.lower(), args.model.upper())
                mras.driver_function()
            if args.model.upper() == "PPLUS":
                pplus = PplusDriver(args.env.lower(), args.model.upper())
                pplus.driver_function()
            if args.model.upper() == "PDS":
                pds = PdsDriver(args.env.lower(), args.model.upper())
                pds.driver_function()
            if args.model.upper() == "PHD":
                phd = PhdDriver(args.env.lower(), args.model.upper())
                phd.driver_function()
            if args.model.upper() == "DSSP7":
                dssp7 = Dssp7Driver(args.env.lower(), args.model.upper())
                dssp7.driver_function()
            if args.model.upper() == "HZN_ORACLE":
                logging.info("Entering HZN data load main")
                print("Entering HZN data load main")
                hzn_main = HznDataLoad()
                hzn_main.secret_id = 'ta-individual-apps-{0}-common'.format(args.env.lower())
                hzn_main.env = args.env
                hzn_main.deltaLoad = 'DELTA'
                hzn_main.loadType = 'DELTA'
                print(hzn_main.loadType)
                hzn_main.hzn_load_main()
                attempts = 0

            if args.model.upper() == "PDS_DELETE_DIRECT":
                logging.info("üî•üî•üî• DIRECT DELETE OPERATION - BYPASSING OLD CODE üî•üî•üî•")
                print("üî•üî•üî• DIRECT DELETE OPERATION - BYPASSING OLD CODE üî•üî•üî•")
                job_run_flag = False
                from src.streaming_apps.stream_to_datalake.driver_methods.pds_oracle_operations import PdsOracleDeleteOperations
                from pyspark.sql import SparkSession
                spark = SparkSession.builder.appName("PDS_DELETE_DIRECT").enableHiveSupport().getOrCreate()
                dboperations = PdsOracleDeleteOperations(spark, args.env.lower())
                dboperations.delete_records_from_datalake()
                attempts = 0

            if args.model.upper() == "PDS_ORACLE":
                # EMERGENCY REDIRECT: If DELETE operation, use new delete handler
                if load_type and load_type.upper() == "DELETE":
                    logging.info("üîÑ REDIRECTING PDS_ORACLE DELETE TO NEW HANDLER")
                    print("üîÑ REDIRECTING PDS_ORACLE DELETE TO NEW HANDLER")
                    from src.streaming_apps.stream_to_datalake.driver_methods.pds_oracle_operations import PdsOracleDeleteOperations
                    from pyspark.sql import SparkSession
                    spark = SparkSession.builder.appName("PDS_ORACLE_DELETE_REDIRECT").enableHiveSupport().getOrCreate()
                    dboperations = PdsOracleDeleteOperations(spark, args.env.lower())
                    dboperations.delete_records_from_datalake()
                    attempts = 0
                else:
                    logging.info("Entering PDS data load main")
                    print("Entering PDS data load main")
                    pds_main = PdsDataLoad()
                    pds_main.deltaLoad = 'DELTA'
                    pds_main.secret_id = 'ta-individual-apps-{0}-common'.format(args.env.lower())
                    pds_main.env = args.env
                    pds_main.loadType = 'DELTA'
                    print(pds_main.loadType)
                    pds_main.pds_load_main()
                    attempts = 0

            if args.model == "policy_90_activity_nrt":
                policy_90_driver_obj = Policy90NRTDriver(args.env, args.model)
                logging.info("Calling Policy 90 driver function")
                job_run_flag = False
                policy_90_driver_obj.policy_90_activity_nrt_driver(args.es_vpc,args.opensearch_index_notification_name)

            if args.model == "policy_85_financial_nrt":
                policy_85_driver_obj = Policy85NRTDriver(args.env, args.model)
                logging.info("Calling Policy 85 driver function")
                job_run_flag = False
                policy_85_driver_obj.policy_85_financial_nrt_driver()

            if args.model == "policy_80_riders_nrt":
                policy_80_driver_obj = Policy80nrtDriver(args.env, args.model)
                logging.info("Calling Policy 80 driver function for streaming")
                job_run_flag = False
                policy_80_driver_obj.policy_80_riders_nrt_driver()

            if args.model == "policy_inforce_45_customer_nrt":
                pds_policy_inf_obj = AgentPolicyInforce45Customer(args.env, args.model)
                logging.info("Calling PDS policy_inforce_45_customer nrt driver function")
                job_run_flag = False
                pds_policy_inf_obj.agent_policy_inforce_45_customer_driver_function()

            if args.model == "policy_40_main_nrt":
                policy_40_main_driver_obj = Policy40MainNrtDriver(args.env, args.model)
                logging.info("Calling Policy 40 main function for streaming")
                job_run_flag = False
                policy_40_main_driver_obj.policy_40_main_nrt_driver(args.es_vpc,args.opensearch_index_name)

            if args.model == "policy_40_main_eft_nrt":
                policy_40_main_driver_obj = Policy40MainNrtDriver(args.env, args.model)
                logging.info("Calling Policy 40 main and EFT function for streaming")
                job_run_flag = False
                policy_40_main_driver_obj.policy_40_main_EFT_nrt_driver(args.es_vpc,args.opensearch_index_notification_name)

            if args.model == "policy_inforce_60_benficiary_nrt":
                pds_policy_inf_obj = AgentPolicyInforce60Benficiary(args.env, args.model)
                logging.info("Calling PDS policy_inforce_60_benficiary_tracker driver function")
                job_run_flag = False
                pds_policy_inf_obj.agent_policy_inforce_60_benficiary_driver_function()

            if args.model == "pds_policy_customer":
                pds_policy_customer_driver_obj = PDSCustDriver(args.env, args.model)
                logging.info("Calling PDS Policy Customer driver function")
                job_run_flag = False
                pds_policy_customer_driver_obj.pds_customer_driver_function()

            if args.model == "pds_policy_requirement":
                pds_policy_requirement_driver_obj = PDSReqDriver(args.env, args.model)
                logging.info("Calling PDS Policy Requirement driver function")
                job_run_flag = False
                pds_policy_requirement_driver_obj.pds_requirement_driver_function()

            if args.model == "pds_policy_main_pending":
                pds_policy_main_pending_driver_obj = PDSMainPendingDriver(args.env, args.model)
                logging.info("Calling PDS Policy Main Pending driver function")
                job_run_flag = False
                pds_policy_main_pending_driver_obj.pds_main_pending_driver_function(args.es_vpc,args.opensearch_index_name)

            if args.model == "policy_agent_status_tracker":
                pds_policy_agent_status_tracker_driver_obj = AgentPolicyStatusTracker(args.env, args.model)
                logging.info("Calling  PDS Policy Agent Status Tracker driver function")
                job_run_flag = False
                pds_policy_agent_status_tracker_driver_obj.agent_policy_status_tracker_driver_function()

            if args.model.upper() == "AWD":
                job_run_flag = False
                logging.info("Entering AWD data load main. LOAD TYPE : {0}".format(load_type))
                awd_main = AwdDataLoad()
                awd_main.secret_id = 'ta-individual-apps-{0}-common'.format(args.env.lower())
                awd_main.env = args.env
                awd_main.awd_load_main(load_type)

            if args.model == "policy_70":
                job_run_flag = False
                policy_70_driver_obj = Policy70Driver(args.env, args.model)
                logging.info("Calling Policy 70 driver function")
                policy_70_driver_obj.policy_70_driver_function()

            if args.model.upper() == "PDS_ORACLE_FULL_LOAD":
                logging.info("Entering PDS data load main")
                print("Entering PDS data load main")
                job_run_flag = False
                pds_main = PdsDataLoad()
                pds_main.secret_id = 'ta-individual-apps-{0}-common'.format(args.env.lower())
                pds_main.env = args.env
                pds_main.loadType = 'FULL'
                print(pds_main.loadType)
                pds_main.pds_load_main()

            if args.model.upper() == "PPLUS_DB2_LOAD":
                logging.info("Entering PPLUS data load main")
                print("Entering PPLUS data load main")
                job_run_flag = False
                pplus_main = PplusDataLoad()
                pplus_main.secret_id = 'ta-individual-apps-{0}-common'.format(args.env.lower())
                pplus_main.env = args.env
                pplus_main.loadType = load_type
                print(pplus_main.loadType)
                pplus_main.pplus_load_main()

            if args.model.upper() == "PPLUS_DB2_CUSTOM":
                logging.info("Entering PPLUS data load main")
                print("Entering PPLUS data load main")
                pplus_main = PplusDataLoad()
                pplus_main.secret_id = 'ta-individual-apps-{0}-common'.format(args.env.lower())
                pplus_main.env = args.env
                pplus_main.loadType = 'DELTA'
                pplus_main.deltaLoad = 'DELTA'
                print(pplus_main.loadType)
                pplus_main.pplus_load_main()
                attempts = 0

            if args.model.upper() == "PDS_ORACLE_CLOB":
                logging.info("Entering PDS data load main")
                print("Entering PDS data load main")
                job_run_flag = False
                pds_main = PdsDataLoad()
                pds_main.deltaLoad = 'DELTA'
                pds_main.secret_id = 'ta-individual-apps-{0}-common'.format(args.env.lower())
                pds_main.env = args.env
                pds_main.appType = args.model.upper()
                pds_main.loadType = load_type
                print(pds_main.loadType)
                pds_main.pds_load_main()
                attempts = 0

            if args.model.upper() == "PDS_ORACLE_INFORCE":
                logging.info("Entering PDS data load main")
                print("Entering PDS data load main")
                job_run_flag = False
                pds_main = PdsDataLoad()
                pds_main.deltaLoad = 'DELTA'
                pds_main.secret_id = 'ta-individual-apps-{0}-common'.format(args.env.lower())
                pds_main.env = args.env
                pds_main.appType = args.model.upper()
                pds_main.loadType = load_type
                print(pds_main.loadType)
                pds_main.pds_load_main()
                attempts = 0

            if args.model.upper() == "PDS_ORACLE_CLOB_DELETE":
                logging.info("Entering PDS CLOB delete operation")
                job_run_flag = False
                pds_main = PdsDataLoad()
                pds_main.secret_id = 'ta-individual-apps-{0}-common'.format(args.env.lower())
                pds_main.env = args.env
                pds_main.appType = 'PDS_ORACLE_CLOB'
                pds_main.loadType = 'DELETE'
                pds_main.deltaLoad = 'DELETE'
                pds_main.table_name = 'PLCY_CLOB'
                pds_main.pds_load_main()
                logging.info("PDS CLOB delete operation completed")
                attempts = 0




            if args.model.upper() == "HZN_ORACLE_LOAD":
                logging.info("Entering HZN data load main")
                print("Entering HZN data load main")
                job_run_flag = False
                hzn_main = HznDataLoad()
                hzn_main.secret_id = 'ta-individual-apps-{0}-common'.format(args.env.lower())
                hzn_main.env = args.env
                hzn_main.loadType = load_type
                print(hzn_main.loadType)
                hzn_main.hzn_load_main()
            
            if args.model.upper() == "PHD_CUSTOM_LOAD":
                logging.info("Entering PHD data load main")
                print("Entering PHD data load main")
                if load_type.upper() == 'DELTA':
                    job_run_flag = True
                else:
                    job_run_flag = False
                phd_main = PhdDataLoadCustom()
                phd_main.secret_id = 'ta-individual-apps-{0}-common'.format(args.env.lower())
                phd_main.env = args.env
                phd_main.loadType = load_type
                print(phd_main.loadType)
                phd_main.phd_load_main()
                attempts = 0
            if attempts == 20:
                print('maximum number of attempts completed')
                sys.exit()
            attempts += 1
        except SdkClientException:
            print("SdkClientException OCCURED... Rtrying the job")
            logging.info("SdkClientException OCCURED... Rtrying the job")
        except ConcurrentModificationException:
            print("ConcurrentModificationException OCCURED... Rtrying the job")
            logging.info("ConcurrentModificationException OCCURED... Rtrying the job")
        except ResourceNotFoundException:
            print("ResourceNotFoundException OCCURED... Rtrying the job")
            logging.info("ResourceNotFoundException OCCURED... Rtrying the job")
        except UnknownHostException:
            print("UnknownHostException OCCURED... Rtrying the job")
            logging.info("UnknownHostException OCCURED... Rtrying the job")
        except StreamingQueryException:
            print("StreamingQueryException OCCURED... Rtrying the job")
            logging.info("StreamingQueryException OCCURED... Rtrying the job")
        except Exception as e:
            print("StreamingQueryException OCCURED... Rtrying the job")
            logging.info("StreamingQueryException OCCURED... Rtrying the job")
            print(e)
            logging.info(e)
        finally:
            if attempts == 20:
                raise Exception
        try :    
            if args.model.upper() == "HZN_ORACLE_LOAD_DOWN_DEPENDENT":
                logging.info("Entering HZN data load main")
                print("Entering HZN data load main")
                job_run_flag = False
                hzn_main = HznDataLoad()
                hzn_main.secret_id = 'ta-individual-apps-{0}-common'.format(args.env.lower())
                hzn_main.env = args.env
                hzn_main.loadType = load_type
                print(hzn_main.loadType)
                hzn_main.hzn_load_main(args.model.upper())

            if args.model.upper() == "HZN_ORACLE_LOAD_DOWN_NON_DEPENDENT":
                logging.info("Entering HZN data load main")
                print("Entering HZN data load main")
                job_run_flag = False
                hzn_main = HznDataLoad()
                hzn_main.secret_id = 'ta-individual-apps-{0}-common'.format(args.env.lower())
                hzn_main.env = args.env
                hzn_main.loadType = load_type
                print(hzn_main.loadType)
                hzn_main.hzn_load_main(args.model.upper())
            if args.model.upper() == "PPLUS_DB2_CUSTOM_DOWN_NON_DEPENDENT":
                logging.info("Entering PPLUS data load main")
                print("Entering PPLUS data load main")
                job_run_flag = False
                pplus_main = PplusDataLoad()
                pplus_main.secret_id = 'ta-individual-apps-{0}-common'.format(args.env.lower())
                pplus_main.env = args.env
                pplus_main.loadType = load_type
                print(pplus_main.loadType)
                pplus_main.pplus_load_main(args.model.upper())

            if args.model.upper() == "PPLUS_DB2_CUSTOM_DOWN_DEPENDENT":
                logging.info("Entering PPLUS data load main")
                print("Entering PPLUS data load main")
                job_run_flag = False
                pplus_main = PplusDataLoad()
                pplus_main.secret_id = 'ta-individual-apps-{0}-common'.format(args.env.lower())
                pplus_main.env = args.env
                pplus_main.loadType = load_type
                print(pplus_main.loadType)
                pplus_main.pplus_load_main(args.model.upper())
        except:
            raise Exception


streaming_main.py

++++++++

import logging
from src.common_utils.custom_logging import CustomLogging
from src.common_utils.get_spark_context import GetSparkContext
from src.common_utils.server_conn import GetServerConn
from pyspark.sql import DataFrame
import psycopg2


class ReadWriteDB(GetSparkContext, GetServerConn):

    def __init__(self):
        GetSparkContext.__init__(self)
        GetServerConn.__init__(self)
        self.generate_spark_context()
        self.read_df = DataFrame
        self._schema_name = ''
        self._table_name = ''
        self._s3_temp_path = ''
        self._source_schema_name = ''
        self._source = ''
        self._read_query = ''
        self._numpartitions = ''
        self._fetchsize = ''
        self._batchsize = ''
        self._partitioncolumn = ''
        self._lowerbound = ''
        self._upperbound = ''
        self.num_partitions = ''
        self._type = ''
        self.read_count_query = ''
        self._swap_db_ = ''
        self._partitioncolumn1 = ''
        self._count_val = 0

    @property
    def swap_db(self):
        return self._swap_db_

    @swap_db.setter
    def swap_db(self, value):
        self._swap_db_ = value

    @property
    def type(self):
        return self._type

    @type.setter
    def type(self, value):
        self._type = value

    @property
    def num_partitions(self):
        return self._num_partitions

    @num_partitions.setter
    def num_partitions(self, value):
        self._num_partitions = value

    @property
    def numpartitions(self):
        return self._numpartitions

    @numpartitions.setter
    def numpartitions(self, value):
        self._numpartitions = value

    @property
    def fetchsize(self):
        return self._fetchsize

    @fetchsize.setter
    def fetchsize(self, value):
        self._fetchsize = value

    @property
    def batchsize(self):
        return self._batchsize

    @batchsize.setter
    def batchsize(self, value):
        self._batchsize = value

    @property
    def partitioncolumn(self):
        return self._partitioncolumn

    @partitioncolumn.setter
    def partitioncolumn(self, value):
        self._partitioncolumn = value

    @property
    def partitioncolumn1(self):
        return self._partitioncolumn1

    @partitioncolumn1.setter
    def partitioncolumn1(self, value):
        self._partitioncolumn1 = value

    @property
    def lowerbound(self):
        return self._lowerbound

    @lowerbound.setter
    def lowerbound(self, value):
        self._lowerbound = value

    @property
    def upperbound(self):
        return self._upperbound

    @upperbound.setter
    def upperbound(self, value):
        self._upperbound = value

    @property
    def read_query(self):
        return self._read_query

    @read_query.setter
    def read_query(self, value):
        self._read_query = value

    @property
    def read_count_query(self):
        return self._read_count_query

    @read_count_query.setter
    def read_count_query(self, value):
        self._read_count_query = value

    @property
    def source(self):
        return self._source

    @source.setter
    def source(self, value):
        self._source = value

    @property
    def read_df(self):
        return self._read_df

    @read_df.setter
    def read_df(self, value):
        self._read_df = value

    @property
    def s3_temp_dir(self):
        return 's3a://' + self.s3_temp_path + '/' + self.table_name + '_temps'

    @property
    def read_dbtable(self):
        if self.source == 'PDS' or self.source == "INF" or self.source == "SDM":
            return self.source_schema_name + '.' + self.table_name
        elif self.source == 'ARL':
            return self.read_query
        else:
            return self.source_schema_name + '.' + self.table_name

    @property
    def write_dbtable(self):
        if self.source == 'SDM':
            return self.schema_name + '.' + self.source.lower() + '_' + self.table_name.lower()
        elif self.source == 'ARL':
            return self.schema_name + '.' + self.source.lower() + '_' + self.table_name
        elif self.source == 'PDS':
            return self.schema_name + '.' + self.source.lower() + '_' + self.table_name
        elif self.source == 'INF':
            return self.schema_name + '.' + self.source.lower() + '_' + self.table_name

    @property
    def pg_con_prop(self):
        return self.spark_postgres()

    @property
    def sql_con_prop(self):
        if self.source == 'SDM':
            return self.spark_sqlserver_sdm()
        elif self.source == 'INF':
            return self.spark_sqlserver_infomart()
        elif self.source == 'AWD':
            return self.spark_sqlserver_awd()
        elif self.source == 'AWD_VW':
            return self.spark_sqlserver_awd_vw()

    @property
    def sql_sdm_ctrl(self):
        if self.source == 'SDM':
            return self.spark_sqlserver_sdm_control()

    @property
    def redshift_con_prop(self):
        return self.spark_redshift()

    @property
    def oracle_con_prop(self):
        return self.spark_oracle()

    @property
    def hzn_oracle_con_prop(self):
        return self.spark_oracle_hzn()

    @property
    def pplus_db2_con_prop(self):
        return self.spark_pplus_db2()

    @property
    def phd_oracle_con_prop(self):
        return self.spark_oracle_phd()

    @property
    def schema_name(self):
        return self._schema_name

    @schema_name.setter
    def schema_name(self, value):
        self._schema_name = value

    @property
    def source_schema_name(self):
        return self._source_schema_name

    @source_schema_name.setter
    def source_schema_name(self, value):
        self._source_schema_name = value

    @property
    def table_name(self):
        return self._table_name

    @table_name.setter
    def table_name(self, value):
        self._table_name = value

    @property
    def s3_temp_path(self):
        return self._s3_temp_path

    @s3_temp_path.setter
    def s3_temp_path(self, value):
        self._s3_temp_path = value

    @property
    def count_val(self):
        return self._count_val

    @count_val.setter
    def count_val(self, value):
        self._count_val = value

    def read_sql_without_partition(self, query_table):
        try:
            if query_table == 'dbtable':
                dbtable = self.read_dbtable
            elif query_table == 'query':
                dbtable = self.read_query
            else:
                query_table = query_table.split(".")[1]
                dbtable = self.read_query

            logging.info(
                "read_sql_without_partition :: Running SQL Query without partition for table " + self.table_name + "  Start",
                extra=CustomLogging.job.add_data())
            logging.info("dbtable: " + dbtable)
            logging.info("query_table: " + query_table)
            self.read_df = self.spark_session.read.format("jdbc") \
                .option('url', self.sql_con_prop['url']) \
                .option(query_table, dbtable) \
                .option('user', self.sql_con_prop['user']) \
                .option('password', self.sql_con_prop['password']) \
                .option('driver', self.sql_con_prop['driver']) \
                .option('encoding', 'UTF-8') \
                .option("numPartitions", "100") \
                .option("fetchsize", "200000") \
                .load()
            # .option("numPartitions", "40") \
            # .option("fetchsize", "100000") \

            logging.info(
                "read_sql_without_partition :: Running SQL Query without partition for table " + self.table_name + "  End",
                extra=CustomLogging.job.add_data())

        except Exception as readerror:
            logging.error(
                "read_sql_without_partition :: While Reading SQL Data without partition for table " + self.table_name + "  : " +
                str(readerror), extra=CustomLogging.job.add_data())
            raise readerror

    def read_sql_without_partition_incremental(self):
        try:
            logging.info(
                "read_sql_without_partition :: Running SQL Query without partition for table " + self.table_name + "  Start",
                extra=CustomLogging.job.add_data())

            self.read_df = self.spark_session.read.format("jdbc") \
                .option('url', self.sql_con_prop['url']) \
                .option('query', self.read_query) \
                .option('user', self.sql_con_prop['user']) \
                .option('password', self.sql_con_prop['password']) \
                .option('driver', self.sql_con_prop['driver']) \
                .option('encoding', 'UTF-8') \
                .load()

            logging.info(
                "read_sql_without_partition :: Running SQL Query without partition for table " + self.table_name + "  End",
                extra=CustomLogging.job.add_data())

        except Exception as readerror:
            logging.error(
                "read_sql_without_partition :: While Reading SQL Data without partition for table " + self.table_name + "  : " +
                str(readerror), extra=CustomLogging.job.add_data())
            raise readerror

    def pds_oracle_tmp(self, table_name):
        try:
            # Full load: Select all records without timestamp logic
            query = f"(SELECT * FROM {table_name}) pds"
            logging.info(f"Generated Oracle query for full load: {query}")
            return query
        except Exception as e:
            logging.error(f"Error in pds_oracle_tmp for {table_name}: {str(e)}", exc_info=True)
            raise

    def read_oracle_without_partition(self, dbtable, query):
        try:
            logging.info(f"Running Oracle query: {query}")
            self.read_df = self.spark_session.read.format("jdbc") \
                .option('url', self.oracle_con_prop['url']) \
                .option(dbtable, query) \
                .option('user', self.oracle_con_prop['user']) \
                .option('password', self.oracle_con_prop['password']) \
                .option('driver', self.oracle_con_prop['driver']) \
                .option('encoding', 'UTF-8') \
                .option("numPartitions", "150") \
                .option("fetchsize", "200000") \
                .load()
            logging.info(f"Oracle query completed. Schema: {self.read_df.schema}")
            logging.info(f"Sample Oracle data (up to 5 rows):")
            self.read_df.show(5, truncate=False)
        except Exception as e:
            logging.error(f"Error in read_oracle_without_partition for {query}: {str(e)}", exc_info=True)
            raise

    def audit_insert(self, query):
        try:
            logging.info("Preparing audit query to insert in job audit ::" + query, extra=CustomLogging.job.add_data())
            self.read_df = self.spark_session.sql(query)
            return self.read_df
        except Exception as readerror:
            logging.error("EXCEPTION RAISED WHILE INSERTING INTO JOB AUDIT TABLE " + self.table_name + "  : " +
                          str(readerror), extra=CustomLogging.job.add_data())
            raise readerror

    def create_hive_db(self, db_name):
        try:
            self.spark_session.sql("CREATE DATABASE IF NOT EXISTS {0}".format(db_name))
        except Exception as e:
            logging.error("Unexpected ERROR occured during creating hive database.")
            raise e
        return db_name

    def read_sql_sdm_control(self):
        try:
            logging.info(
                "read_sql_without_partition :: Running SQL Query without partition for table " + self.table_name + "  Start",
                extra=CustomLogging.job.add_data())

            self.read_df = self.spark_session.read.format("jdbc") \
                .option('url', self.sql_sdm_ctrl['url']) \
                .option('query', self.read_query) \
                .option('user', self.sql_sdm_ctrl['user']) \
                .option('password', self.sql_sdm_ctrl['password']) \
                .option('driver', self.sql_sdm_ctrl['driver']) \
                .option('encoding', 'UTF-8') \
                .load()

            logging.info(
                "read_sql_without_partition :: Running SQL Query without partition for table " + self.table_name + "  End",
                extra=CustomLogging.job.add_data())

        except Exception as readerror:
            logging.error(
                "read_sql_without_partition :: While Reading SQL Data without partition for table " + self.table_name + "  : " +
                str(readerror), extra=CustomLogging.job.add_data())
            raise readerror

    def read_sql_with_partition(self, query_table):
        try:
            if query_table == 'dbtable':
                dbtable = self.read_dbtable
            elif query_table == 'query':
                dbtable = self.read_dbtable
            else:
                query_table = query_table.split(".")[1]
                dbtable = self.read_query

            logging.info(
                "read_sql_with_partition :: Running SQL Query with partition for table " + self.table_name + "  Start",
                extra=CustomLogging.job.add_data())

            min_df = self.spark_session.read.format("jdbc").option('driver', self.sql_con_prop['driver']) \
                .option('url', self.sql_con_prop['url']).option('password', self.sql_con_prop['password']) \
                .option('user', self.sql_con_prop['user']).option('dbtable', "(SELECT MIN(" + self.partitioncolumn +
                                                                  ") AS COUNT FROM " + self.table_name + ") tmp").load()

            max_df = self.sql_context.read.format("jdbc").option('driver', self.sql_con_prop['driver']) \
                .option('url', self.sql_con_prop['url']).option('password', self.sql_con_prop['password']). \
                option('user', self.sql_con_prop['user']).option('dbtable', "(SELECT MAX(" + self.partitioncolumn +
                                                                 ") AS COUNT FROM " + self.table_name + ") tmp").load()

            count_df = self.sql_context.read.format("jdbc").option('driver', self.sql_con_prop['driver']) \
                .option('url', self.sql_con_prop['url']).option('password', self.sql_con_prop['password']) \
                .option('user', self.sql_con_prop['user']).option(
                'dbtable', "(SELECT COUNT_BIG(*) AS COUNT FROM " + self.table_name
                           + ") tmp").load()

            #self.num_partitions = int(count_df.collect()[0][0] / self.fetchsize)

            self.read_df = self.spark_session.read.format("jdbc") \
                .option('url', self.sql_con_prop['url']) \
                .option(query_table, dbtable) \
                .option('user', self.sql_con_prop['user']) \
                .option('password', self.sql_con_prop['password']) \
                .option('driver', self.sql_con_prop['driver']) \
                .option("numPartitions", "150") \
                .option("fetchsize", "200000") \
                .option("partitionColumn", self.partitioncolumn) \
                .option("lowerBound", min_df.collect()[0][0]) \
                .option("upperBound", max_df.collect()[0][0]) \
                .option('encoding', 'UTF-8') \
                .load()

            logging.info(
                "read_sql_with_partition :: Running SQL Query with partition for table " + self.table_name + "  End",
                extra=CustomLogging.job.add_data())

        except Exception as readerror:
            logging.error(
                "read_sql_with_partition :: While Reading SQL Data with partition for table " + self.table_name + "  : " +
                str(readerror), extra=CustomLogging.job.add_data())
            raise readerror

    def read_postgres_without_partition(self):
        try:

            logging.info(
                "read_postgres_without_partition :: Running Postgres Query without partition for table " + self.table_name + "  Start",
                extra=CustomLogging.job.add_data())

            self.read_df = self.spark_session.read.format("jdbc") \
                .option('url', self.pg_con_prop['url']) \
                .option('dbtable', "(" + self.read_dbtable + ") arl") \
                .option('user', self.pg_con_prop['user']) \
                .option('password', self.pg_con_prop['password']) \
                .option('driver', self.pg_con_prop['driver']) \
                .option('encoding', 'UTF-8') \
                .option("numPartitions", "50") \
                .option("fetchsize", "100000") \
                .load()

            # .option("numPartitions", "40") \
            # .option("fetchsize", "100000") \

            logging.info(
                "read_postgres_without_partition :: Running Postgres Query without partition for table " + self.table_name + "  End",
                extra=CustomLogging.job.add_data())

        except Exception as readerror:
            logging.error(
                "read_postgres_without_partition :: While Reading Postgres Data without partition for table " + self.table_name + "  : " +
                str(readerror), extra=CustomLogging.job.add_data())
            raise readerror

    def read_postgres_with_partition(self):
        try:
            logging.info(
                "read_postgres_with_partition :: Running Postgres Query with partition for table " + self.table_name + "  Start",
                extra=CustomLogging.job.add_data())

            self.read_count_query = 'select min(' + self.partitioncolumn + ') as count from (' + self.read_query + ') as a'
            print(self.read_count_query)
            min_df = self.get_count_from_postgres()

            self.read_count_query = 'select max(' + self.partitioncolumn + ') as count from (' + self.read_query + ') as a'
            print(self.read_count_query)
            max_df = self.get_count_from_postgres()

            self.read_count_query = 'select count(*) as count from (' + self.read_query + ') as a'
            print(self.read_count_query)
            count_df = self.get_count_from_postgres()

            self.num_partitions = int(count_df / self.fetchsize)

            self.read_df = self.spark_session.read.format("jdbc") \
                .option('url', self.pg_con_prop['url']) \
                .option('dbtable', "(" + self.read_dbtable + ") arl") \
                .option('user', self.pg_con_prop['user']) \
                .option('password', self.pg_con_prop['password']) \
                .option('driver', self.pg_con_prop['driver']) \
                .option("numPartitions", "150") \
                .option("fetchsize", "200000") \
                .option("partitionColumn", self.partitioncolumn) \
                .option("lowerBound", min_df) \
                .option("upperBound", max_df) \
                .option('encoding', 'UTF-8') \
                .load()

            logging.info(
                "read_postgres_with_partition :: Running Postgres Query with partition for table " + self.table_name + "  End",
                extra=CustomLogging.job.add_data())

        except Exception as readerror:
            logging.error(
                "read_postgres_with_partition :: While Reading Postgres Data with partition for table " + self.table_name + "  : " +
                str(readerror), extra=CustomLogging.job.add_data())
            raise readerror

    def read_postgres_without_partition_arl(self):
        try:

            logging.info(
                "read_postgres_without_partition :: Running Postgres Query without partition for table " + self.read_query + "  Start",
                extra=CustomLogging.job.add_data())

            self.read_df = self.spark_session.read.format("jdbc") \
                .option('url', self.pg_con_prop['url']) \
                .option('dbtable', self.read_query) \
                .option('user', self.pg_con_prop['user']) \
                .option('password', self.pg_con_prop['password']) \
                .option('driver', self.pg_con_prop['driver']) \
                .option('encoding', 'UTF-8') \
                .option("numPartitions", "50") \
                .option("fetchsize", "100000") \
                .load()

            self.read_df.printSchema()
            self.read_df.show()

            # .option("numPartitions", "40") \
            # .option("fetchsize", "100000") \

            logging.info(
                "read_postgres_without_partition :: Running Postgres Query without partition for table " + self.table_name + "  End",
                extra=CustomLogging.job.add_data())

        except Exception as readerror:
            logging.error(
                "read_postgres_without_partition :: While Reading Postgres Data without partition for table " + self.table_name + "  : " +
                str(readerror), extra=CustomLogging.job.add_data())
            raise readerror

    def read_postgres_with_partition_arl(self):
        try:
            logging.info(
                "read_postgres_with_partition :: Running Postgres Query with partition for table " + self.table_name + "  Start",
                extra=CustomLogging.job.add_data())

            self.read_count_query = 'select min(' + self.partitioncolumn + ') as count from ' + self.read_query
            print(self.read_count_query)
            min_df = self.get_count_from_postgres()

            self.read_count_query = 'select max(' + self.partitioncolumn + ') as count from ' + self.read_query
            print(self.read_count_query)
            max_df = self.get_count_from_postgres()

            self.read_count_query = 'select count(*) as count from ' + self.read_query
            print(self.read_count_query)
            # count_df = self.get_count_from_postgres()

            # self.num_partitions = int(count_df / self.fetchsize)

            self.read_df = self.spark_session.read.format("jdbc") \
                .option('url', self.pg_con_prop['url']) \
                .option('dbtable', self.read_query) \
                .option('user', self.pg_con_prop['user']) \
                .option('password', self.pg_con_prop['password']) \
                .option('driver', self.pg_con_prop['driver']) \
                .option("numPartitions", "150") \
                .option("fetchsize", "200000") \
                .option("partitionColumn", self.partitioncolumn) \
                .option("lowerBound", min_df) \
                .option("upperBound", max_df) \
                .option('encoding', 'UTF-8') \
                .load()

            logging.info(
                "read_postgres_with_partition :: Running Postgres Query with partition for table " + self.table_name + "  End",
                extra=CustomLogging.job.add_data())

        except Exception as readerror:
            logging.error(
                "read_postgres_with_partition :: While Reading Postgres Data with partition for table " + self.table_name + "  : " +
                str(readerror), extra=CustomLogging.job.add_data())
            raise readerror

    def write_postgres_with_repartition(self):
        try:

            logging.info("write_postgres_with_partitionby :: Writing data to Postgres with partition for table " +
                         self.schema_name + '.' + self.table_name + " Start ", extra=CustomLogging.job.add_data())

            self.read_df.repartition(self.partitioncolumn).write.format('jdbc').options(url=self.pg_con_prop['url'],
                                                                                        driver=self.pg_con_prop[
                                                                                            'driver'],
                                                                                        dbtable=self.schema_name + '.' + self.table_name,
                                                                                        user=self.pg_con_prop['user'],
                                                                                        password=self.pg_con_prop[
                                                                                            'password'],
                                                                                        batchsize=100000).mode(
                'append').save()

            logging.info(
                "write_postgres_with_partitionby :: Writing data to Postgres with partition for table " + self.table_name + " End",
                extra=CustomLogging.job.add_data())

        except Exception as writeerror:
            logging.error(
                "write_redshift_with_partition :: While writing data to Postgres with partition for table " + self.table_name + " : " +
                str(writeerror), extra=CustomLogging.job.add_data())
            raise writeerror

    def read_pplus_db2_without_partition(self,type,input,env):

        url = ''
        if env == 'dev':
            url = 'jdbc:db2://DSNT_GRP.US.AEGON.COM:5026/VALDSNT'
        elif env == 'tst':
            url = 'jdbc:db2://DSNT_GRP.US.AEGON.COM:5026/VALDSNT'
        elif env == 'mdl':
            url = 'jdbc:db2://DSNM_GRP.US.AEGON.COM:5028/VALDDSNM'
        elif env == 'prd':
            url = 'jdbc:db2://DSNP_GRP.US.AEGON.COM:5020/CRS1DSNP'

        logging.info("read_db2_without_partition :: Running db2 Query without partition for table " + input + "  Start",extra=CustomLogging.job.add_data())
        self.read_df  = self.spark_session.read.format('jdbc') \
            .option('user', self.pplus_db2_con_prop['user']) \
            .option('password', self.pplus_db2_con_prop['password']) \
            .option('driver', 'com.ibm.db2.jcc.DB2Driver') \
            .option('url', url) \
            .option(type, input) \
            .option("numPartitions", "250") \
            .option("fetchsize", "200000") \
            .load()

    def read_oracle_without_partition(self, type, input):

        try:
            logging.info(
                "read_oracle_without_partition :: Running Oracle Query without partition for table " + input + "  Start",
                extra=CustomLogging.job.add_data())
            self.read_df = self.spark_session.read.format("jdbc") \
                .option('url', self.oracle_con_prop['url']) \
                .option(type, input) \
                .option('user', self.oracle_con_prop['user']) \
                .option('password', self.oracle_con_prop['password']) \
                .option('driver', self.oracle_con_prop['driver']) \
                .option('encoding', 'UTF-8') \
                .option("numPartitions", "150") \
                .option("fetchsize", "200000") \
                .load()

            logging.info(
                "read_oracle_without_partition :: Running Oracle Query without partition for table " + self.table_name + "  End",
                extra=CustomLogging.job.add_data())

        except Exception as readerror:
            logging.error(
                "read_oracle_without_partition :: While Reading Oracle without partition Data for table " + self.table_name + "  : " +
                str(readerror), extra=CustomLogging.job.add_data())
            raise readerror

    def hzn_read_oracle_without_partition(self, type, input,env):
        url=''
        if env=='dev':
            url='jdbc:oracle:thin:@' + 'dcup60m.us.aegon.com' + ':' + '1681' + '/' + 'hzmo.us.aegon.com'
        elif env =='tst':
            url = 'jdbc:oracle:thin:@' + 'dcup60m.us.aegon.com' + ':' + '1681' + '/' + 'hzmo.us.aegon.com'
        elif env =='mdl':
            url = 'jdbc:oracle:thin:@' + 'dcup60m.us.aegon.com' + ':' + '1681' + '/' + 'hzmo.us.aegon.com'
        elif env =='prd':
            url = 'jdbc:oracle:thin:@' + 'dcup61p.us.aegon.com' + ':' + '1581' + '/' + 'horizon.us.aegon.com'
        try:
            logging.info(
                "read_oracle_without_partition :: Running Oracle Query without partition for table " + input + "  Start",
                extra=CustomLogging.job.add_data())
            print(type)
            print(input)
            self.read_df = self.spark_session.read.format("jdbc") \
                .option('url', url) \
                .option(type, input) \
                .option('user', self.hzn_oracle_con_prop['user']) \
                .option('password', self.hzn_oracle_con_prop['password']) \
                .option('driver', self.hzn_oracle_con_prop['driver']) \
                .option('encoding', 'UTF-8') \
                .option("numPartitions", "150") \
                .option("fetchsize", "200000") \
                .load()

            logging.info(
                "read_oracle_without_partition :: Running Oracle Query without partition for table " + self.table_name + "  End",
                extra=CustomLogging.job.add_data())

        except Exception as readerror:
            logging.error(
                "read_oracle_without_partition :: While Reading Oracle without partition Data for table " + self.table_name + "  : " +
                str(readerror), extra=CustomLogging.job.add_data())
            raise readerror

    def phd_read_oracle_without_partition(self, type, input):

        try:
            logging.info(
                "read_oracle_without_partition :: Running Oracle Query without partition for table " + input + "  Start",
                extra=CustomLogging.job.add_data())
            print(type)
            print(input)
            self.read_df = self.spark_session.read.format("jdbc") \
                .option('url', self.phd_oracle_con_prop['url']) \
                .option(type, input) \
                .option('user', self.phd_oracle_con_prop['user']) \
                .option('password', self.phd_oracle_con_prop['password']) \
                .option('driver', self.phd_oracle_con_prop['driver']) \
                .option('encoding', 'UTF-8') \
                .option("numPartitions", "150") \
                .option("fetchsize", "200000") \
                .load()

            logging.info(
                "read_oracle_without_partition :: Running Oracle Query without partition for table " + self.table_name + "  End",
                extra=CustomLogging.job.add_data())

        except Exception as readerror:
            logging.error(
                "read_oracle_without_partition :: While Reading Oracle without partition Data for table " + self.table_name + "  : " +
                str(readerror), extra=CustomLogging.job.add_data())
            raise readerror

    def read_oracle_with_partition(self, type, input, partitioncolumn):

        try:
            self.spark_session.sql("set spark.sql.legacy.parquet.int96RebaseModeInWrite=LEGACY")
            self.spark_session.sql("set spark.sql.legacy.parquet.datetimeRebaseModeInWrite=LEGACY")
            self.read_count_query = 'select min(' + partitioncolumn + ') as count from ' + input
            min_df = self.get_count_from_oracle_pds_values()

            self.read_count_query = 'select max(' + partitioncolumn + ') as count from ' + input
            max_df = self.get_count_from_oracle_pds_values()

            # self.read_count_query = 'select count(*) as count from ' + self.read_dbtable
            # count_df = self.get_count_from_oracle_pds_values()

            # self.num_partitions = int(count_df / int(self.fetchsize))

            logging.info(
                "read_oracle_with_partition :: Running Oracle Query with partition for table " + input + "  Start",
                extra=CustomLogging.job.add_data())

            self.read_df = self.spark_session.read.format("jdbc") \
                .option('url', self.oracle_con_prop['url']) \
                .option(type, input) \
                .option('user', self.oracle_con_prop['user']) \
                .option('password', self.oracle_con_prop['password']) \
                .option('driver', self.oracle_con_prop['driver']) \
                .option("numPartitions", "250") \
                .option("fetchsize", "200000") \
                .option("partitionColumn", partitioncolumn) \
                .option("lowerBound", min_df) \
                .option("upperBound", max_df) \
                .option('encoding', 'UTF-8') \
                .option("sessionInitStatement","alter session set NLS_TIMESTAMP_FORMAT = YYYY-MM-DD HH24:MI:SS.FF")\
                .load()

            logging.info(
                "read_oracle_with_partition :: Running Oracle Query with partition for table " + input + "  End",
                extra=CustomLogging.job.add_data())
        except Exception as readerror:
            logging.error(
                "read_oracle_with_partition :: While Reading Oracle Data with partition for table " + input + "  : " +
                str(readerror), extra=CustomLogging.job.add_data())
            raise readerror

    def hzn_read_oracle_with_partition(self, type, input, partitioncolumn,env):
            url = ''
            if env == 'dev':
                url = 'jdbc:oracle:thin:@' + 'dcup60m.us.aegon.com' + ':' + '1681' + '/' + 'hzmo.us.aegon.com'
            elif env == 'tst':
                url = 'jdbc:oracle:thin:@' + 'dcup60m.us.aegon.com' + ':' + '1681' + '/' + 'hzmo.us.aegon.com'
            elif env == 'mdl':
                url = 'jdbc:oracle:thin:@' + 'dcup60m.us.aegon.com' + ':' + '1681' + '/' + 'hzmo.us.aegon.com'
            elif env == 'prd':
                url = 'jdbc:oracle:thin:@' + 'dcup61p.us.aegon.com' + ':' + '1581' + '/' + 'horizon.us.aegon.com'

            try:
                self.spark_session.sql("set spark.sql.legacy.parquet.int96RebaseModeInWrite=LEGACY")
                self.spark_session.sql("set spark.sql.legacy.parquet.datetimeRebaseModeInWrite=LEGACY")
                self.read_count_query = 'select min(' + partitioncolumn + ') as count from ' + input
                min_df = self.get_count_from_oracle_pds()

                self.read_count_query = 'select max(' + partitioncolumn + ') as count from ' + input
                max_df = self.get_count_from_oracle_pds()

                # self.read_count_query = 'select count(*) as count from ' + self.read_dbtable
                # count_df = self.get_count_from_oracle_pds()

                # self.num_partitions = int(count_df / int(self.fetchsize))

                logging.info(
                    "read_oracle_with_partition :: Running Oracle Query with partition for table " + input + "  Start",
                    extra=CustomLogging.job.add_data())

                self.read_df = self.spark_session.read.format("jdbc") \
                    .option('url', url) \
                    .option(type, input) \
                    .option('user', self.hzn_oracle_con_prop['user']) \
                    .option('password', self.hzn_oracle_con_prop['password']) \
                    .option('driver', self.hzn_oracle_con_prop['driver']) \
                    .option("numPartitions", "250") \
                    .option("fetchsize", "200000") \
                    .option("partitionColumn", partitioncolumn) \
                    .option("lowerBound", min_df) \
                    .option("upperBound", max_df) \
                    .option('encoding', 'UTF-8') \
                    .load()

                logging.info(
                    "read_oracle_with_partition :: Running Oracle Query with partition for table " + input + "  End",
                    extra=CustomLogging.job.add_data())
            except Exception as readerror:
                logging.error(
                    "read_oracle_with_partition :: While Reading Oracle Data with partition for table " + input + "  : " +
                    str(readerror), extra=CustomLogging.job.add_data())
                raise readerror

    def phd_read_oracle_with_partition(self, type, input, partitioncolumn):

        try:
            self.spark_session.sql("set spark.sql.legacy.parquet.int96RebaseModeInWrite=LEGACY")
            self.spark_session.sql("set spark.sql.legacy.parquet.datetimeRebaseModeInWrite=LEGACY")
            self.read_count_query = 'select min(' + partitioncolumn + ') as count from ' + input
            min_df_d = self.phd_get_count_from_oracle_pds_values()
            min_df = min_df_d.strftime("%Y-%m-%d")

            self.read_count_query = 'select max(' + partitioncolumn + ') as count from ' + input
            max_df_d = self.phd_get_count_from_oracle_pds_values()
            max_df = max_df_d.strftime("%Y-%m-%d")

            # self.read_count_query = 'select count(*) as count from ' + self.read_dbtable
            # count_df = self.get_count_from_oracle_pds()

            # self.num_partitions = int(count_df / int(self.fetchsize))

            logging.info(
                "read_oracle_with_partition :: Running Oracle Query with partition for table " + input + "  Start",
                extra=CustomLogging.job.add_data())

            self.read_df = self.spark_session.read.format("jdbc") \
                .option('url', self.phd_oracle_con_prop['url']) \
                .option(type, input) \
                .option('user', self.phd_oracle_con_prop['user']) \
                .option('password', self.phd_oracle_con_prop['password']) \
                .option('driver', self.phd_oracle_con_prop['driver']) \
                .option("numPartitions", "250") \
                .option("fetchsize", "200000") \
                .option("partitionColumn", partitioncolumn) \
                .option("oracle.jdbc.mapDateToTimestamp", "false") \
                .option("sessionInitStatement", "ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD'") \
                .option("lowerBound", min_df) \
                .option("upperBound", max_df) \
                .option('encoding', 'UTF-8') \
                .load()

            logging.info(
                "read_oracle_with_partition :: Running Oracle Query with partition for table " + input + "  End",
                extra=CustomLogging.job.add_data())

        except Exception as readerror:
            logging.error(
                "read_oracle_with_partition :: While Reading Oracle Data with partition for table " + input + "  : " +
                str(readerror), extra=CustomLogging.job.add_data())
            raise readerror

    def write_redshift_with_partition(self):
        try:

            if self.partitioncolumn1.isdigit() == True:
                self.partitioncolumn1 = int(self.partitioncolumn1)

            logging.info(
                "write_redshift_with_partition :: Writing data to Redshift with partition for table " + self.table_name + " Start ",
                extra=CustomLogging.job.add_data())
            logging.info("dbtable: " + self.write_dbtable)
            self.read_df.repartition(self.partitioncolumn1).write.format("com.databricks.spark.redshift") \
                .option("url", self.redshift_con_prop['url']) \
                .option("user", self.redshift_con_prop['user']) \
                .option("password", self.redshift_con_prop['password']) \
                .option("dbtable", self.write_dbtable) \
                .option("tempdir", self.s3_temp_dir) \
                .option("truncate", True) \
                .option("batchsize", "15000") \
                .option('encoding', 'UTF-8') \
                .mode("overwrite") \
                .save()

            # .option("encoding", "UTF-8") \
            logging.info(
                "write_redshift_with_partition :: Writing data to Redshift with partition for table " + self.table_name + " End",
                extra=CustomLogging.job.add_data())

        except Exception as writeerror:
            logging.error(
                "write_redshift_with_partition :: While writing data to Redshift with partition for table " + self.table_name + " : " +
                str(writeerror), extra=CustomLogging.job.add_data())
            raise writeerror

    def write_redshift_table_status(self):
        try:

            logging.info(
                "write_redshift_with_partition :: Writing data to Redshift with partition for table " + self.table_name + " Start ",
                extra=CustomLogging.job.add_data())
            logging.info("dbtable: " + self.write_dbtable)
            self.read_df.write.format("com.databricks.spark.redshift") \
                .option("url", self.redshift_con_prop['url']) \
                .option("user", self.redshift_con_prop['user']) \
                .option("password", self.redshift_con_prop['password']) \
                .option("dbtable", "controls." + self.table_name.lower()) \
                .option("tempdir", self.s3_temp_dir) \
                .option("batchsize", "15000") \
                .option('encoding', 'UTF-8') \
                .mode("append") \
                .save()

            logging.info(
                "write_redshift_with_partition :: Writing data to Redshift with partition for table " + self.table_name + " End",
                extra=CustomLogging.job.add_data())

        except Exception as writeerror:
            logging.error(
                "write_redshift_with_partition :: While writing data to Redshift with partition for table " + self.table_name + " : " +
                str(writeerror), extra=CustomLogging.job.add_data())
            raise writeerror

    def read_redshift_with_partition(self, query_table):
        try:
            if query_table == 'dbtable':
                dbtable = "controls." + self.table_name
            elif query_table == 'query':
                dbtable = self.read_query
            else:
                query_table = query_table.split(".")[1]
                dbtable = self.read_query

            logging.info(
                "read_redshift_with_partition :: Read data to Redshift with partition for table " + self.table_name + " Start ",
                extra=CustomLogging.job.add_data())
            logging.info("dbtable: " + dbtable)
            logging.info("query_table: " + query_table)
            self.read_df = self.spark_session.read.format("com.databricks.spark.redshift") \
                .option("url", self.redshift_con_prop['url']) \
                .option("user", self.redshift_con_prop['user']) \
                .option("password", self.redshift_con_prop['password']) \
                .option(query_table, dbtable) \
                .option("tempdir", self.s3_temp_dir) \
                .option("batchsize", self.batchsize) \
                .option('encoding', 'UTF-8') \
                .load()
            logging.info(
                "read_redshift_with_partition :: Read data to Redshift with partition for table " + self.table_name + " End",
                extra=CustomLogging.job.add_data())

        except Exception as writeerror:
            logging.error(
                "read_redshift_with_partition :: While Read data to Redshift with partition for table " + self.table_name + " : " +
                str(writeerror), extra=CustomLogging.job.add_data())
            raise writeerror

    def write_redshift_without_partition(self):
        try:
            if self.partitioncolumn1.isdigit() == True:
                self.partitioncolumn1 = int(self.partitioncolumn1)

            logging.info(
                "write_redshift_without_partition :: Writing data to Redshift without partition for table " + self.table_name + " Start",
                extra=CustomLogging.job.add_data())
            logging.info("write target Table:" + self.write_dbtable)
            self.read_df.repartition(self.partitioncolumn1).write.format("com.databricks.spark.redshift") \
                .option("url", self.redshift_con_prop['url']) \
                .option("user", self.redshift_con_prop['user']) \
                .option("password", self.redshift_con_prop['password']) \
                .option("dbtable", self.write_dbtable) \
                .option("tempdir", self.s3_temp_dir) \
                .option("truncate", True) \
                .option("batchsize", "15000") \
                .option('encoding', 'UTF-8') \
                .mode("overwrite") \
                .save()

            logging.info(
                "write_redshift_without_partition :: Writing data to Redshift without partition for table " + self.table_name + "  End",
                extra=CustomLogging.job.add_data())

        except Exception as writeerror:
            logging.error(
                "write_redshift_without_partition :: While writing data to Redshift without partition for table " + self.table_name + " : " +
                str(writeerror), extra=CustomLogging.job.add_data())
            raise writeerror

    def read_redshift(self):
        try:
            logging.info("read_redshift :: Running Redshift Query for table " + self.table_name + "  Start",
                         extra=CustomLogging.job.add_data())
            self.read_df = self.spark_session.read.format("com.databricks.spark.redshift") \
                .option('url', self.redshift_con_prop['url']) \
                .option("user", self.redshift_con_prop['user']) \
                .option("password", self.redshift_con_prop['password']) \
                .option('query', self.read_query) \
                .option("tempdir", self.s3_temp_dir) \
                .option('encoding', 'UTF-8') \
                .load()

            logging.info("read_redshift :: Running Redshift Query for table " + self.table_name + "  End",
                         extra=CustomLogging.job.add_data())

        except Exception as readerror:
            logging.error("read_redshift :: While Reading Redshift Data for table " + self.table_name + "  : " +
                          str(readerror), extra=CustomLogging.job.add_data())
            raise readerror

    def get_count_from_postgres(self):
        try:
            logging.info(
                "get_count_from_postgres :: Get total count from postgres for table " + self.table_name + "  Start",
                extra=CustomLogging.job.add_data())
            postgres_conn = self.psycopg_postgres()
            cursor = postgres_conn.cursor()
            cursor.execute(self.read_count_query)
            fetched_rows = cursor.fetchone()
            postgres_conn.close()

            logging.info(
                "get_count_from_postgres :: Get total count from postgres for table " + self.table_name + "  End",
                extra=CustomLogging.job.add_data())

            return fetched_rows[0]

        except Exception as readerror:
            logging.error(
                "get_count_from_postgres :: While Getting total count from postgres for table " + self.table_name + "  : " +
                str(readerror), extra=CustomLogging.job.add_data())
            raise readerror

    def get_count_from_redshift(self):
        try:
            logging.info(
                "get_count_from_redshift :: Get total count from redshift for table " + self.table_name + "  Start",
                extra=CustomLogging.job.add_data())
            redshift_conn = self.psycopg_redshift()
            cursor = redshift_conn.cursor()
            cursor.execute(self.read_count_query)
            fetched_rows = cursor.fetchone()
            redshift_conn.close()

            logging.info(
                "get_count_from_redshift :: Get total count from redshift for table " + self.table_name + "  End",
                extra=CustomLogging.job.add_data())

            return fetched_rows[0]

        except Exception as readerror:
            logging.error(
                "get_count_from_redshift :: While Getting total count from redshift for table " + self.table_name + "  : " +
                str(readerror), extra=CustomLogging.job.add_data())
            raise readerror

    def get_count_from_sql_sdm(self):
        try:
            logging.info(
                "get_count_from_sql_sdm :: Get total count from SDM sql server for table " + self.table_name + "  Start",
                extra=CustomLogging.job.add_data())
            sql_sdm_conn = self.pymssql_sqlserver_sdm()
            cursor = sql_sdm_conn.cursor()
            cursor.execute(self.read_count_query)
            fetched_rows = cursor.fetchone()
            sql_sdm_conn.close()

            logging.info(
                "get_count_from_sql_sdm :: Get total count from SDM sql server for table " + self.table_name + "  End",
                extra=CustomLogging.job.add_data())

            return fetched_rows[0]

        except Exception as readerror:
            logging.error(
                "get_count_from_sql_sdm :: While Getting total count from SDM sql server for table " + self.table_name + "  : " +
                str(readerror), extra=CustomLogging.job.add_data())
            raise readerror

    def get_count_from_sql_inf(self):
        try:
            logging.info(
                "get_count_from_sql_inf :: Get total count from INF sql server for table " + self.table_name + "  Start",
                extra=CustomLogging.job.add_data())
            inf_sdm_conn = self.pymssql_sqlserver_inf()
            cursor = inf_sdm_conn.cursor()
            cursor.execute(self.read_count_query)
            fetched_rows = cursor.fetchone()
            inf_sdm_conn.close()

            logging.info(
                "get_count_from_sql_inf :: Get total count from INF sql server for table " + self.table_name + "  End",
                extra=CustomLogging.job.add_data())

            return fetched_rows[0]

        except Exception as readerror:
            logging.error(
                "get_count_from_sql_inf :: While Getting total count from INF sql server for table " + self.table_name + "  : " +
                str(readerror), extra=CustomLogging.job.add_data())
            raise readerror

    def get_count_from_oracle_pds_values(self):
        try:
            logging.info(
                "get_count_from_oracle_pds :: Get total count from PDS oracle server for table " + self.table_name + "  Start",
                extra=CustomLogging.job.add_data())



            self.read_df = self.spark_session.read.format("jdbc") \
                .option('url', self.oracle_con_prop['url']) \
                .option('query', self.read_count_query) \
                .option('user', self.oracle_con_prop['user']) \
                .option('password', self.oracle_con_prop['password']) \
                .option('driver', self.oracle_con_prop['driver']) \
                .option("fetchsize", "50000") \
                .option('encoding', 'UTF-8') \
                .load()
            logging.info(
                "get_count_from_oracle_pds :: Get total count from PDS oracle server for table " + self.table_name + "  End",
                extra=CustomLogging.job.add_data())
            return self.read_df.collect()[0][0]
        except Exception as readerror:
            logging.error(
                "get_count_from_oracle_pds :: While Getting total count from PDS oracle server for table " + self.table_name + "  : " +
                str(readerror), extra=CustomLogging.job.add_data())
            raise readerror

    def read_pplus_db2_time(self,env):
        url = ''
        if env == 'dev':
            url = 'jdbc:db2://DSNT_GRP.US.AEGON.COM:5026/VALDSNT'
        elif env == 'tst':
            url = 'jdbc:db2://DSNT_GRP.US.AEGON.COM:5026/VALDSNT'
        elif env == 'mdl':
            url = 'jdbc:db2://DSNM_GRP.US.AEGON.COM:5028/VALDDSNM'
        elif env == 'prd':
            url = 'jdbc:db2://DSNP_GRP.US.AEGON.COM:5020/CRS1DSNP'

        logging.info("read_db2_time :: Running db2 Query for timestamp for table  Start",extra=CustomLogging.job.add_data())
        read_df  = self.spark_session.read.format('jdbc') \
            .option('user', self.pplus_db2_con_prop['user']) \
            .option('password', self.pplus_db2_con_prop['password']) \
            .option('driver', 'com.ibm.db2.jcc.DB2Driver') \
            .option('url', url) \
            .option('dbtable', self.read_count_query) \
            .load()
        return read_df.collect()[0][0]

    def get_count_from_oracle_hzn_values(self,env):
        url = ''
        if env == 'dev':
            url = 'jdbc:oracle:thin:@' + 'dcup60m.us.aegon.com' + ':' + '1681' + '/' + 'hzmo.us.aegon.com'
        elif env == 'tst':
            url = 'jdbc:oracle:thin:@' + 'dcup60m.us.aegon.com' + ':' + '1681' + '/' + 'hzmo.us.aegon.com'
        elif env == 'mdl':
            url = 'jdbc:oracle:thin:@' + 'dcup60m.us.aegon.com' + ':' + '1681' + '/' + 'hzmo.us.aegon.com'
        elif env == 'prd':
            url = 'jdbc:oracle:thin:@' + 'dcup61p.us.aegon.com' + ':' + '1581' + '/' + 'horizon.us.aegon.com'
        try:
            logging.info(
                "get_count_from_oracle_hzn :: Get total count from HZN oracle server for table " + self.table_name + "  Start",
                extra=CustomLogging.job.add_data())

            self.read_df = self.spark_session.read.format("jdbc") \
                .option('url', url) \
                .option('query', self.read_count_query) \
                .option('user', self.hzn_oracle_con_prop['user']) \
                .option('password', self.hzn_oracle_con_prop['password']) \
                .option('driver', self.hzn_oracle_con_prop['driver']) \
                .option("fetchsize", "50000") \
                .option('encoding', 'UTF-8') \
                .load()
            logging.info(
                "get_count_from_oracle_hzn :: Get total count from HZN oracle server for table " + self.table_name + "  End",
                extra=CustomLogging.job.add_data())
            return self.read_df.collect()[0][0]
        except Exception as readerror:
            logging.error(
                "get_count_from_oracle_hzn :: While Getting total count from HZN oracle server for table " + self.table_name + "  : " +
                str(readerror), extra=CustomLogging.job.add_data())
            raise readerror

    def phd_get_count_from_oracle_pds_values(self):
        try:
            logging.info(
                "get_count_from_oracle_pds :: Get total count from PDS oracle server for table " + self.table_name + "  Start",
                extra=CustomLogging.job.add_data())

            self.read_df = self.spark_session.read.format("jdbc") \
                .option('url', self.phd_oracle_con_prop['url']) \
                .option('query', self.read_count_query) \
                .option('user', self.phd_oracle_con_prop['user']) \
                .option('password', self.phd_oracle_con_prop['password']) \
                .option('driver', self.phd_oracle_con_prop['driver']) \
                .option("fetchsize", "50000") \
                .option('encoding', 'UTF-8') \
                .load()
            logging.info(
                "get_count_from_oracle_pds :: Get total count from PDS oracle server for table " + self.table_name + "  End",
                extra=CustomLogging.job.add_data())
            return self.read_df.collect()[0][0]
        except Exception as readerror:
            logging.error(
                "get_count_from_oracle_pds :: While Getting total count from PDS oracle server for table " + self.table_name + "  : " +
                str(readerror), extra=CustomLogging.job.add_data())
            raise readerror

    def get_count_from_postgres_values(self):
        try:
            logging.info(
                "get_count_from_oracle_pds :: Get total count from PDS oracle server for table " + self.table_name + "  Start",
                extra=CustomLogging.job.add_data())

            self.read_df = self.spark_session.read.format("jdbc") \
                .option('url', self.pg_con_prop['url']) \
                .option('query', self.read_count_query) \
                .option('user', self.pg_con_prop['user']) \
                .option('password', self.pg_con_prop['password']) \
                .option('driver', self.pg_con_prop['driver']) \
                .option("fetchsize", "50000") \
                .option('encoding', 'UTF-8') \
                .load()
            logging.info(
                "get_count_from_oracle_pds :: Get total count from PDS oracle server for table " + self.table_name + "  End",
                extra=CustomLogging.job.add_data())

            return self.read_df.collect()[0][0]
        except Exception as readerror:
            logging.error(
                "get_count_from_oracle_pds :: While Getting total count from PDS oracle server for table " + self.table_name + "  : " +
                str(readerror), extra=CustomLogging.job.add_data())
            raise readerror

    def get_count_from_oracle_pds(self):
        try:
            logging.info(
                "get_count_from_oracle_pds :: Get total count from PDS oracle server for table " + self.table_name + "  Start",
                extra=CustomLogging.job.add_data())

            self.read_df = self.spark_session.read.format("jdbc") \
                .option('url', self.oracle_con_prop['url']) \
                .option('query', self.read_count_query) \
                .option('user', self.oracle_con_prop['user']) \
                .option('password', self.oracle_con_prop['password']) \
                .option('driver', self.oracle_con_prop['driver']) \
                .option("fetchsize", "50000") \
                .option('encoding', 'UTF-8') \
                .load()

            logging.info(
                "get_count_from_oracle_pds :: Get total count from PDS oracle server for table " + self.table_name + "  End",
                extra=CustomLogging.job.add_data())

            return int(self.read_df.collect()[0][0])

        except Exception as readerror:
            logging.error(
                "get_count_from_oracle_pds :: While Getting total count from PDS oracle server for table " + self.table_name + "  : " +
                str(readerror), extra=CustomLogging.job.add_data())
            raise readerror

    def get_count_from_oracle_hzn(self,env):
        url = ''
        if env == 'dev':
            url = 'jdbc:oracle:thin:@' + 'dcup60m.us.aegon.com' + ':' + '1681' + '/' + 'hzmo.us.aegon.com'
        elif env == 'tst':
            url = 'jdbc:oracle:thin:@' + 'dcup60m.us.aegon.com' + ':' + '1681' + '/' + 'hzmo.us.aegon.com'
        elif env == 'mdl':
            url = 'jdbc:oracle:thin:@' + 'dcup60m.us.aegon.com' + ':' + '1681' + '/' + 'hzmo.us.aegon.com'
        elif env == 'prd':
            url = 'jdbc:oracle:thin:@' + 'dcup61p.us.aegon.com' + ':' + '1581' + '/' + 'horizon.us.aegon.com'
        try:
            logging.info(
                "get_count_from_oracle_hzn :: Get total count from HZN oracle server for table " + self.table_name + "  Start",
                extra=CustomLogging.job.add_data())

            self.read_df = self.spark_session.read.format("jdbc") \
                .option('url', url) \
                .option('query', self.read_count_query) \
                .option('user', self.hzn_oracle_con_prop['user']) \
                .option('password', self.hzn_oracle_con_prop['password']) \
                .option('driver', self.hzn_oracle_con_prop['driver']) \
                .option("fetchsize", "50000") \
                .option('encoding', 'UTF-8') \
                .load()

            logging.info(
                "get_count_from_oracle_hzn :: Get total count from HZN oracle server for table " + self.table_name + "  End",
                extra=CustomLogging.job.add_data())

            return int(self.read_df.collect()[0][0])

        except Exception as readerror:
            logging.error(
                "get_count_from_oracle_hzn :: While Getting total count from HZN oracle server for table " + self.table_name + "  : " +
                str(readerror), extra=CustomLogging.job.add_data())
            raise readerror

    def phd_get_count_from_oracle_pds(self):
        try:
            logging.info(
                "get_count_from_oracle_pds :: Get total count from PDS oracle server for table " + self.table_name + "  Start",
                extra=CustomLogging.job.add_data())

            self.read_df = self.spark_session.read.format("jdbc") \
                .option('url', self.phd_oracle_con_prop['url']) \
                .option('query', self.read_count_query) \
                .option('user', self.phd_oracle_con_prop['user']) \
                .option('password', self.phd_oracle_con_prop['password']) \
                .option('driver', self.phd_oracle_con_prop['driver']) \
                .option("fetchsize", "50000") \
                .option('encoding', 'UTF-8') \
                .load()

            logging.info(
                "get_count_from_oracle_pds :: Get total count from PDS oracle server for table " + self.table_name + "  End",
                extra=CustomLogging.job.add_data())

            return int(self.read_df.collect()[0][0])

        except Exception as readerror:
            logging.error(
                "get_count_from_oracle_pds :: While Getting total count from PDS oracle server for table " + self.table_name + "  : " +
                str(readerror), extra=CustomLogging.job.add_data())
            raise readerror

    def get_count_from_awd_sql(self):
        try:
            logging.info(
                "get_count_from_awd :: Get total count from AWD sql server for table " + self.table_name + "  Start",
                extra=CustomLogging.job.add_data())

            self.read_df = self.spark_session.read.format("jdbc") \
                .option('url', self.sql_con_prop['url']) \
                .option('query', self.read_count_query) \
                .option('user', self.sql_con_prop['user']) \
                .option('password', self.sql_con_prop['password']) \
                .option('driver', self.sql_con_prop['driver']) \
                .option("fetchsize", "50000") \
                .option('encoding', 'UTF-8') \
                .load()

            logging.info(
                "get_count_from_awd_sql :: Get total count from AWD sql server for table " + self.table_name + "  End",
                extra=CustomLogging.job.add_data())

            return self.read_df.collect()[0][0]

        except Exception as readerror:
            logging.error(
                "get_count_from_awd_sql :: While Getting total count from AWD sql server for table " + self.table_name + "  : " +
                str(readerror), extra=CustomLogging.job.add_data())
            raise readerror



read_wiret.py
+++++++

import os
import pendulum
from datetime import datetime, timedelta
from airflow.models import DAG, SlaMiss, TaskInstance, Variable, DagRun, SkipMixin
from airflow.operators.bash_operator import BashOperator
from airflow.operators.python_operator import PythonOperator
from airflow.utils.trigger_rule import TriggerRule
# from airflow.utils.email import send_email
from control_frame.control_framework.send_mail import send_email
import subprocess
import traceback
from pytz import timezone
import re
from airflow import settings
from airflow.operators.dummy_operator import DummyOperator
import signal
import json
from pytz import timezone
from datetime import datetime as dt
from builtins import str, Exception
from typing import Iterable
from airflow.hooks.base_hook import BaseHook
from airflow.sensors.base_sensor_operator import BaseSensorOperator
from airflow.utils.decorators import apply_defaults
from airflow.exceptions import AirflowException
from control_frame.control_framework.api import api
# from control_framework_ida.api import api
import boto3
import boto3.session
from botocore.config import Config
import psycopg2
import time
import uuid
import logging
import requests
from airflow.providers.amazon.aws.operators.emr import EmrContainerOperator
from airflow.providers.amazon.aws.sensors.emr import EmrContainerSensor

from airflow.operators.dagrun_operator import TriggerDagRunOperator


class JSONObject:
    def __init__(self, dicts):
        vars(self).update(dicts)


dag_config = Variable.get("BATCH_DAG", deserialize_json=True)
inputs_ask = dag_config['IDA_SNOW_INCIDENT_CONFIG']
log_s3path = dag_config['log_s3path']
checkControls = dag_config['check_controls']
VIRTUAL_CLUSTER_ID = dag_config["STREAMING_VIRTUAL_CLUSTER_ID"]
JOB_ROLE = dag_config['JOB_ROLE']
load_type = dag_config['load_type']
airflow_config = dag_config['airflow_config']


def notify_email(**context):
    session = settings.Session()
    current_dag = context['dag_run'].dag_id
    print(current_dag)
    title = str(env).upper() + " Airflow alert: " + current_dag
    email_task_row = """
                            <tr>
                          <td style="border: 1px solid white; padding: 5px; font-family: Arial, sans-serif; font-size: 16px; line-height: 20px; width: 10.3001%; background-color: #99ccff; vertical-align: middle; text-align: left;" valign="top"><span style="font-family: Verdana,Geneva,sans-serif;"><span style="font-size: 14px;">{TASK_NAME}</span></span></td>
                          <td style="border: 1px solid white; padding: 5px; font-family: Arial, sans-serif; font-size: 16px; line-height: 20px; width: 9.4843%; background-color: {TASK_STATUS_COLOR}; vertical-align: middle; text-align: left;" valign="top"><span style="font-family: Verdana,Geneva,sans-serif;"><span style="font-size: 14px;">{TASK_STATUS}</span></span></td>
                          <td style="border: 1px solid white; width: 13.4615%; background-color: #99ccff; vertical-align: middle; text-align: left;"><span style="font-family: Verdana,Geneva,sans-serif;"><span style="font-size: 14px;"><span style="color: #000000;">&nbsp;{TASK_SEV}</span></span></span></td>
                          <td style="border: 1px solid white; width: 13.4615%; background-color: #99ccff; vertical-align: middle; text-align: left;"><span style="font-family: Verdana,Geneva,sans-serif;"><span style="font-size: 14px;"><span style="color: #000000;">&nbsp;&nbsp;</span><span style="color: #54acd2;">&nbsp;&nbsp;</span><span style="color: #0033cc;">&nbsp;
                                  <a href="{LOG_URL}">Log-Link</a>&nbsp; &nbsp; &nbsp; &nbsp;
                                </span><span style="color: #54acd2;">&nbsp; &nbsp;&nbsp;</span></span></span></td>
                          <td style="border: 1px solid white; width: 13.7675%; background-color: #99ccff; vertical-align: middle; text-align: left;"><span style="font-family: Verdana,Geneva,sans-serif;"><span style="font-size: 14px;">{TASK_START_TIME}</span></span></td>
                          <td style="border: 1px solid white; width: 13.4615%; background-color: #99ccff; vertical-align: middle; text-align: left;"><span style="font-family: Verdana,Geneva,sans-serif;"><span style="font-size: 14px;"><span style="color: #000000;">{TASK_END_TIME}</span></span></span></td>
                          <td style="border: 1px solid white; width: 13.8695%; background-color: {TASK_SLA_COLOR}; vertical-align: middle; text-align: left;"><span style="font-family: Verdana,Geneva,sans-serif;"><span style="font-size: 14px;"><span style="color: #000000;">{TASK_EXE_TIME}</span></span></span></td>
                          <td style="border: 1px solid white; width: 12.1212%; background-color: #99ccff; vertical-align: middle; text-align: left;"><span style="font-family: Verdana,Geneva,sans-serif;"><span style="font-size: 14px;">{TASK_SLA}</span></span></td>
                        </tr>
                    """

    email_body = """
                    <p>Hi,&nbsp;</p>
                    <p>&nbsp; &nbsp; Please review the below Airflow Job status and take action if required,</p>
                    <p>
                      <br>
                    </p>
                    <p><b>{UPSTREAM_FAILURE_MSG}</b><br></p>
                      <table cellpadding="0" cellspacing="0" style="min-width: 100%; border: 1px solid white; border-collapse: collapse;" width="50%">
                      <thead>
                        <tr>
                          <th colspan="2" style="border: 1px solid white; padding: 5px; background-color: #000000; vertical-align: middle;"><span style="font-family: Verdana,Geneva,sans-serif;">
                              <br><span style="font-size: 14px;"><span style="color: #ffffff;">Airflow DAG Run Details</span></span>
                              <br>
                              <br>
                            </span></th>
                        </tr>
                      </thead>
                      <tbody>
                        <tr>
                          <td style="border: 1px solid white; padding: 5px; width: 30.4802%; background-color: #606060; vertical-align: middle; text-align: left;"><span style="font-family: Verdana,Geneva,sans-serif;"><span style="font-size: 14px;"><span style="color: #ffffff;"><strong>DAG Name :</strong></span></span></span></td>      <td style="border: 1px solid white; padding: 5px; width: 69.311%; background-color: #efefef; vertical-align: middle; text-align: left;"><span style="font-family: Verdana,Geneva,sans-serif;"><span style="font-size: 14px;">{DAG_Name}</span></span></td>
                        </tr>
                        <tr>
                          <td style="border: 1px solid white; padding: 5px; width: 39.2484%; background-color: rgb(96, 96, 96); vertical-align: middle; text-align: left;"><span style="color: rgb(255, 255, 255);"><strong>DAG Status :</strong></span></td>
                          <td style="border: 1px solid white; padding: 5px; width: 60.5428%; background-color: {DAG_STATUS_COLOR}; vertical-align: middle; text-align: left;">{DAG_STATUS}</td>
                        </tr>
                        <tr>
                          <td style="border: 1px solid white; padding: 5px; width: 30.4802%; background-color: #606060; vertical-align: middle; text-align: left;"><span style="font-family: Verdana,Geneva,sans-serif;"><span style="font-size: 14px;"><span style="color: #ffffff;"><strong>DAG Start Time :</strong></span></span></span></td>
                          <td style="border: 1px solid white;width: 69.311%; background-color: #efefef; vertical-align: middle; text-align: left;"><span style="color: rgb(0, 0, 0);">{DAG_Start_Time}</span>
                            <br>
                          </td>
                        </tr>
                        <tr>
                          <td style="border: 1px solid white; padding: 5px; width: 30.4802%; background-color: #606060; vertical-align: middle; text-align: left;"><span style="font-family: Verdana,Geneva,sans-serif;"><span style="font-size: 14px;"><span style="color: #ffffff;"><strong>DAG End Time :</strong></span></span></span></td>
                          <td style="border: 1px solid white;width: 69.311%; background-color: #efefef; vertical-align: middle; text-align: left;"><span style="color: rgb(0, 0, 0);">{DAG_End_Time}</span>
                            <br>
                          </td>
                        </tr>
                        <tr>
                          <td style="border: 1px solid white; padding: 5px; width: 39.2484%; background-color: rgb(96, 96, 96); vertical-align: middle; text-align: left;"><strong><span style="color: rgb(255, 255, 255);">DAG Run Time (Mins) :</span></strong></td>
                          <td style="border: 1px solid white; width: 60.5428%; background-color: {DAG_SLA_COLOR}; vertical-align: middle; text-align: left;">{DAG_Run_Time}</td>
                        </tr>
                        <tr>
                          <td style="border: 1px solid white; padding: 5px; width: 39.2484%; background-color: rgb(96, 96, 96); vertical-align: middle; text-align: left;"><span style="color: rgb(255, 255, 255);"><strong>DAG SLA (Mins):</strong></span></td>
                          <td style="border: 1px solid white; width: 60.5428%; background-color: rgb(239, 239, 239); vertical-align: middle; text-align: left;">{DAG_SLA}</td>
                        </tr>
                        <tr>
                          <td style="border: 1px solid white; padding: 5px; width: 30.4802%; background-color: #606060; vertical-align: middle; text-align: left;"><span style="font-family: Verdana,Geneva,sans-serif;"><span style="font-size: 14px;"><span style="color: #ffffff;"><strong>Env:</strong></span></span></span></td>
                          <td style="border: 1px solid white;width: 69.311%; background-color: #efefef; vertical-align: middle; text-align: left;"><span style="font-family: Verdana,Geneva,sans-serif;"><span style="font-size: 14px; color: rgb(0, 0, 0);">{ENV}</span></span></td>
                        </tr>
                        <tr>
                          <td style="border: 1px solid white; padding: 5px; width: 30.4802%; background-color: #606060; vertical-align: middle; text-align: left;"><span style="font-family: Verdana,Geneva,sans-serif;"><span style="font-size: 14px;"><span style="color: #ffffff;"><strong>Affected Apps:</strong></span></span></span></td>
                          <td style="border: 1px solid white;width: 69.311%; background-color: #efefef; vertical-align: middle; text-align: left;"><span style="font-family: Verdana,Geneva,sans-serif;"><span style="font-size: 14px; color: rgb(0, 0, 0);">{APPS}</span></span></td>
                        </tr>
                        <tr>
                          <td style="border: 1px solid white; padding: 5px; width: 30.4802%; background-color: #606060; vertical-align: middle; text-align: left;"><span style="font-family: Verdana,Geneva,sans-serif;"><span style="font-size: 14px;"><span style="color: #ffffff;"><strong>Contact:</strong></span></span></span></td>
                          <td style="border: 1px solid white; padding: 5px; width: 69.311%; background-color: #efefef; vertical-align: middle; text-align: left;"><span style="font-family: Verdana,Geneva,sans-serif;"><span style="font-size: 14px; color: rgb(0, 0, 0);">{Contact}</span></span></td>
                        </tr>
                        <tr>
                          <td style="border: 1px solid white; padding: 5px; width: 30.4802%; background-color: #606060; vertical-align: middle; text-align: left;"><strong><span style="color: rgb(255, 255, 255);">DAG URL:</span></strong></td>
                          <td style="border: 1px solid white; padding: 5px; width: 69.311%; background-color: #efefef; vertical-align: middle; text-align: left;">
                            <a href="{DAG_Link}">Link</a>
                            </td>
                        </tr>
                        <tr>	
                          <td style="border: 1px solid white; padding: 5px; width: 30.4802%; background-color: #606060; vertical-align: middle; text-align: left;"><span style="font-family: Verdana,Geneva,sans-serif;"><span style="font-size: 14px;"><span style="color: #ffffff;"><strong>Incident Number:</strong></span></span></span></td>
                          <td style="border: 1px solid white;width: 69.311%; background-color: #efefef; vertical-align: middle; text-align: left;"><span style="font-family: Verdana,Geneva,sans-serif;"><span style="font-size: 14px; color: rgb(0, 0, 0);">{INCIDENT}</span></span>
                            <br>
                          </td>
                        </tr>
                      </tbody>
                    </table>
                    <p><span style="font-family: Verdana,Geneva,sans-serif;"><span style="font-size: 14px;">
                          <br>
                        </span></span></p>
                    <table cellpadding="0" cellspacing="0" style="min-width: 100%; border: 1px solid white; border-collapse: collapse;" width="100%">
                      <thead>
                        <tr>
                          <th colspan="8" scope="col" style="border: 1px solid white; padding: 5px; font-family: Arial, sans-serif; font-size: 16px; line-height: 30px; width: 99.7669%; background-color: #28324e;"><span style="font-family: Verdana,Geneva,sans-serif;"><span style="font-size: 14px;"><span style="color: #ffffff;">Task Run Status
                                  <br>
                                </span></span></span></th>
                        </tr>
                        <tr>
                          <th scope="col" style="border: 1px solid white; padding: 5px; font-family: Arial, sans-serif; font-size: 16px; line-height: 30px; width: 10.3001%; background-color: #28324e;"><span style="font-family: Verdana,Geneva,sans-serif;"><span style="font-size: 14px;"><span style="color: #ffffff;">Task Name
                                  <br>
                                </span></span></span></th>
                          <th scope="col" style="border: 1px solid white; padding: 5px; font-family: Arial, sans-serif; font-size: 16px; line-height: 30px; width: 9.4843%; background-color: #28324e;"><span style="font-family: Verdana,Geneva,sans-serif;"><span style="font-size: 14px;"><span style="color: #ffffff;">Status
                                  <br>
                                </span></span></span></th>
                          <th style="border: 1px solid white; width: 13.4615%; background-color: #28324e;"><span style="font-family: Verdana,Geneva,sans-serif;"><span style="font-size: 14px;"><span style="color: #ffffff;">Severity
                                  <br>
                                </span></span></span></th>
                          <th style="border: 1px solid white; width: 13.4615%; background-color: #28324e;"><span style="font-family: Verdana,Geneva,sans-serif;"><span style="font-size: 14px;"><span style="color: #ffffff;">Log URL
                                  <br>
                                </span></span></span></th>
                          <th style="border: 1px solid white; width: 13.7675%; background-color: #28324e;"><span style="font-family: Verdana,Geneva,sans-serif;"><span style="font-size: 14px;"><span style="color: #ffffff;">Start Time</span></span></span></th>
                          <th style="border: 1px solid white; width: 13.4615%; background-color: #28324e;"><span style="font-family: Verdana,Geneva,sans-serif;"><span style="font-size: 14px;"><span style="color: #ffffff;">End Time</span></span></span></th>
                          <th style="border: 1px solid white; width: 13.8695%; background-color: #28324e;"><span style="font-family: Verdana,Geneva,sans-serif;"><span style="font-size: 14px;"><span style="color: #ffffff;">Execution Time(Mins)
                                  <br>
                                </span></span></span></th>
                          <th style="border: 1px solid white; width: 12.1212%; background-color: #28324e;"><span style="font-family: Verdana,Geneva,sans-serif;"><span style="font-size: 14px;"><span style="color: #ffffff;">SLA (Mins)</span></span></span></th>
                        </tr>
                      </thead>
                      <tbody>
                      {EMAIL_TASK_ROW}
                      </tbody>
                    </table>	
                    <p>
                      <br>
                    </p>
                    <p>****** Note: All DateTime  are in UTC ******</p>
                    <p>****** This is an auto generated email. Do Not Reply to this email ******</p>
                    <p>&nbsp;</p>
                    <p>Regards,</p>
                    <p>Airflow Admin .</p>
                    <p>
                      <br>
                    </p>


                """
    entries_f = session.query(TaskInstance).filter(TaskInstance.dag_id == current_dag,
                                                   TaskInstance.execution_date == context['execution_date'],
                                                   TaskInstance.state.in_(['failed', 'upstream_failed'])) \
        .order_by(TaskInstance.state).all()
    entries_s = session.query(TaskInstance).filter(TaskInstance.dag_id == current_dag,
                                                   TaskInstance.execution_date == context['execution_date'],
                                                   TaskInstance.state.notin_(['failed', 'upstream_failed'])) \
        .order_by(TaskInstance.state).all()
    entries = entries_f + entries_s
    task_alert_ind = False
    sla_alert_ind = False
    alert_ind = False
    dag_url = ''
    email_task_rows = ''
    control_failure_msg = ''
    upstream_failure_ind = False
    for tsk in entries:
        print(tsk)
        task_name = str(tsk.task_id)
        if task_name != 'TASK_Send_Email':
            task_status = str(tsk.state)
            task_start_time = str(tsk.start_date)
            task_end_time = str(tsk.end_date)
            if tsk.duration is not None:
                task_duration = str(round(float(tsk.duration) / 60, 2))
            else:
                task_duration = '-1'
            task_url = str(tsk.log_url)
            dag_url = task_url
            if task_status == 'success':
                task_status_color = '#41a85f'
            elif task_status == 'failed' or task_status == 'upstream_failed' or task_status == 'shutdown':
                task_alert_ind = True
                task_status_color = '#b8312f'
            else:
                task_status_color = '#ff9900'
            if float(task_duration) < float(all_task_sla):
                task_sla_color = '#41a85f'
            else:
                sla_alert_ind = True
                task_sla_color = '#b8312f'
            email_task_rows = email_task_rows + email_task_row.replace('{TASK_NAME}', task_name). \
                replace('{TASK_STATUS}', task_status). \
                replace('{TASK_STATUS_COLOR}', task_status_color). \
                replace('{TASK_SEV}', 'high'). \
                replace('{LOG_URL}', task_url). \
                replace('{TASK_START_TIME}', task_start_time). \
                replace('{TASK_END_TIME}', task_end_time). \
                replace('{TASK_EXE_TIME}', task_duration). \
                replace('{TASK_SLA}', str(all_task_sla)). \
                replace('{TASK_SLA_COLOR}', task_sla_color)

    entries = session.query(DagRun).filter(DagRun.dag_id == current_dag,
                                           DagRun.execution_date == context['execution_date']).all()
    for tsk in entries:
        print(tsk)
        dag_start_time = str(tsk.start_date)
        dag_end_time = str(tsk.end_date)
        dag_status = str(tsk.state)
        print(datetime.utcnow())
        print(tsk.start_date)
        dag_dutation = round((datetime.utcnow() - tsk.start_date.replace(tzinfo=None)).total_seconds() / 60, 2)
        print(dag_dutation)
        print(type(dag_dutation))
        if float(dag_dutation) < float(dag_sla):
            dag_sla_color = 'rgb(65, 168, 95)'
        else:
            alert_ind = True
            sla_alert_ind = True
            dag_sla_color = 'rgb(184, 49, 47)'
        if dag_status == 'running' and task_alert_ind is False and sla_alert_ind is False:
            dag_status = 'success'
            dag_color = 'rgb(65, 168, 95)'
        elif sla_alert_ind is True and task_alert_ind is False:
            dag_status = 'warning'
            alert_ind = True
            dag_color = 'rgb(255,140,0)'
        else:
            dag_status = 'failed'
            alert_ind = True
            dag_color = 'rgb(184, 49, 47)'
        dag_url = dag_url[:dag_url.index(current_dag)]
        dag_url = re.sub(r"/log", "/graph", dag_url + current_dag)
        email_body = email_body.replace('{DAG_Name}', current_dag).replace('{DAG_Start_Time}', dag_start_time). \
            replace('{DAG_End_Time}', dag_end_time).replace('{DAG_SLA}', str(dag_sla)). \
            replace('{DAG_SLA_COLOR}', dag_sla_color). \
            replace('{DAG_STATUS_COLOR}', dag_color). \
            replace('{DAG_STATUS}', dag_status). \
            replace('{ENV}', env). \
            replace('{APPS}', affected_apps). \
            replace('{Contact}', poc). \
            replace('{DAG_Run_Time}', str(dag_dutation)). \
            replace('{DAG_Link}', dag_url).replace('{EMAIL_TASK_ROW}', email_task_rows)
        break

    if upstream_failure_ind:
        control_failure_msg = "No record present in the IDW Control Batch table. Manual intervention is needed to trigger the DAG."
    else:
        control_failure_msg = ""
    logging.info("control_failure_msg : " + str(control_failure_msg))
    email_body = email_body.replace('{UPSTREAM_FAILURE_MSG}', control_failure_msg)

    entries_f = session.query(TaskInstance).filter(TaskInstance.dag_id == current_dag,
                                                   TaskInstance.execution_date == context['execution_date'],
                                                   TaskInstance.state.in_(['failed', 'upstream_failed'])) \
        .order_by(TaskInstance.start_date).all()
    entries_s = session.query(TaskInstance).filter(TaskInstance.dag_id == current_dag,
                                                   TaskInstance.execution_date == context['execution_date'],
                                                   TaskInstance.state.notin_(['failed', 'upstream_failed'])) \
        .order_by(TaskInstance.start_date).all()
    entries = entries_f + entries_s

    # csv header
    attachment_task_rows = 'Task_Name,Status,Severity,Log_URL,Start_Time,End_Time,SLA' + '\r\n'
    delimiter = ','

    # csv data rows
    for tsk in entries:
        print(tsk)
        task_name = str(tsk.task_id)
        if task_name != 'TASK_Send_Email':
            attachment_task_rows = attachment_task_rows + \
                                   str(tsk.task_id) + delimiter + \
                                   str(tsk.state) + delimiter + \
                                   'high' + delimiter + \
                                   str(tsk.log_url) + delimiter + \
                                   str(tsk.start_date) + delimiter + \
                                   str(tsk.end_date) + delimiter + \
                                   str(all_task_sla) + delimiter + '\r\n'

    # temp directory
    ec2_staging_dir = "/usr/local/airflow/_fsmount/tmp/"

    # create temp dir to store file
    out = subprocess.Popen(['mkdir', '-p', ec2_staging_dir],
                           stdout=subprocess.PIPE,
                           stderr=subprocess.STDOUT)
    stdout, stderr = out.communicate()
    wait_status = out.wait()
    print(stdout)
    print(stderr)
    print(str(wait_status))

    # csv file name
    filename = str(tsk.dag_id) + '_' + str(tsk.start_date)[:19].replace(' ', '_').replace(':', '_') + '.csv'

    # create file paths
    stage_file_location = ec2_staging_dir + "/" + filename

    # save file txt
    with open(stage_file_location, "w") as text_file:
        text_file.write(attachment_task_rows)

    # send email with attachment
    if alert_ind is True:
        # All SNOW ticket creation and adding to Incident number to mail starts from here
        try:
            if inputs_ask['ask_enable']:
                session = boto3.session.Session()
                sm_boto_config = Config(connect_timeout=120, read_timeout=120, retries={'max_attempts': 10})
                sm_client = session.client('secretsmanager', region_name='us-east-1', config=sm_boto_config)
                secret_response = sm_client.get_secret_value(SecretId=inputs_ask['ask_secret_id'])
                secret_value = json.loads(secret_response['SecretString'])
                ask_desc = current_dag + " has failed"
                body_var = "{\"Assignment Group\": \"" + inputs_ask[
                    'ask_assign_group'] + "\", \"Configuration Item\": \"" + \
                           inputs_ask['ask_config_item'] + "\", \"Contact\": \"" + inputs_ask[
                               'ask_contact'] + "\", \"Description\": \"" + ask_desc + "\", \"Impact\": \"" + \
                           inputs_ask[
                               'ask_impact'] + "\", \"Short Description\": \"" + ask_desc + "\", \"Usage Designation\": \"" + \
                           inputs_ask['ask_usage_d'] + "\",\"Urgency\": \"" + inputs_ask[
                               'ask_urgency'] + "\", \"Category\": \"" + inputs_ask[
                               'ask_category'] + "\", \"Subcategory\": \"" + inputs_ask['ask_subcategory'] + "\"}"
                ask_body = {"values[IncidentData]": body_var}
                ask_url = inputs_ask['ask_url']
                ask_response = requests.post(ask_url, data=ask_body,
                                             auth=(secret_value['ASKAPI']['UNAME'], secret_value['ASKAPI']['PWD']))
                print("Status is: " + str(ask_response.status_code))
                print(ask_response.content)
                if int(ask_response.status_code) == 200:
                    ask_json = ask_response.json()
                    print(ask_json)
                    print(json.dumps(ask_json['record']['attributes']['Results']))
                    final_ask = eval(ask_json['record']['attributes']['Results'])
                    print("Status: " + final_ask['Status'])
                    print("Incident Number: " + final_ask['Incident'])
                    incident_msg = final_ask['Incident']
                else:
                    print("Error in Create Incident API")
                    incident_msg = "Error in Create Incident API"
            else:
                print("Incident Creation is Disabled for this env.")
                incident_msg = "Disabled for this env."
        except Exception as e:
            print("Some Issue Occured while calling ASK API " + str(e))
            incident_msg = "Could not able to raise Incident by ASK API"
            print("Could not able to raise Incident by ASK API")
        email_body = email_body.replace('{INCIDENT}', incident_msg)

        # SNOW code ends here ------------------------------------------------------------
        send_email(alert_email_list, title, email_body, files=[stage_file_location])
    else:
        print('No Alert Email Sent')

    # delete csv file
    out = subprocess.Popen('rm -rf ' + stage_file_location, shell=True,
                           stdout=subprocess.PIPE,
                           stderr=subprocess.STDOUT)
    stdout, stderr = out.communicate()
    wait_status = out.wait()
    print(stdout)
    print(stderr)
    print(str(wait_status))


def get_config_file_values(path):
    try:
        with open(path) as files:
            config_file = json.load(files)
            files.close()
    except Exception as e:
        print("Could not open json file! Exiting! in individual_applications_domain_data_model_dag.py" + str(e))
        raise
    return config_file


class CustomShortCircuitOperator(PythonOperator, SkipMixin):
    """
    Allows a workflow to continue only if a condition is met. Otherwise, the
    workflow "short-circuits" and downstream tasks are skipped.

    The ShortCircuitOperator is derived from the PythonOperator. It evaluates a
    condition and short-circuits the workflow if the condition is False. Any
    downstream tasks are marked with a state of "skipped". If the condition is
    True, downstream tasks proceed as normal.

    The condition is determined by the result of `python_callable`.
    """

    def execute(self, context):
        condition = super(CustomShortCircuitOperator, self).execute(context)
        self.log.info("Condition result is %s", condition)

        if condition:
            self.log.info('Proceeding with downstream tasks...')
            return

        self.log.info('Skipping downstream tasks...')

        downstream_tasks = context['task'].get_flat_relatives(upstream=False)
        self.log.info("Downstream task_ids %s", downstream_tasks)

        # skip all downstream tasks except terminate emr cluster
        downstream_tasks_to_skip = list()
        for task in downstream_tasks:
            print(task.task_id)
            task_id = task.task_id
            if task_id not in ["TASK_terminate_cluster", "TASK_Send_Email"]:
                downstream_tasks_to_skip.append(task)
        self.log.info("Downstream task_ids filtered %s", downstream_tasks_to_skip)

        if downstream_tasks_to_skip:
            self.skip(context['dag_run'], context['ti'].execution_date, downstream_tasks_to_skip)

        self.log.info("Done.")


def create_python_operator(dag, task_name, python_callable, trigger_rule, weight_rule, op_kwargs):
    task_operator = PythonOperator(
        task_id=task_name,
        python_callable=python_callable,
        provide_context=True,
        op_kwargs=op_kwargs,
        weight_rule=weight_rule,
        trigger_rule=trigger_rule,
        pool=pool_name,
        dag=dag)
    return task_operator


def generate_timestamp_container_tag(base_image, prefix="emergency-fix"):
    """
    Generate timestamp-based container image tag to bypass EMR caching.
    
    This function creates unique container image tags using Unix timestamps,
    ensuring EMR pulls fresh containers on each execution.
    
    Args:
        base_image (str): Base container image path without tag
        prefix (str): Prefix for the tag (default: "emergency-fix")
    
    Returns:
        str: Complete container image path with timestamp tag
        
    Example:
        generate_timestamp_container_tag(
            "123456789012.dkr.ecr.us-east-1.amazonaws.com/ta-individual-spark",
            "pds-delete"
        )
        # Returns: "123456789012.dkr.ecr.us-east-1.amazonaws.com/ta-individual-spark:pds-delete-1703123456"
    """
    timestamp = str(int(time.time()))
    return f"{base_image}:{prefix}-{timestamp}"


def spark_configurations(value, eks_configs, dag_configs):
    JOB_DRIVER_ARG = {
        "sparkSubmitJobDriver": {
            "entryPoint": "s3://ta-individual-idw-dev-airflowdags/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/scripts/ida/src/streaming_apps/stream_to_datalake/main/streaming_main.py",
            "entryPointArguments": [
                "--model", "PDS_CUSTOM_DELETE",
                "--env", eks_configs['env'],
                "--load_type", "DELETE",
                "--log_s3_bucket", log_s3path,
                "--secret_id", secret_id,
                "--dynamo_table", "ida-policy-data",
                "--batch_id", "{{ ds }}",
                "--pipeline_id", "pds_delete_pipeline",
                "--partition_key", "plcy_nbr",
                "--checkControls", checkControls,
                "--ida_batch_id", "{{ ds }}",
                "--ida_pipeline_id", "pds_delete_ida_pipeline",
                "--VIRTUAL_CLUSTER_ID", VIRTUAL_CLUSTER_ID,
                "--arn", role_arn
            ],
            "sparkSubmitParameters": eks_configs['sparkSubmitParameters']
        }
    }

    CONFIGURATION_OVERRIDES_ARG = {
        "applicationConfiguration": [{
            "classification": "spark-defaults",
            "properties": {
                "spark.executor.memory": value['spark_executor_memory'],
                "spark.executor.cores": value['spark_executor_cores'],
                "spark.driver.memory": value['spark_driver_memory'],
                "spark.driver.cores": value['spark_driver_cores'],
                "spark.kubernetes.container.image": generate_timestamp_container_tag(eks_configs['spark_kubernetes_container_image'], "emergency-fix"),
                "spark.kubernetes.pyspark.pythonVersion": '3',
                "spark.serializer": "org.apache.spark.serializer.KryoSerializer",
                "spark.dynamicAllocation.enabled": 'true',
                "spark.dynamicAllocation.shuffleTracking.enabled": 'true',
                "spark.dynamicAllocation.minExecutors": value['spark_dynamicAllocation_minExecutors'],
                "spark.dynamicAllocation.maxExecutors": value['spark_dynamicAllocation_maxExecutors'],
                "spark.dynamicAllocation.initialExecutors": value['spark_dynamicAllocation_initialExecutors'],
                "spark.sql.hive.convertMetastoreParquet": 'false',
                "spark.sql.warehouse.dir": eks_configs['spark_sql_warehouse_dir'],
                "spark.hadoop.hive.metastore.client.factory.class": "com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory",
                "spark.streaming.stopGracefullyOnShutdown": "true",
                "spark.streaming.kinesis.retry.maxAttempts":"3",
                "spark.streaming.kinesis.retry.waitTime":"100ms",
                "spark.scheduler.listenerbus.eventqueue.capacity":"500000"

            }
        },
        {
                "classification": "spark-log4j",
                "properties": {
                    # Define some default values that can be overridden by system properties
                    "spark.yarn.app.container.log.dir": "/var/log/spark/user/${user.name}",

                    # Set everything to be logged to the console ok
                    "log4j.rootCategory": "ERROR,console",
                    "log4j.appender.console": "org.apache.log4j.ConsoleAppender",
                    "log4j.appender.console.target": "System.err",
                    "log4j.appender.console.layout": "org.apache.log4j.PatternLayout",
                    "log4j.appender.console.layout.ConversionPattern": "%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n",

                    # Set the default spark streaming app log level to INFO. When running spark streaming app,
                    # the logs generated by this class will be rolled on hourly basis to avoid deletion
                    # of stdout and stderr logs by LogPusher on crossing the 4 hour retention period
                    "log4j.logger.org.apache.spark.streaming": "ERROR,DRFA-stderr,DRFA-stdout",
                    "log4j.appender.DRFA-stderr": "org.apache.log4j.DailyRollingFileAppender",
                    "log4j.appender.DRFA-stderr.file": "${spark.yarn.app.container.log.dir}/stderr",
                    "log4j.appender.DRFA-stderr.DatePattern": "'.'yyyy-MM-dd-HH",
                    "log4j.appender.DRFA-stderr.layout": "org.apache.log4j.PatternLayout",
                    "log4j.appender.DRFA-stderr.layout.ConversionPattern": "%d{ISO8601} %p [%t] %c:%m%n",
                    # Roll stdout logs
                    "log4j.appender.DRFA-stdout": "org.apache.log4j.DailyRollingFileAppender",
                    "log4j.appender.DRFA-stdout.file": "${spark.yarn.app.container.log.dir}/stdout",
                    "log4j.appender.DRFA-stdout.DatePattern": "'.'yyyy-MM-dd-HH",
                    "log4j.appender.DRFA-stdout.layout": "org.apache.log4j.PatternLayout",
                    "log4j.appender.DRFA-stdout.layout.ConversionPattern": "%d{ISO8601} %p [%t] %c:%m%n",

                    # Set the default spark-shell log level to WARN. When running the spark-shell, the
                    # log level for this class is used to overwrite the root logger's log level, so that
                    # the user can have different defaults for the shell and regular Spark apps.
                    "log4j.logger.org.apache.spark.repl.Main": "WARN",

                    # Settings to quiet third party logs that are too verbose
                    "log4j.logger.org.spark_project.jetty": "WARN",
                    "log4j.logger.org.spark_project.jetty.util.component.AbstractLifeCycle": "ERROR",
                    "log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper": "INFO",
                    "log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter": "INFO",
                    "log4j.logger.org.apache.parquet": "ERROR",
                    "log4j.logger.parquet": "ERROR",
                    "log4j.logger.org.apache.hudi": "WARN",

                    # SPARK-9183: Settings to avoid annoying messages when looking up nonexistent UDFs in SparkSQL with Hive support
                    "log4j.logger.org.apache.hadoop.hive.metastore.RetryingHMSHandler": "FATAL",
                    "log4j.logger.org.apache.hadoop.hive.ql.exec.FunctionRegistry": "ERROR"
                }
        }
        ],
        "monitoringConfiguration": {
            "persistentAppUI": 'ENABLED',
            "cloudWatchMonitoringConfiguration": {
                "logGroupName": eks_configs['cloudwatch_log_group'],
                "logStreamNamePrefix": value['table_name']
            },
            "s3MonitoringConfiguration": {
                "logUri": eks_configs['logUri']
            }
        }
    }
    return JOB_DRIVER_ARG, CONFIGURATION_OVERRIDES_ARG


def task_ids(table_name):
    if table_name == "HZN_ORACLE_FULL_LOAD":
        return "{{task_instance.xcom_pull(task_ids='TASK_HZN_ORACLE_FULL_LOAD_Data_Insert', key='return_value') }}"
    if table_name == "AWD_FULL_LOAD":
        return "{{task_instance.xcom_pull(task_ids='TASK_AWD_FULL_LOAD_Data_Insert', key='return_value') }}"
    if table_name == "PPLUS_DB2_FULL_LOAD":
        return "{{task_instance.xcom_pull(task_ids='TASK_PPLUS_DB2_FULL_LOAD_Data_Insert', key='return_value') }}"


def create_dag(dag_id, schedule_interval, dag_catchup, tag, default_args):
    dag = DAG(dag_id=dag_id, max_active_runs=1, catchup=dag_catchup, schedule_interval=schedule_interval,
              tags=tag, default_args=default_args)

    # Main Pending DAG
    start = DummyOperator(dag=dag, task_id="check_virtual_cluster")

    # Simple PDS Delete task configuration
    value = {
        "table_name": "PDS_DELETE",
        "spark_driver_memory": "5G",
        "spark_executor_memory": "10G",
        "spark_driver_cores": "1",
        "spark_executor_cores": "3",
        "spark_dynamicAllocation_minExecutors": "1",
        "spark_dynamicAllocation_maxExecutors": "10",
        "spark_dynamicAllocation_initialExecutors": "1"
    }
    
    taskid = "PDS_DELETE"
    job_driver_config, configuration_overrides_config = spark_configurations(value, configs_table_inputs['new_streaming_eks_configs'], configs_dag_inputs)
    JOB_ROLE_ARN = configs_table_inputs['new_streaming_eks_configs'][JOB_ROLE]
    
    job_starter = EmrContainerOperator(
        task_id='TASK_PDS_DELETE_Data_Insert',
        virtual_cluster_id=VIRTUAL_CLUSTER_ID,
        execution_role_arn=JOB_ROLE_ARN,
        release_label="emr-6.3.0-latest",
        job_driver=job_driver_config,
        configuration_overrides=configuration_overrides_config,
        name=taskid,
        client_request_token=uuid.uuid4().hex,
        wait_for_completion=False,
        retries=1,
        retry_delay=timedelta(minutes=5),
        dag=dag
    )

    job_checker = EmrContainerSensor(
        job_id="{{ task_instance.xcom_pull(task_ids='TASK_PDS_DELETE_Data_Insert') }}",
        task_id='TASK_PDS_DELETE_Data_Insert_Status',
        virtual_cluster_id=VIRTUAL_CLUSTER_ID,
        aws_conn_id="aws_default",
        retries=2,
        retry_delay=timedelta(minutes=5),
        timeout=3600,
        poke_interval=60,
        mode="poke",
        dag=dag
    )

    op_kwargs_email = {
        "secret_id": secret_id
    }

    task_email_notify = create_python_operator(dag, "TASK_Send_Email_PDS_DELETE", notify_email,
                                               TriggerRule.ONE_FAILED,
                                               "downstream", op_kwargs_email)

    start >> job_starter >> job_checker >> task_email_notify

    return dag


try:
    local_tz = pendulum.timezone("America/Chicago")
    central = timezone('US/Central')
    basedir_code = "/usr/local"
    affected_apps = 'TA Individual Applications'
    dag_sla = 480
    app_name = 'STREAMING_TO_DATALAKE_DELETE_COMPARE'

    airflow_configs_project = "airflow/dags/transamerica_individual"
    airflow_configs_repo = "transamerica_product_life_individual_oneida"
    airflow_configs_branch = "default"
    airflow_configs_ec2_base_location = "dags/airflow-configs/"
    
    # configs from airflow config files
    configs_dag_inputs = get_config_file_values(airflow_configs_ec2_base_location + "dag_configs.json")
    if airflow_config.lower() == 'new':
        configs_table_inputs = get_config_file_values(airflow_configs_ec2_base_location + "table-inputs_new.json")
    else:
        configs_table_inputs = get_config_file_values(airflow_configs_ec2_base_location + "table-inputs.json")
    log_s3_bucket = configs_dag_inputs['s3_bucket_names']['log']
    secret_id = configs_dag_inputs['secret-id']
    control_dynamodb_table = configs_dag_inputs['dynamo_db']['control']

    job_control_redshift = configs_dag_inputs['redshift_control']['job_control']
    job_control_redshift_hist = configs_dag_inputs['redshift_control']['job_control_hist']
    system_flow_redshift = configs_dag_inputs['redshift_control']['system_flow']
    system_flow_redshift_hist = configs_dag_inputs['redshift_control']['system_flow_hist']

    pool_name = configs_dag_inputs['pool_name']
    venv_activation_path = configs_dag_inputs['venv_activation_path']
    env = configs_dag_inputs['env']
    Dag_Name = 'DAG_INDV_APP_' + app_name +'_PDS_CUSTOM'

    alert_email_list = configs_dag_inputs['alert_email_list']
    dag_sla = configs_dag_inputs['dag_sla']
    all_task_sla = configs_dag_inputs['all_task_sla']
    affected_apps = configs_dag_inputs['affected_apps']
    poc = configs_dag_inputs['poc']
    redshift_s3_temp_dir = configs_dag_inputs['redshift_prop']['s3_temp_path']
    role_arn = configs_dag_inputs['dynamodb_roles']['role_arn']
    role_session_name = configs_dag_inputs['dynamodb_roles']['role_session_name']

    # airflow scripts project
    airflow_scripts_project = "airflow/dags/transamerica_individual"
    airflow_scripts_repo = "transamerica_product_life_individual_oneida"
    airflow_scripts_branch = "default"
    airflow_scripts_ec2_base_location = basedir_code + "/" + airflow_scripts_project + "/" \
                                        + airflow_scripts_repo + "/" + airflow_scripts_branch \
                                        + "/airflow-scripts/"

    # application scripts project
    application_scripts_project = "airflow/dags/transamerica_individual"
    application_scripts_project_s3 = "airflow/dag/transamerica_individual"
    application_scripts_repo = "transamerica_product_life_individual_oneida"
    application_scripts_branch = "default"
    application_scripts_ec2_base_location = basedir_code + "/" + application_scripts_project + "/" + application_scripts_repo + "/" \
                                            + application_scripts_branch + "/scripts"
    application_scripts_s3_base_location = "s3://" + configs_dag_inputs["s3_bucket_names"]["tech_common_deploy"] + "/" \
                                           + application_scripts_project_s3 + "/" + application_scripts_repo + "/" \
                                           + application_scripts_branch + "/scripts"
    application_scripts_zip_s3_location = application_scripts_s3_base_location + "/" + "app_ida.zip"

    # application configs project
    configs_for_application_project = "airflow/dag/transamerica_individual"
    configs_for_application_repo = "transamerica_product_life_individual_oneida"
    configs_for_application_branch = "default"
    configs_for_lake_scripts_s3_base_location = "s3://" + configs_dag_inputs["s3_bucket_names"][
        "deploy"] + "/" + configs_for_application_project + "/" + configs_for_application_repo + "/" + 'default'

    configs_for_lake_scripts_zip_s3_path = configs_for_lake_scripts_zip_s3_path = configs_for_lake_scripts_s3_base_location \
                                                                                  + "/" + "pyspark-scripts-configs" + "/" \
                                                                                  + env + "-pyspark-scripts-configs.zip"

    default_args = {
        'owner': configs_dag_inputs['owner'],
        'depends_on_past': configs_dag_inputs['depends_on_past'],
        'start_date': str(datetime.strptime(str(configs_dag_inputs['start_date']), '%Y-%m-%d')),
        'email': configs_dag_inputs['email'],
        'email_on_failure': configs_dag_inputs['email_on_failure'],
        'email_on_retry': configs_dag_inputs['email_on_retry'],
        'retries': configs_dag_inputs['retries'],
        'max_active_runs': configs_dag_inputs['max_active_runs'],
        'provide_context': configs_dag_inputs['provide_context']
    }
    schedule_interval = None
    dag_catchup = False

    schedule_interval = configs_dag_inputs["dm_arl_dag_schedule"]
    if schedule_interval.lower() == 'none':
        schedule_interval = None

    globals()[Dag_Name] = create_dag(Dag_Name, schedule_interval, dag_catchup, ['ida', 'emr_on_eks', 'streaming', 'new'],
                                     default_args)


except Exception as e:
    print("Exception while creating DAG " + str(e))
    print(str(e) + ": Traceback Log : " + traceback.format_exc())



indviudal_streimgn_pds_dataloading_pds_delete.py
========
import logging
from src.streaming_apps.config.ingestion_config import IngestionConfig
from src.streaming_framework.utility.common_utility.driver import Driver
from src.streaming_apps.stream_to_dynamodb.model.apah_bob_pds_service_model import ApahBobPdsModel

'''
DRIVER INJECTION FUNCTION FOR HORIZON BOB.
'''


class ApahBobPdsServiceDriver(Driver):

    def __init__(self, env, model):
        super().__init__(env, model)

    def apah_bob_pds_driver(self, es_vpc, opensearch_index_name):
        try:
            conf = IngestionConfig(self.env)
            properties = conf.get_apah_bob_pds_service_config()
            logging.info("Create kinesis read_stream.")
            dynamo_details = conf.get_dynamo_details()
            extra_param = conf.get_extra_parm()
            dynamo_db_table = extra_param.get('book-of-business-ddb')
            table_key = []
            if dynamo_details.get(dynamo_db_table).get('sort_key'):
                table_key.append(dynamo_details.get(dynamo_db_table).get('partition_key'))
                table_key.append(dynamo_details.get(dynamo_db_table).get('sort_key'))
            else:
                table_key.append(dynamo_details.get(dynamo_db_table).get('partition_key'))
            self.consumer.spark_session = self.spark_session
            self.consumer.spark_session.conf.set("spark.sql.autoBroadcastJoinThreshold", -1)
            self.consumer.stream_name = properties.get("CONSUMER").get("stream_name")
            self.consumer.end_point_url = properties.get("CONSUMER").get("endpoint_url")
            self.consumer.region = properties.get("CONSUMER").get("region")
            self.consumer.starting_position = properties.get("CONSUMER").get('starting_position')
            logging.info("Create kinesis read_stream.")
            read_stream = self.consumer.read_stream_from_kinesis()
            logging.info("{0} -> Setting objects.".format(str(properties['TABLE_LIST'])))
            bob_pds = ApahBobPdsModel()
            bob_pds.spark_session = self.spark_session
            bob_pds.lake_table_name = properties['TABLE_LIST']
            bob_pds.dynamodb_table_name = dynamo_db_table
            bob_pds.dynamo_table_key = table_key
            bob_pds.table_name = properties['TABLE_LIST']
            bob_pds.role_arn = extra_param.get('role_arn')
            bob_pds.role_session_name = extra_param.get('role_session_name')
            bob_pds.secret_id = extra_param.get('secret_id')
            bob_pds.s3_temp_dir = extra_param.get("s3_temp_dir")
            bob_pds.load_type = "DELTA"
            bob_pds.env = self.env
            bob_pds.es_url = es_vpc
            bob_pds.es_index = opensearch_index_name
            s3_checkpoint_location = properties.get('check_point').get('s3HudiCheckpointLocation')
            logging.info("{0} - Get stream for table.".format(str(properties['TABLE_LIST'])))
            read_stream.writeStream.foreachBatch(bob_pds.stream_runner) \
                .option("checkpointLocation", s3_checkpoint_location).start()
            self.spark_session.streams.awaitAnyTermination()
        except Exception as e:
            print(e)
            raise

    def full_apah_bob_pds_driver(self, es_vpc, opensearch_index_name):
        try:
            conf = IngestionConfig(self.env)
            properties = conf.get_apah_bob_pds_service_config()
            logging.info("Create kinesis read_stream.")
            dynamo_details = conf.get_dynamo_details()
            extra_param = conf.get_extra_parm()
            dynamo_db_table = extra_param.get('book-of-business-ddb')
            table_key = []
            if dynamo_details.get(dynamo_db_table).get('sort_key'):
                table_key.append(dynamo_details.get(dynamo_db_table).get('partition_key'))
                table_key.append(dynamo_details.get(dynamo_db_table).get('sort_key'))
            else:
                table_key.append(dynamo_details.get(dynamo_db_table).get('partition_key'))
            self.consumer.spark_session = self.spark_session
            self.consumer.spark_session.conf.set("spark.sql.autoBroadcastJoinThreshold", -1)
            self.consumer.stream_name = properties.get("CONSUMER").get("stream_name")
            self.consumer.end_point_url = properties.get("CONSUMER").get("endpoint_url")
            self.consumer.region = properties.get("CONSUMER").get("region")
            self.consumer.starting_position = properties.get("CONSUMER").get('starting_position')
            logging.info("Create kinesis read_stream.")
            read_stream = self.consumer.read_stream_from_kinesis()
            bob_pds = ApahBobPdsModel()
            bob_pds.spark_session = self.spark_session
            bob_pds.lake_table_name = 'FULL LOAD'
            bob_pds.dynamodb_table_name = dynamo_db_table
            bob_pds.dynamo_table_key = table_key
            bob_pds.table_name = 'FULL LOAD'
            bob_pds.role_arn = extra_param.get('role_arn')
            bob_pds.role_session_name = extra_param.get('role_session_name')
            bob_pds.secret_id = extra_param.get('secret_id')
            bob_pds.s3_temp_dir = extra_param.get("s3_temp_dir")
            bob_pds.load_type = "FULL"
            bob_pds.env = self.env
            bob_pds.es_url = es_vpc
            bob_pds.es_index = opensearch_index_name
            bob_pds.apha_bob_pds_ingestion('streaming_df', dynamo_db_table, table_key, extra_param.get('role_arn'),
                                           extra_param.get('role_session_name'), 'FULL', extra_param.get("s3_temp_dir"))
        except Exception as e:
            print(e)
            raise

    def pds_oracle_clob_delete_driver(self):
        try:
            from src.streaming_apps.stream_to_datalake.driver_methods.pds_oracle_delete_operations import PdsOracleDeleteOperations
            
            logging.info("Starting PDS Oracle CLOB Delete Driver")
            
            # Initialize delete operations
            delete_ops = PdsOracleDeleteOperations()
            delete_ops.spark_session = self.spark_session
            delete_ops.env = self.env
            
            # Execute delete operation
            delete_ops.delete_records_from_datalake()
            
            logging.info("PDS Oracle CLOB Delete Driver completed successfully")
            
        except Exception as e:
            logging.error(f"PDS Oracle CLOB Delete Driver failed: {e}")
            raise



apah_bob_pds_serivce_driver.py 

========

{
  "domain_data_model_task": [
    {
      "table_name": "weekly_apah_bob_pplus_service",
      "driver_memory":"9G",
      "executor_memory":"18G",
      "driver_cores": "1",
      "executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "500",
      "spark_dynamicAllocation_initialExecutors": "1",
       "partition_key": "pkey",
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
      "query_name": [],
      "sort_key": ""
    },
    {
      "table_name": "weekly_bob_p07",
      "driver_memory":"9G",
      "executor_memory":"18G",
      "driver_cores": "1",
      "executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "500",
      "spark_dynamicAllocation_initialExecutors": "1",
      "partition_key": "pkey",
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
      "query_name": [],
      "sort_key": ""
    },
    {
      "table_name": "weekly_apah_bob_hzn_pplus_service",
      "driver_memory":"9G",
      "executor_memory":"18G",
      "driver_cores": "1",
      "executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "500",
      "spark_dynamicAllocation_initialExecutors": "1",
       "partition_key": "pkey",
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
      "query_name": [],
      "sort_key": ""
    },
    {
      "table_name": "weekly_apah_bob_hzn_service",
      "driver_memory":"9G",
      "executor_memory":"18G",
      "driver_cores": "1",
      "executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "500",
      "spark_dynamicAllocation_initialExecutors": "1",
       "partition_key": "pkey",
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
      "query_name": [],
      "sort_key": ""
    },
    { "table_name": "copy-dim-product",
      "partition_key": "pkey",
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
      "query_name": [],
      "executor_memory": "4G",
      "driver_memory": "2G",
      "sort_key": "",
      "executor_cores": "1",
      "driver_cores": "1",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "5",
      "spark_dynamicAllocation_initialExecutors": "1",
      "source_schema_name": "DBO"
    },
    {
      "table_name": "PDS_ORACLE_CLOB_DELETE",
      "spark_driver_memory": "4G",
      "spark_executor_memory": "8G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "2",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "10",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "table_name": "policy_85_financial",
      "query_name": [],
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
      "partition_key": "pkey",
      "sort_key": "",
      "executor_memory": "7G",
      "driver_memory": "3G",
      "executor_cores":"2",
      "driver_cores": "1",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "5",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "table_name": "agent",
      "query_name": [],
      "dynamo_table": "ta-individual-data-service-domains-dev-agent",
      "partition_key": "advisor_agent_id",
      "sort_key": "source_system",
      "executor_memory": "4G",
      "driver_memory": "2G",
      "executor_cores": "1",
      "driver_cores": "1"
    },
    {
      "table_name": "policy-segment",
      "query_name": [],
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
      "partition_key": "pkey",
      "sort_key": "",
      "executor_memory": "8G",
      "driver_memory": "6G",
      "executor_cores":"1",
      "driver_cores": "1"
    },
    {
      "table_name": "nic_xml_feed",
      "query_name": [],
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
      "partition_key": "pkey",
      "sort_key": "",
      "executor_memory": "15G",
      "driver_memory": "12G",
      "executor_cores":"1",
      "driver_cores": "1"
    },
    {
      "table_name": "nic_xml_feed_sftp",
      "query_name": [],
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
      "partition_key": "pkey",
      "sort_key": "",
      "executor_memory": "8G",
      "driver_memory": "5G",
      "executor_cores":"1",
      "driver_cores": "1"
    },
    {
      "table_name": "delta",
      "query_name": [],
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy-delta",
      "partition_key": "policy_num",
      "sort_key": "",
      "executor_memory": "4G",
      "driver_memory": "2G",
      "executor_cores":"1",
      "driver_cores": "1"
    },
    {
      "table_name": "policy-beneficiary-segment",
      "query_name": [],
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
      "partition_key": "pkey",
      "sort_key": "",
      "executor_memory": "4G",
      "driver_memory": "2G",
      "executor_cores":"1",
      "driver_cores": "1"
    },
    {
      "table_name": "policy-activity-segment",
      "query_name": [],
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
      "partition_key": "pkey",
      "sort_key": "",
      "executor_memory": "8G",
      "driver_memory": "2G",
      "executor_cores":"2",
      "driver_cores": "1",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "5",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "table_name": "policy-activity-segment-clean-up",
      "query_name": [],
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
      "partition_key": "pkey",
      "sort_key": "",
      "executor_memory": "18G",
      "driver_memory": "9G",
      "executor_cores":"3",
      "driver_cores": "1",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "100",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "table_name": "policy-riders-segment",
      "query_name": [],
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
      "partition_key": "pkey",
      "sort_key": "",
      "executor_memory": "5G",
      "driver_memory": "2G",
      "executor_cores":"2",
      "driver_cores": "1"
    },
    {
      "table_name": "policy-45-customer",
      "query_name": [],
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
      "partition_key": "pkey",
      "sort_key": "",
      "executor_memory": "8G",
      "driver_memory": "6G",
      "executor_cores":"3",
      "driver_cores": "1"
    },
    {
      "table_name": "policy-customer-impersonation",
      "query_name": [],
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
      "partition_key": "pkey",
      "sort_key": "",
      "executor_memory": "8G",
      "driver_memory": "6G",
      "executor_cores":"3",
      "driver_cores": "1"
    },
    {
      "table_name": "carrier-tracking",
      "query_name": [],
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
      "partition_key": "pkey",
      "sort_key": "",
      "executor_memory": "4G",
      "driver_memory": "2G",
      "executor_cores":"1",
      "driver_cores": "1"
    },
    {
      "table_name": "policy-pplus",
      "query_name": [],
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
      "partition_key": "pkey",
      "sort_key": "",
      "executor_memory": "5G",
      "driver_memory": "2G",
      "executor_cores":"1",
      "driver_cores": "1"
    },
    {
      "table_name": "agent-43-pplus-upline",
      "query_name": [],
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
      "partition_key": "pkey",
      "sort_key": "",
      "executor_memory": "17G",
      "driver_memory": "10G",
      "executor_cores":"3",
      "driver_cores": "1",
      "spark_dynamicAllocation_minExecutors": "5",
      "spark_dynamicAllocation_maxExecutors": "20",
      "spark_dynamicAllocation_initialExecutors": "5"
    },
    {
      "table_name": "agent-43-hzn-upline",
      "query_name": [],
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
      "partition_key": "pkey",
      "sort_key": "",
      "executor_memory": "17G",
      "driver_memory": "10G",
      "executor_cores":"3",
      "driver_cores": "1",
      "spark_dynamicAllocation_minExecutors": "5",
      "spark_dynamicAllocation_maxExecutors": "20",
      "spark_dynamicAllocation_initialExecutors": "5"
    },
    {
      "table_name": "policy-p07",
      "query_name": [],
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
      "partition_key": "pkey",
      "sort_key": "",
      "executor_memory": "4G",
      "driver_memory": "2G",
      "executor_cores":"1",
      "driver_cores": "1",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "5",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "table_name": "nic_bga_tbl",
      "query_name": [],
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
      "partition_key": "pkey",
      "sort_key": "",
      "executor_memory": "8G",
      "driver_memory": "4G",
      "executor_cores":"1",
      "driver_cores": "1",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "5",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "table_name": "policy-phd-260-beneficiary",
      "query_name": [],
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
      "partition_key": "pkey",
      "sort_key": "",
      "executor_memory": "8G",
      "driver_memory": "6G",
      "executor_cores":"3",
      "driver_cores": "1",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "5",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "table_name": "phd-245-customer-segment",
      "query_name": [],
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
      "partition_key": "pkey",
      "sort_key": "",
      "executor_memory": "8G",
      "driver_memory": "6G",
      "executor_cores":"3",
      "driver_cores": "1",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "5",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "table_name": "policy-phd-280-coverages-segment",
      "query_name": [],
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
      "partition_key": "pkey",
      "sort_key": "",
      "executor_memory": "8G",
      "driver_memory": "6G",
      "executor_cores":"3",
      "driver_cores": "1",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "5",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "table_name": "policy-phd-240-main",
      "query_name": [],
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
      "partition_key": "pkey",
      "sort_key": "",
      "executor_memory": "8G",
      "driver_memory": "6G",
      "executor_cores":"3",
      "driver_cores": "1",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "5",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "table_name": "apah-bob-inforce-dictionary-table",
      "query_name": [],
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
      "partition_key": "pkey",
      "sort_key": "",
      "executor_memory": "15G",
      "driver_memory": "12G",
      "executor_cores":"3",
      "driver_cores": "1"
    },
    {
      "table_name": "commission_agent_attribute",
      "query_name": [],
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
      "partition_key": "pkey",
      "sort_key": "",
      "executor_memory": "15G",
      "driver_memory": "12G",
      "executor_cores":"3",
      "driver_cores": "1"
    },
    {
      "table_name": "policy-40-main-update-job",
      "query_name": [],
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
      "partition_key": "pkey",
      "sort_key": "",
      "executor_memory": "15G",
      "driver_memory": "12G",
      "executor_cores":"3",
      "driver_cores": "1"
    },
    {
      "table_name": "policy-delete-job",
      "query_name": [],
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
      "partition_key": "pkey",
      "sort_key": "",
      "executor_memory": "15G",
      "driver_memory": "12G",
      "executor_cores":"3",
      "driver_cores": "1"
    }
  ],

  "md_smd_ingestion_task": [
    {
      "table_name": "md_smd_ingestion",
      "query_name": [],
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
      "partition_key": "pkey",
      "sort_key": "",
      "executor_memory": "7G",
      "driver_memory": "5G",
      "executor_cores":"3",
      "driver_cores": "1"
    }
  ],

"life_purge_deletion": [
  {
    "table_name": "policy-life-purge",
    "query_name": [],
    "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
    "partition_key": "pkey",
    "sort_key": "",
    "executor_memory": "10G",
    "driver_memory": "5G",
    "executor_cores":"3",
    "driver_cores": "1",
    "spark_dynamicAllocation_minExecutors": "1",
    "spark_dynamicAllocation_maxExecutors": "20",
    "spark_dynamicAllocation_initialExecutors": "1"
  },
  {
    "table_name": "policy-life-purge-opensearch",
    "query_name": [],
    "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
    "partition_key": "pkey",
    "sort_key": "",
    "executor_memory": "10G",
    "driver_memory": "5G",
    "executor_cores":"3",
    "driver_cores": "1",
    "spark_dynamicAllocation_minExecutors": "1",
    "spark_dynamicAllocation_maxExecutors": "20",
    "spark_dynamicAllocation_initialExecutors": "1"
  }
],
  "one_time_delete_task": [
    {
      "table_name": "onetime-policies-delete-from-dynamodb",
      "query_name": [],
      "dynamo_table": "NONE",
      "partition_key": "pkey",
      "sort_key": "",
      "executor_memory": "15G",
      "driver_memory": "12G",
      "executor_cores":"3",
      "driver_cores": "1"
    },
    {
      "table_name": "dynamodb_delete_func",
      "query_name": [],
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
      "partition_key": "pkey",
      "sort_key": "",
      "executor_memory": "15G",
      "driver_memory": "5G",
      "executor_cores":"3",
      "driver_cores": "1",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "10",
      "spark_dynamicAllocation_initialExecutors": "1"
    }
  ],

  "mras_prelanding_to_landing_task": [
    {
      "table_name": "mras-prelanding",
      "sort_key": "",
      "query_name": [],
      "executor_memory": "15G",
      "driver_memory": "12G",
      "executor_cores":"3",
      "driver_cores": "1"
    },
    {
      "table_name": "mras-last",
      "sort_key": "",
      "query_name": [],
      "executor_memory": "8G",
      "driver_memory": "6G",
      "executor_cores":"3",
      "driver_cores": "1"
    },
    {
      "table_name": "mras-disclosure-load",
      "query_name": [],
      "partition_key": "pkey",
      "sort_key": "",
      "executor_memory": "15G",
      "driver_memory": "12G",
      "executor_cores":"3",
      "driver_cores": "1"
    },
    {
      "table_name": "mras-overallcase-q2q-audit-kyc-load",
      "query_name": [],
      "partition_key": "pkey",
      "sort_key": "",
      "executor_memory": "15G",
      "driver_memory": "12G",
      "executor_cores":"3",
      "driver_cores": "1"
    },
    {
      "table_name": "mras-rx-physicians-load",
      "query_name": [],
      "partition_key": "pkey",
      "sort_key": "",
      "executor_memory": "15G",
      "driver_memory": "12G",
      "executor_cores":"3",
      "driver_cores": "1"
    },
    {
      "table_name": "lab-hos-blood-load",
      "query_name": [],
      "partition_key": "pkey",
      "sort_key": "",
      "executor_memory": "15G",
      "driver_memory": "12G",
      "executor_cores":"3",
      "driver_cores": "1"
    },
    {
      "table_name": "ti-truerisk-trl-load",
      "query_name": [],
      "partition_key": "pkey",
      "sort_key": "",
      "executor_memory": "15G",
      "driver_memory": "12G",
      "executor_cores":"3",
      "driver_cores": "1"
    },
    {
      "table_name": "entity-keys-rqmnt-result-load",
      "query_name": [],
      "partition_key": "pkey",
      "sort_key": "",
      "executor_memory": "15G",
      "driver_memory": "12G",
      "executor_cores":"3",
      "driver_cores": "1"
    },
    {
      "table_name": "life-load",
      "query_name": [],
      "partition_key": "pkey",
      "sort_key": "",
      "executor_memory": "15G",
      "driver_memory": "12G",
      "executor_cores":"3",
      "driver_cores": "1"
    },
    {
      "table_name": "mib-mvr-load",
      "query_name": [],
      "partition_key": "pkey",
      "sort_key": "",
      "executor_memory": "15G",
      "driver_memory": "12G",
      "executor_cores":"3",
      "driver_cores": "1"
    }
  ],
  
  "lifecomm_data_model_task": [
    {
      "table_name": "policy-40-main-lifecomm",
      "query_name": [],
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
      "partition_key": "pkey",
      "sort_key": "",
      "executor_memory": "15G",
      "driver_memory": "12G",
      "executor_cores":"3",
      "driver_cores": "1"
    },
    {
      "table_name": "policy-60-beneficiary-lifecomm",
      "query_name": [],
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
      "partition_key": "pkey",
      "sort_key": "",
      "executor_memory": "15G",
      "driver_memory": "12G",
      "executor_cores":"3",
      "driver_cores": "1"
    },
    {
      "table_name": "policy-45-customer-lifecomm",
      "query_name": [],
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
      "partition_key": "pkey",
      "sort_key": "",
      "executor_memory": "15G",
      "driver_memory": "12G",
      "executor_cores":"3",
      "driver_cores": "1"
    },
    {
      "table_name": "policy-riders-lifecomm",
      "query_name": [],
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
      "partition_key": "pkey",
      "sort_key": "",
      "executor_memory": "15G",
      "driver_memory": "12G",
      "executor_cores":"3",
      "driver_cores": "1"
    },
    {
      "table_name": "policy-customer-lifecomm-impersonation",
      "query_name": [],
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
      "partition_key": "pkey",
      "sort_key": "",
      "executor_memory": "15G",
      "driver_memory": "12G",
      "executor_cores":"3",
      "driver_cores": "1"
    },
    {
      "table_name": "daily_full_apah_bob_pplus_lifecomm",
      "query_name": [],
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
      "partition_key": "pkey",
      "sort_key": "",
      "executor_memory": "15G",
      "driver_memory": "12G",
      "executor_cores":"3",
      "driver_cores": "1"
    },
    {
      "table_name": "policy-activity-lifecomm",
      "query_name": [],
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
      "partition_key": "pkey",
      "sort_key": "",
      "executor_memory": "15G",
      "driver_memory": "12G",
      "executor_cores":"3",
      "driver_cores": "1"
    },
    {
      "table_name": "delta",
      "query_name": [],
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy-delta",
      "partition_key": "policy_num",
      "sort_key": "",
      "executor_memory": "4G",
      "driver_memory": "2G",
      "executor_cores":"1",
      "driver_cores": "1"
    },
    {
      "table_name": "apah-bob-lifecomm-inforce-dictionary-table",
      "query_name": [],
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
      "partition_key": "pkey",
      "sort_key": "",
      "executor_memory": "15G",
      "driver_memory": "12G",
      "executor_cores":"3",
      "driver_cores": "1"
    },
    {
      "table_name": "policy_85_lifecomm",
      "query_name": [],
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
      "partition_key": "pkey",
      "sort_key": "",
      "executor_memory": "15G",
      "driver_memory": "12G",
      "executor_cores":"3",
      "driver_cores": "1"
    }
  ],
  "weekly_load_task": [
    {
      "table_name": "weekly_policy_70",
      "spark_driver_memory":"5G",
      "spark_executor_memory":"10G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "50",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "table_name": "weekly_agent_40_hzn",
      "spark_driver_memory":"2G",
      "spark_executor_memory":"4G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "5",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "table_name": "weekly_policy_agent_status_tracker",
      "spark_driver_memory":"12G",
      "spark_executor_memory":"15G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "30",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "table_name": "weekly_agent_42_pplus",
      "spark_driver_memory":"5G",
      "spark_executor_memory":"10G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "10",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "table_name": "weekly_agent_40_pplus",
      "spark_driver_memory":"5G",
      "spark_executor_memory":"10G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "10",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "table_name": "weekly_apah_bob_pplus_service",
      "spark_driver_memory":"9G",
      "spark_executor_memory":"21G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "700",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "table_name": "weekly_agent_42_hzn",
      "partition_key": "pkey",
      "spark_driver_memory":"4G",
      "spark_executor_memory":"8G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "10",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "table_name": "weekly_agent_44_hzn",
      "spark_driver_memory": "2G",
      "spark_executor_memory": "5G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "5",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "table_name": "weekly_apah_bob_hzn_pplus_service",
      "spark_driver_memory":"9G",
      "spark_executor_memory":"18G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "500",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "table_name": "weekly_apah_bob_hzn_service",
      "spark_driver_memory": "9G",
      "spark_executor_memory": "21G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "700",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "table_name": "weekly_agent_43_hzn",
      "spark_driver_memory": "4G",
      "spark_executor_memory": "8G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "15",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "table_name": "weekly_agent_44_pplus",
      "spark_driver_memory": "4G",
      "spark_executor_memory": "8G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "5",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "table_name": "weekly_pds_policy_customer",
      "spark_driver_memory": "2G",
      "spark_executor_memory": "8G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "1",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "5",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "table_name": "weekly_pds_policy_requirement",
      "spark_driver_memory": "12G",
      "spark_executor_memory": "15G",
      "spark_driver_cores": "3",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "30",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "table_name": "weekly_pds_policy_main_pending",
      "spark_driver_memory": "4G",
      "spark_executor_memory": "8G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "3",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "table_name": "weekly_agent_43_pplus",
      "spark_driver_memory": "2G",
      "spark_executor_memory": "4G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "5",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "table_name": "policy_phd_260_beneficiary_weekly",
      "spark_driver_memory":"2G",
      "spark_executor_memory":"4G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "5",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "table_name": "full_phd_245_customer",
      "spark_driver_memory":"2G",
      "spark_executor_memory":"4G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "5",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "table_name": "full_phd_280_coverages",
      "spark_driver_memory":"2G",
      "spark_executor_memory":"5G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "5",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "table_name": "weekly_phd_240_main",
      "spark_driver_memory": "2G",
      "spark_executor_memory": "8G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "1",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "10",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "table_name": "agent_event_notification_sqs_full",
      "spark_driver_memory": "2G",
      "spark_executor_memory": "8G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "1",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "10",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "table_name": "agent_notification_sqs_uw_full",
      "spark_driver_memory": "2G",
      "spark_executor_memory": "8G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "1",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "10",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "table_name": "agent_notification_sqs_uw_mras_full",
      "spark_driver_memory": "2G",
      "spark_executor_memory": "8G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "1",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "10",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "table_name": "policy_40_main_nrt_full",
      "spark_driver_memory": "6G",
      "spark_executor_memory": "16G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "10",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "table_name": "policy_80_riders_full",
      "spark_driver_memory": "5G",
      "spark_executor_memory": "10G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "1",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "10",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "table_name": "weekly_agent_44_p07",
      "spark_driver_memory": "4G",
      "spark_executor_memory": "8G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "5",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "table_name": "weekly_agent_42_p07",
      "spark_driver_memory": "4G",
      "spark_executor_memory": "8G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "5",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "table_name": "weekly_agent_43_p07",
      "spark_driver_memory": "4G",
      "spark_executor_memory": "8G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "5",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "table_name": "weekly_bob_p07",
      "spark_driver_memory": "8G",
      "spark_executor_memory": "18G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "3",
      "spark_dynamicAllocation_maxExecutors": "100",
      "spark_dynamicAllocation_initialExecutors": "3"
    }
  ],
  "weekly_load_task_p07": [
    {
      "table_name": "loop_agent_42_p07",
      "spark_driver_memory": "4G",
      "spark_executor_memory": "8G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "5",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "table_name": "loop_agent_44_p07",
      "spark_driver_memory": "4G",
      "spark_executor_memory": "8G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "5",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "table_name": "loop_agent_43_p07",
      "spark_driver_memory": "4G",
      "spark_executor_memory": "8G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "5",
      "spark_dynamicAllocation_initialExecutors": "1"
    }
  ],
  "multibatch_load_task_pplus": [
    {
      "table_name": "loop_agent_42_pplus",
      "spark_driver_memory": "4G",
      "spark_executor_memory": "8G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "5",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "table_name": "loop_agent_43_pplus",
      "spark_driver_memory": "4G",
      "spark_executor_memory": "8G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "5",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "table_name": "loop_agent_44_pplus",
      "spark_driver_memory": "4G",
      "spark_executor_memory": "8G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "5",
      "spark_dynamicAllocation_initialExecutors": "1"
    }
  ],
  "ARL_Task": [
    {
      "table_name": "ARL",
      "partition_key": "pkey",
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
      "query_name": [],
      "executor_memory": "8G",
      "driver_memory": "2G",
      "sort_key": "",
      "executor_cores": "2",
      "driver_cores": "1",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "5",
      "spark_dynamicAllocation_initialExecutors": "1"
    }
  ],
  "multibatch_load_task_hzn": [
    {
      "table_name": "loop_agent_42_hzn",
      "spark_driver_memory": "4G",
      "spark_executor_memory": "8G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "5",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "table_name": "loop_agent_43_hzn",
      "spark_driver_memory": "4G",
      "spark_executor_memory": "8G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "5",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "table_name": "loop_agent_44_hzn",
      "spark_driver_memory": "4G",
      "spark_executor_memory": "8G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "5",
      "spark_dynamicAllocation_initialExecutors": "1"
    }
  ],
  "spark_run_parameter": {
    "env": "dev",
    "entryPoint": "s3://ta-individual-idw-dev-airflowdags/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/scripts/ida/src/domain_data_model/main_eks_domain_data_model.py",
    "sparkSubmitParameters": "--py-files s3://ta-individual-idw-dev-airflowdags/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/scripts/app_ida.zip --jars s3://ta-individual-idw-dev-airflowdags/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/ojdbc8.jar,s3://ta-individual-idw-dev-airflowdags/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/mssql-jdbc-7.2.1.jre8.jar,s3://ta-individual-idw-dev-airflowdags/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/spark-avro_2.12-3.1.1.jar,s3://ta-individual-idw-dev-airflowdags/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/postgresql-42.2.5.jar,s3://ta-individual-idw-dev-airflowdags/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/RedshiftJDBC42-no-awssdk-1.2.41.1065.jar,s3://ta-individual-idw-dev-airflowdags/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/minimal-json-0.9.4.jar,s3://ta-individual-idw-dev-airflowdags/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/spark-redshift_2.12-4.2.0.jar,s3://ta-individual-idw-dev-airflowdags/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/hudi-spark3-bundle_2.12-0.9.0.jar,s3://ta-individual-idw-dev-airflowdags/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/spark-sql-kinesis_2.12-1.2.0_spark-3.0-snapshot.jar,s3://ta-individual-idw-dev-airflowdags/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/elasticsearch-spark-30_2.12-7.16.3.jar --conf spark.sql.legacy.parquet.int96RebaseModeInRead=LEGACY --conf spark.sql.legacy.parquet.int96RebaseModeInWrite=LEGACY --conf spark.sql.legacy.parquet.datetimeRebaseModeInRead=LEGACY --conf spark.sql.legacy.parquet.datetimeRebaseModeInWrite=LEGACY",
    "logUri": "s3://ta-individual-idw-dev-airflowdags-applications-logs/pyspark_logs/virtual_cluster/",
    "cloudwatch_log_group": "/aws/eks/ta-individual-ida-dev-eks-dev-fargate-cluster/cluster",
    "spark_kubernetes_container_image": "209497537643.dkr.ecr.us-east-1.amazonaws.com/ta-individual-ida-tst-ecr-tst-repo:ida-spark-image-v1",
    "JOB_ROLE_FARGATE_ARN": "arn:aws:iam::209497537643:role/amazoneksfargatepodexecutionroledev",
    "JOB_ROLE_ARN": "arn:aws:iam::209497537643:role/ta-individual-ida-eks-dev-eks",
    "spark_sql_warehouse_dir": "s3://ta-individual-idw-dev-airflowdags/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/stream_data_warehouse/"
  },
  "spark_run_parameter_mras_prelanding_landing":{
    "env": "dev",
    "entryPoint":"s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/scripts/ida/src/mras/mras_main.py",
    "sparkSubmitParameters": "--py-files s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/scripts/app_ida.zip --jars s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/ojdbc8.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/mssql-jdbc-7.2.1.jre8.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/spark-avro_2.12-3.1.1.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/postgresql-42.2.5.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/RedshiftJDBC42-no-awssdk-1.2.41.1065.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/minimal-json-0.9.4.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/spark-redshift_2.12-4.2.0.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/hudi-spark3-bundle_2.12-0.9.0.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/elasticsearch-spark-30_2.12-7.16.3.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/spark-sql-kinesis_2.12-1.2.0_spark-3.0-snapshot.jar --conf spark.sql.legacy.parquet.int96RebaseModeInRead=LEGACY --conf spark.sql.legacy.parquet.int96RebaseModeInWrite=LEGACY --conf spark.sql.legacy.parquet.datetimeRebaseModeInRead=LEGACY --conf spark.sql.legacy.parquet.datetimeRebaseModeInWrite=LEGACY",
    "logUri": "s3://ta-individual-idw-dev-airflowdags-applications-logs/pyspark_logs/virtual_cluster/",
    "cloudwatch_log_group": "/aws/eks/ta-individual-ida-dev-eks-dev-fargate-cluster/cluster",
    "spark_kubernetes_container_image": "209497537643.dkr.ecr.us-east-1.amazonaws.com/ta-individual-ida-tst-ecr-tst-repo:ida-spark-image-v1",
    "JOB_ROLE_FARGATE_ARN": "arn:aws:iam::209497537643:role/amazoneksfargatepodexecutionroledev",
    "JOB_ROLE_ARN": "arn:aws:iam::209497537643:role/ta-individual-ida-eks-dev-eks",
    "spark_sql_warehouse_dir": "s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/stream_data_warehouse/"
  },

  "spark_run_parameter_lifecomm":{
    "env": "dev",
    "entryPoint":"s3://ta-individual-idw-dev-airflowdags/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/scripts/ida/src/domain_data_model/main_eks_domain_data_model_lifecomm.py",
    "sparkSubmitParameters": "--py-files s3://ta-individual-idw-dev-airflowdags/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/scripts/app_ida.zip --jars s3://ta-individual-idw-dev-airflowdags/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/ojdbc8.jar,s3://ta-individual-idw-dev-airflowdags/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/mssql-jdbc-7.2.1.jre8.jar,s3://ta-individual-idw-dev-airflowdags/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/spark-avro_2.12-3.1.1.jar,s3://ta-individual-idw-dev-airflowdags/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/postgresql-42.2.5.jar,s3://ta-individual-idw-dev-airflowdags/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/RedshiftJDBC42-no-awssdk-1.2.41.1065.jar,s3://ta-individual-idw-dev-airflowdags/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/minimal-json-0.9.4.jar,s3://ta-individual-idw-dev-airflowdags/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/spark-redshift_2.12-4.2.0.jar,s3://ta-individual-idw-dev-airflowdags/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/hudi-spark3-bundle_2.12-0.9.0.jar,s3://ta-individual-idw-dev-airflowdags/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/spark-sql-kinesis_2.12-1.2.0_spark-3.0-snapshot.jar,s3://ta-individual-idw-dev-airflowdags/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/elasticsearch-spark-30_2.12-7.16.3.jar --conf spark.sql.legacy.parquet.int96RebaseModeInRead=LEGACY --conf spark.sql.legacy.parquet.int96RebaseModeInWrite=LEGACY --conf spark.sql.legacy.parquet.datetimeRebaseModeInRead=LEGACY --conf spark.sql.legacy.parquet.datetimeRebaseModeInWrite=LEGACY",
    "logUri": "s3://ta-individual-idw-dev-airflowdags-applications-logs/pyspark_logs/virtual_cluster/",
    "cloudwatch_log_group": "/aws/eks/ta-individual-ida-dev-eks-dev-fargate-cluster/cluster",
    "spark_kubernetes_container_image": "209497537643.dkr.ecr.us-east-1.amazonaws.com/ta-individual-ida-tst-ecr-tst-repo:ida-spark-image-v1",
    "JOB_ROLE_FARGATE_ARN": "arn:aws:iam::209497537643:role/amazoneksfargatepodexecutionroledev",
    "JOB_ROLE_ARN": "arn:aws:iam::209497537643:role/ta-individual-ida-eks-dev-eks",
    "spark_sql_warehouse_dir": "s3://ta-individual-idw-dev-airflowdags/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/stream_data_warehouse/"

  },

  "spark_run_parameter_md_smd_ingestion":{
    "env": "dev",
    "entryPoint":"s3://ta-individual-idw-dev-airflowdags/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/scripts/ida/src/domain_data_model/main_eks_md_smd_ingestion.py",
    "sparkSubmitParameters": "--py-files s3://ta-individual-idw-dev-airflowdags/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/scripts/app_ida.zip --jars s3://ta-individual-idw-dev-airflowdags/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/ojdbc8.jar,s3://ta-individual-idw-dev-airflowdags/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/mssql-jdbc-7.2.1.jre8.jar,s3://ta-individual-idw-dev-airflowdags/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/spark-avro_2.12-3.1.1.jar,s3://ta-individual-idw-dev-airflowdags/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/postgresql-42.2.5.jar,s3://ta-individual-idw-dev-airflowdags/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/RedshiftJDBC42-no-awssdk-1.2.41.1065.jar,s3://ta-individual-idw-dev-airflowdags/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/minimal-json-0.9.4.jar,s3://ta-individual-idw-dev-airflowdags/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/spark-redshift_2.12-4.2.0.jar,s3://ta-individual-idw-dev-airflowdags/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/hudi-spark3-bundle_2.12-0.9.0.jar,s3://ta-individual-idw-dev-airflowdags/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/spark-sql-kinesis_2.12-1.2.0_spark-3.0-snapshot.jar,s3://ta-individual-idw-dev-airflowdags/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/elasticsearch-spark-30_2.12-7.16.3.jar --conf spark.sql.legacy.parquet.int96RebaseModeInRead=LEGACY --conf spark.sql.legacy.parquet.int96RebaseModeInWrite=LEGACY --conf spark.sql.legacy.parquet.datetimeRebaseModeInRead=LEGACY --conf spark.sql.legacy.parquet.datetimeRebaseModeInWrite=LEGACY",
    "logUri": "s3://ta-individual-idw-dev-airflowdags-applications-logs/pyspark_logs/virtual_cluster/",
    "cloudwatch_log_group": "/aws/eks/ta-individual-ida-dev-eks-dev-fargate-cluster/cluster",
    "spark_kubernetes_container_image": "209497537643.dkr.ecr.us-east-1.amazonaws.com/ta-individual-ida-tst-ecr-tst-repo:ida-spark-image-v1",
    "JOB_ROLE_FARGATE_ARN": "arn:aws:iam::209497537643:role/amazoneksfargatepodexecutionroledev",
    "JOB_ROLE_ARN": "arn:aws:iam::209497537643:role/ta-individual-ida-eks-dev-eks",
    "spark_sql_warehouse_dir": "s3://ta-individual-idw-dev-airflowdags/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/stream_data_warehouse/"

  },

  "streaming_task_inforce": [
    {
      "model": "PDS_ORACLE_INFORCE",
      "spark_driver_memory":"10G",
      "spark_executor_memory":"15G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "20",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "model": "policy_90_activity_nrt",
      "spark_driver_memory": "7G",
      "spark_executor_memory": "20G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "30",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "model": "policy_inforce_60_benficiary_nrt",
      "spark_driver_memory":"2G",
      "spark_executor_memory":"6G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "5",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "model": "policy_inforce_45_customer_nrt",
      "spark_driver_memory":"2G",
      "spark_executor_memory":"6G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "5",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "model": "policy_80_riders_nrt",
      "spark_driver_memory": "5G",
      "spark_executor_memory": "10G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "5",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "model": "policy_40_main_nrt",
      "spark_driver_memory": "5G",
      "spark_executor_memory": "10G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "10",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "model": "policy_40_main_eft_nrt",
      "spark_driver_memory": "5G",
      "spark_executor_memory": "10G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "10",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "model": "policy_85_financial_nrt",
      "spark_driver_memory": "7G",
      "spark_executor_memory": "20G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "30",
      "spark_dynamicAllocation_initialExecutors": "1"
    }
  ],

  "streaming_task_pds": [
    {
      "model": "PDS_CUSTOM_DELETE",
      "spark_driver_memory":"10G",
      "spark_executor_memory":"15G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "20",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "model": "PDS_ORACLE_CLOB",
      "spark_driver_memory":"10G",
      "spark_executor_memory":"15G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "20",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "model": "pds_policy_customer",
      "spark_driver_memory": "10G",
      "spark_executor_memory": "15G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "10",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
      {
      "model": "pds_policy_requirement",
      "spark_driver_memory": "10G",
      "spark_executor_memory": "15G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "10",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
      {
      "model": "pds_policy_main_pending",
      "spark_driver_memory": "5G",
      "spark_executor_memory": "15G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "10",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "model": "policy_agent_status_tracker",
      "spark_driver_memory":"7G",
      "spark_executor_memory":"12G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "30",
      "spark_dynamicAllocation_initialExecutors": "1"
    }
  ],

  "streaming_task_pp_all": [
    {
      "model": "agent_42_pplus",
      "spark_driver_memory":"5G",
      "spark_executor_memory":"10G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "10",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "model": "agent_44_pplus",
      "spark_driver_memory": "5G",
      "spark_executor_memory": "10G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "5",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "model": "agent_43_pplus",
      "spark_driver_memory":"5G",
      "spark_executor_memory":"10G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "10",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "model": "apah_bob_pplus_service",
      "spark_driver_memory":"9G",
      "spark_executor_memory":"18G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "200",
      "spark_dynamicAllocation_initialExecutors": "1"
    }
  ],

    "streaming_task_hzn_all": [
    {
      "model": "agent_44_hzn",
      "spark_driver_memory": "5G",
      "spark_executor_memory": "10G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "5",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "model": "agent_42_hzn",
      "partition_key": "pkey",
      "spark_driver_memory":"8G",
      "spark_executor_memory":"16G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "10",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "model": "agent_43_hzn",
      "spark_driver_memory": "10G",
      "spark_executor_memory": "15G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "10",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "model": "apah_bob_hzn_service",
      "spark_driver_memory": "9G",
      "spark_executor_memory": "18G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "200",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
      {
      "model": "apah_bob_hzn_pplus_service",
      "spark_driver_memory": "9G",
      "spark_executor_memory": "18G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "200",
      "spark_dynamicAllocation_initialExecutors": "1"
    }
  ],

  "streaming_task": [
    {
      "table_name": "PPLUS_DB2_CUSTOM_DOWN_DEPENDENT",
      "query_name": [],
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
      "partition_key": "pkey",
      "sort_key": "",
      "spark_driver_memory":"10G",
      "spark_executor_memory":"15G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "30",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "table_name": "HZN_ORACLE_LOAD_DOWN_DEPENDENT",
      "query_name": [],
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
      "partition_key": "pkey",
      "sort_key": "",
      "spark_driver_memory":"15G",
      "spark_executor_memory":"5G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "20",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "table_name": "PPLUS_DB2_CUSTOM_DOWN_NON_DEPENDENT",
      "query_name": [],
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
      "partition_key": "pkey",
      "sort_key": "",
      "spark_driver_memory":"10G",
      "spark_executor_memory":"15G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "30",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "table_name": "HZN_ORACLE_LOAD_DOWN_NON_DEPENDENT",
      "query_name": [],
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
      "partition_key": "pkey",
      "sort_key": "",
      "spark_driver_memory":"15G",
      "spark_executor_memory":"5G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "20",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "table_name": "mras",
      "query_name": [],
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
      "partition_key": "pkey",
      "sort_key": "",
      "spark_driver_memory":"10G",
      "spark_executor_memory":"16G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "10",
      "spark_dynamicAllocation_maxExecutors": "20",
      "spark_dynamicAllocation_initialExecutors": "10"
    },
    {
      "table_name": "AWD",
      "query_name": [],
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
      "partition_key": "pkey",
      "sort_key": "",
      "spark_driver_memory":"8G",
      "spark_executor_memory":"10G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "20",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "table_name": "policy_70",
      "query_name": [],
      "dynamo_table": "ta-individual-data-service-domains-mdl-agent-policy",
      "partition_key": "pkey",
      "sort_key": "",
      "spark_driver_memory":"5G",
      "spark_executor_memory":"10G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "20",
      "spark_dynamicAllocation_initialExecutors": "1"
    }
  ],
  "streaming_eks_configs": {
    "env": "dev",
    "entryPoint":"s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/scripts/src/kinesis_streaming_eks_model/kinesis_streaming_model_main.py",
    "entryPointRetrofit":"s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/scripts/ida/src/streaming_apps/stream_to_dynamodb/main/ingestion_main.py",
    "sparkSubmitParameters": "--py-files s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/scripts/app_ida.zip --jars s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/ojdbc8.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/spark-avro_2.12-3.1.1.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/postgresql-42.2.5.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/RedshiftJDBC42-no-awssdk-1.2.41.1065.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/minimal-json-0.9.4.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/spark-redshift_2.12-4.2.0.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/hudi-spark3-bundle_2.12-0.9.0.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/elasticsearch-spark-30_2.12-7.16.3.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/spark-sql-kinesis_2.12-1.2.0_spark-3.0-snapshot.jar --conf spark.sql.parquet.enableVectorizedReader=false --conf spark.sql.legacy.parquet.int96RebaseModeInRead=LEGACY --conf spark.sql.legacy.parquet.int96RebaseModeInWrite=LEGACY --conf spark.sql.legacy.parquet.datetimeRebaseModeInRead=LEGACY --conf spark.sql.legacy.parquet.datetimeRebaseModeInWrite=LEGACY",
    "logUri": "s3://ta-individual-idw-dev-airflowdags-applications-logs/pyspark_logs/virtual_cluster/",
    "cloudwatch_log_group": "/aws/eks/ta-individual-ida-dev-eks-dev-fargate-cluster/cluster",
    "spark_kubernetes_container_image": "209497537643.dkr.ecr.us-east-1.amazonaws.com/ta-individual-ida-tst-ecr-tst-repo:ida-spark-image-v1",
    "JOB_ROLE_FARGATE_ARN": "arn:aws:iam::209497537643:role/amazoneksfargatepodexecutionroledev",
    "JOB_ROLE_ARN": "arn:aws:iam::209497537643:role/ta-individual-ida-eks-dev-eks",
    "job_config_s3_loc": "s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/pyspark-scripts-configs/dev-pyspark-scripts-configs.json",
    "spark_sql_warehouse_dir": "s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/stream_data_warehouse/"
  },
  "kinesis_to_dynamodb_task": [
    {
      "model": "agent_40_hzn",
      "spark_driver_memory":"2G",
      "spark_executor_memory":"6G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "5",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "model": "policy_agent_status_tracker",
      "spark_driver_memory":"5G",
      "spark_executor_memory":"10G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "10",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "model": "policy_inforce_45_customer_nrt",
      "spark_driver_memory":"2G",
      "spark_executor_memory":"6G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "5",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "model": "agent_42_pplus",
      "spark_driver_memory":"2G",
      "spark_executor_memory":"5G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "5",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "model": "policy_inforce_60_benficiary_nrt",
      "spark_driver_memory":"2G",
      "spark_executor_memory":"6G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "5",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "model": "agent_40_pplus",
      "spark_driver_memory":"2G",
      "spark_executor_memory":"5G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "5",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "model": "apah_bob_pplus_service",
      "spark_driver_memory":"9G",
      "spark_executor_memory":"18G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "200",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "model": "agent_42_hzn",
      "partition_key": "pkey",
      "spark_driver_memory":"2G",
      "spark_executor_memory":"4G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "5",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "model": "agent_44_hzn",
      "spark_driver_memory": "2G",
      "spark_executor_memory": "4G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "5",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "model": "apah_bob_hzn_service",
      "spark_driver_memory": "9G",
      "spark_executor_memory": "18G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "70",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
       {
      "model": "agent_44_pplus",
      "spark_driver_memory": "2G",
      "spark_executor_memory": "4G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "5",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
      {
      "model": "agent_43_hzn",
      "spark_driver_memory": "2G",
      "spark_executor_memory": "4G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "5",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
      {
      "model": "agent_43_pplus",
      "spark_driver_memory": "2G",
      "spark_executor_memory": "5G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "5",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
      {
      "model": "pds_policy_customer",
      "spark_driver_memory": "2G",
      "spark_executor_memory": "8G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "5",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
      {
      "model": "pds_policy_requirement",
      "spark_driver_memory": "4G",
      "spark_executor_memory": "8G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "5",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
      {
      "model": "pds_policy_main_pending",
      "spark_driver_memory": "2G",
      "spark_executor_memory": "4G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "5",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "model": "policy_85_financial_nrt",
      "spark_driver_memory": "7G",
      "spark_executor_memory": "20G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "30",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "model": "agent_event_notification_sqs",
      "spark_driver_memory": "2G",
      "spark_executor_memory": "8G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "5",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "model": "policy_90_activity_nrt",
      "spark_driver_memory": "7G",
      "spark_executor_memory": "20G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "30",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "model": "agent_notification_sqs_uw",
      "spark_driver_memory": "2G",
      "spark_executor_memory": "8G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "5",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "model": "agent_notification_sqs_uw_mras",
      "spark_driver_memory": "2G",
      "spark_executor_memory": "8G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "5",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "model": "policy_80_riders_nrt",
      "spark_driver_memory": "5G",
      "spark_executor_memory": "10G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "10",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "model": "policy_40_main_nrt",
      "spark_driver_memory": "5G",
      "spark_executor_memory": "10G",
      "spark_driver_cores": "1",
      "spark_executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "10",
      "spark_dynamicAllocation_initialExecutors": "1"
    },
    {
      "table_name": "commission_agent_attribute",
      "query_name": [],
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
      "partition_key": "policy_number",
      "sort_key": "",
      "executor_memory": "15G",
      "driver_memory": "12G",
      "executor_cores":"3",
      "driver_cores": "1"
    }

  ],

  "task_notification_inforce_event": [
    {
      "table_name": "notification_hier",
      "driver_memory":"9G",
      "executor_memory":"18G",
      "driver_cores": "1",
      "executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "500",
      "spark_dynamicAllocation_initialExecutors": "1",
      "partition_key": "pkey",
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
      "query_name": [],
      "sort_key": ""
    },
    {
      "table_name": "inforce_premium_renewal_notifications",
      "driver_memory":"9G",
      "executor_memory":"18G",
      "driver_cores": "1",
      "executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "500",
      "spark_dynamicAllocation_initialExecutors": "1",
      "partition_key": "pkey",
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
      "query_name": [],
      "sort_key": ""
    },
    {
      "table_name": "inforce_notification_eft_removal",
      "driver_memory":"9G",
      "executor_memory":"18G",
      "driver_cores": "1",
      "executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "500",
      "spark_dynamicAllocation_initialExecutors": "1",
      "partition_key": "pkey",
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
      "query_name": [],
      "sort_key": ""
    },
    {
      "table_name": "inforce_notification_policy_lapse",
      "driver_memory":"9G",
      "executor_memory":"18G",
      "driver_cores": "1",
      "executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "500",
      "spark_dynamicAllocation_initialExecutors": "1",
      "partition_key": "pkey",
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
      "query_name": [],
      "sort_key": ""
    }
  ],

  "task_lifecomm_notification_inforce_event": [
    {
      "table_name": "lifecomm_notification_hier",
      "driver_memory":"9G",
      "executor_memory":"18G",
      "driver_cores": "1",
      "executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "500",
      "spark_dynamicAllocation_initialExecutors": "1",
      "partition_key": "pkey",
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
      "query_name": [],
      "sort_key": ""
    },
    {
      "table_name": "lifecomm_inforce_notification_lapse",
      "driver_memory":"9G",
      "executor_memory":"18G",
      "driver_cores": "1",
      "executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "500",
      "spark_dynamicAllocation_initialExecutors": "1",
      "partition_key": "pkey",
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
      "query_name": [],
      "sort_key": ""
    },
    {
      "table_name": "lifecomm_inforce_notification_EFT_removal",
      "driver_memory":"9G",
      "executor_memory":"18G",
      "driver_cores": "1",
      "executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "500",
      "spark_dynamicAllocation_initialExecutors": "1",
      "partition_key": "pkey",
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
      "query_name": [],
      "sort_key": ""
    }
  ],

  "task_notification_bestow_inforce_event": [
    {
      "table_name": "bestow_notification_hier",
      "query_name": [],
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
      "partition_key": "pkey",
      "sort_key": "",
      "executor_memory": "18G",
      "driver_memory": "9G",
      "executor_cores": "3",
      "driver_cores": "1",
      "spark_dynamicAllocation_minExecutors": "3",
      "spark_dynamicAllocation_maxExecutors": "500",
      "spark_dynamicAllocation_initialExecutors": "3"
    },
    {
      "table_name": "bestow_inforce_notification_policy_lapse",
      "query_name": [],
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
      "partition_key": "pkey",
      "sort_key": "",
      "executor_memory": "18G",
      "driver_memory": "9G",
      "executor_cores": "3",
      "driver_cores": "1",
      "spark_dynamicAllocation_minExecutors": "3",
      "spark_dynamicAllocation_maxExecutors": "500",
      "spark_dynamicAllocation_initialExecutors": "3"
    }
  ],

  "streaming_task_pending_notification_event": [
    {
      "table_name": "notification_hier",
      "driver_memory":"9G",
      "executor_memory":"18G",
      "driver_cores": "1",
      "executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "500",
      "spark_dynamicAllocation_initialExecutors": "1",
      "partition_key": "pkey",
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
      "query_name": [],
      "sort_key": ""
    },
    {
      "table_name": "pending_notification",
      "driver_memory":"9G",
      "executor_memory":"18G",
      "driver_cores": "1",
      "executor_cores": "3",
      "spark_dynamicAllocation_minExecutors": "1",
      "spark_dynamicAllocation_maxExecutors": "500",
      "spark_dynamicAllocation_initialExecutors": "1",
      "partition_key": "pkey",
      "dynamo_table": "ta-individual-data-service-domains-dev-agent-policy",
      "query_name": [],
      "sort_key": ""
    }
  ],

  "new_streaming_eks_configs":  {
    "env": "dev",
    "entryPoint":"s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/scripts/ida/src/streaming_apps/stream_to_datalake/main/streaming_main.py",
    "entryPointJob":"s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/scripts/ida/src/streaming_apps/stream_to_dynamodb/main/ingestion_main.py",
    "sparkSubmitParameters": "--py-files s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/scripts/app_ida.zip --jars s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/mssql-jdbc-7.2.1.jre8.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/ojdbc8.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/db2jcc4.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/db2jcc_license_cisuz.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/spark-avro_2.12-3.1.1.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/postgresql-42.2.5.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/RedshiftJDBC42-no-awssdk-1.2.41.1065.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/minimal-json-0.9.4.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/spark-redshift_2.12-4.2.0.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/hudi-spark3-bundle_2.12-0.9.0.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/spark-sql-kinesis_2.12-1.2.0_spark-3.0-snapshot.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/elasticsearch-spark-30_2.12-7.16.3.jar --conf spark.eventLog.rotation.enabled=true --conf spark.eventLog.rotation.interval=300 --conf spark.eventLog.rotation.minFileSize=1m --conf spark.eventLog.rotation.maxFilesToRetain=2 --conf spark.driver.maxResultSize=5g --conf spark.rpc.message.maxSize=1024 --conf spark.network.timeout=2000s --conf spark.yarn.executor.memoryOverhead=4096 --conf spark.yarn.driver.memoryOverhead=4096 --conf spark.kryoserializer.buffer.max=1024m --conf spark.sql.legacy.parquet.int96RebaseModeInRead=LEGACY --conf spark.sql.legacy.parquet.int96RebaseModeInWrite=LEGACY --conf spark.sql.legacy.parquet.datetimeRebaseModeInRead=LEGACY --conf spark.sql.legacy.parquet.datetimeRebaseModeInWrite=LEGACY",
    "logUri": "s3://ta-individual-idw-dev-airflowdags-applications-logs/pyspark_logs/virtual_cluster/",
    "cloudwatch_log_group": "/aws/eks/ta-individual-ida-dev-eks-dev-fargate-cluster/cluster",
    "spark_kubernetes_container_image": "209497537643.dkr.ecr.us-east-1.amazonaws.com/ta-individual-ida-tst-ecr-tst-repo:ida-spark-image-v1",
    "JOB_ROLE_FARGATE_ARN": "arn:aws:iam::209497537643:role/amazoneksfargatepodexecutionroledev",
    "JOB_ROLE_ARN": "arn:aws:iam::209497537643:role/ta-individual-ida-eks-dev-eks",
    "spark_sql_warehouse_dir": "s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/stream_data_warehouse/"
  },
  "streaming_emr_on_eks_configs": {
    "entryPoint":"s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/scripts/src/kinesis_streaming_eks_model/kinesis_streaming_model_main.py",
    "sparkSubmitParameters": "--py-files s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/scripts/app_ida.zip --jars s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/ojdbc8.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/hadoop-aws-3.2.1.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/spark-avro_2.12-3.1.1.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/postgresql-42.2.5.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/RedshiftJDBC42-no-awssdk-1.2.41.1065.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/minimal-json-0.9.4.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/spark-redshift_2.12-4.2.0.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/hudi-spark3-bundle_2.12-0.9.0.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/spark-sql-kinesis_2.12-1.2.0_spark-3.0.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/spark-streaming_2.12-3.0.0.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/spark-streaming-kinesis-asl_2.12-3.0.0.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/elasticsearch-spark-30_2.12-7.16.3.jar --conf spark.sql.legacy.parquet.int96RebaseModeInRead=LEGACY --conf spark.sql.legacy.parquet.int96RebaseModeInWrite=LEGACY --conf spark.sql.legacy.parquet.datetimeRebaseModeInRead=LEGACY --conf spark.sql.legacy.parquet.datetimeRebaseModeInWrite=LEGACY",
    "logUri": "s3://ta-individual-idw-dev-airflowdags-applications-logs/pyspark_logs/virtual_cluster/",
    "cloudwatch_log_group": "/aws/eks/ta-individual-ida-dev-eks-dev-cluster/cluster",
    "spark_kubernetes_container_image": "209497537643.dkr.ecr.us-east-1.amazonaws.com/ta-individual-ida-tst-ecr-tst-repo:ida-spark-image-v1",
    "JOB_ROLE_FARGATE_ARN": "arn:aws:iam::209497537643:role/amazoneksfargatepodexecutionroledev",
    "JOB_ROLE_ARN": "arn:aws:iam::209497537643:role/ta-individual-ida-eks-dev-eks",
    "job_config_s3_loc": "s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/pyspark-scripts-configs/dev-pyspark-scripts-configs.json",
    "spark_sql_warehouse_dir": "s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/stream_data_warehouse/"
  },
  "spark_run_parameter_notification_inforce":{
    "env": "dev",
    "entryPoint":"s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/scripts/ida/src/domain_data_model/main_notifications.py",
    "sparkSubmitParameters": "--py-files s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/scripts/app_ida.zip --jars s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/ojdbc8.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/mssql-jdbc-7.2.1.jre8.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/spark-avro_2.12-3.1.1.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/postgresql-42.2.5.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/RedshiftJDBC42-no-awssdk-1.2.41.1065.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/minimal-json-0.9.4.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/spark-redshift_2.12-4.2.0.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/hudi-spark3-bundle_2.12-0.9.0.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/elasticsearch-spark-30_2.12-7.16.3.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/spark-sql-kinesis_2.12-1.2.0_spark-3.0-snapshot.jar --conf spark.sql.legacy.parquet.int96RebaseModeInRead=LEGACY --conf spark.sql.legacy.parquet.int96RebaseModeInWrite=LEGACY --conf spark.sql.legacy.parquet.datetimeRebaseModeInRead=LEGACY --conf spark.sql.legacy.parquet.datetimeRebaseModeInWrite=LEGACY",
    "logUri": "s3://ta-individual-idw-dev-airflowdags-applications-logs/pyspark_logs/virtual_cluster/",
    "cloudwatch_log_group": "/aws/eks/ta-individual-ida-dev-eks-dev-fargate-cluster/cluster",
    "spark_kubernetes_container_image": "209497537643.dkr.ecr.us-east-1.amazonaws.com/ta-individual-ida-dev-ecr-dev-repo:ida-spark-image-v1",
    "JOB_ROLE_FARGATE_ARN": "arn:aws:iam::209497537643:role/amazoneksfargatepodexecutionroletst",
    "JOB_ROLE_ARN": "arn:aws:iam::209497537643:role/ta-individual-ida-eks-dev-eks",
    "spark_sql_warehouse_dir": "s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/stream_data_warehouse/"
  },
  "spark_run_parameter_notification_inforce_lifecomm":{
    "env": "dev",
    "entryPoint":"s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/scripts/ida/src/domain_data_model/main_notifications.py",
    "sparkSubmitParameters": "--py-files s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/scripts/app_ida.zip --jars s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/ojdbc8.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/mssql-jdbc-7.2.1.jre8.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/spark-avro_2.12-3.1.1.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/postgresql-42.2.5.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/RedshiftJDBC42-no-awssdk-1.2.41.1065.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/minimal-json-0.9.4.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/spark-redshift_2.12-4.2.0.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/hudi-spark3-bundle_2.12-0.9.0.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/elasticsearch-spark-30_2.12-7.16.3.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/spark-sql-kinesis_2.12-1.2.0_spark-3.0-snapshot.jar --conf spark.sql.legacy.parquet.int96RebaseModeInRead=LEGACY --conf spark.sql.legacy.parquet.int96RebaseModeInWrite=LEGACY --conf spark.sql.legacy.parquet.datetimeRebaseModeInRead=LEGACY --conf spark.sql.legacy.parquet.datetimeRebaseModeInWrite=LEGACY",
    "logUri": "s3://ta-individual-idw-dev-airflowdags-applications-logs/pyspark_logs/virtual_cluster/",
    "cloudwatch_log_group": "/aws/eks/ta-individual-ida-dev-eks-dev-fargate-cluster/cluster",
    "spark_kubernetes_container_image": "209497537643.dkr.ecr.us-east-1.amazonaws.com/ta-individual-ida-dev-ecr-dev-repo:ida-spark-image-v1",
    "JOB_ROLE_FARGATE_ARN": "arn:aws:iam::209497537643:role/amazoneksfargatepodexecutionroletst",
    "JOB_ROLE_ARN": "arn:aws:iam::209497537643:role/ta-individual-ida-eks-dev-eks",
    "spark_sql_warehouse_dir": "s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/stream_data_warehouse/"
  },
  "bestow_spark_run_parameter_notification_inforce":{
    "env": "dev",
    "entryPoint":"s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/scripts/ida/src/domain_data_model/main_notifications.py",
    "sparkSubmitParameters": "--py-files s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/scripts/app_ida.zip --jars s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/ojdbc8.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/mssql-jdbc-7.2.1.jre8.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/spark-avro_2.12-3.1.1.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/postgresql-42.2.5.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/RedshiftJDBC42-no-awssdk-1.2.41.1065.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/minimal-json-0.9.4.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/spark-redshift_2.12-4.2.0.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/hudi-spark3-bundle_2.12-0.9.0.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/elasticsearch-spark-30_2.12-7.16.3.jar,s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/dependency-jars/spark-sql-kinesis_2.12-1.2.0_spark-3.0-snapshot.jar --conf spark.sql.legacy.parquet.int96RebaseModeInRead=LEGACY --conf spark.sql.legacy.parquet.int96RebaseModeInWrite=LEGACY --conf spark.sql.legacy.parquet.datetimeRebaseModeInRead=LEGACY --conf spark.sql.legacy.parquet.datetimeRebaseModeInWrite=LEGACY",
    "logUri": "s3://ta-individual-idw-dev-airflowdags-applications-logs/pyspark_logs/virtual_cluster/",
    "cloudwatch_log_group": "/aws/eks/ta-individual-ida-dev-eks-dev-fargate-cluster/cluster",
    "spark_kubernetes_container_image": "209497537643.dkr.ecr.us-east-1.amazonaws.com/ta-individual-ida-dev-ecr-dev-repo:ida-spark-image-v1",
    "JOB_ROLE_FARGATE_ARN": "arn:aws:iam::209497537643:role/amazoneksfargatepodexecutionroletst",
    "JOB_ROLE_ARN": "arn:aws:iam::209497537643:role/ta-individual-ida-eks-dev-eks",
    "spark_sql_warehouse_dir": "s3://ta-individual-idw-dev-airflow-dag/airflow/dag/transamerica_individual/transamerica_product_life_individual_oneida/default/stream_data_warehouse/"
  }
}


table_inputs_new.json

======


import logging
from pyspark.sql.functions import col, lit
from src.common_utils.read_write_db import ReadWriteDB

class PdsOracleDeleteOperations(ReadWriteDB):
    def __init__(self, spark_session=None, env=None):
        ReadWriteDB.__init__(self)
        if spark_session:
            self.spark_session = spark_session
        self.env = env or ''
        
    def delete_records_from_datalake(self):
        """Delete records from datalake by comparing Oracle source with curated data"""
        try:
            logging.info("Starting PDS Oracle delete operation")
            
            # Read Oracle source data directly using existing method
            self.read_oracle_without_partition("dbtable", "(SELECT PLCY_NBR FROM AFP_POLL.PLCY_CLOB) oracle_data")
            oracle_df = self.read_df.select(col("PLCY_NBR").alias("plcy_nbr"))
            
            logging.info(f"Oracle source records count: {oracle_df.count()}")
            
            # Read curated table data
            curated_df = self.spark_session.sql("""
                SELECT plcy_nbr 
                FROM ta_individual_datalake_{}_ida_pds_curated_db.afp_poll_plcy_clob_ro
            """.format(self.env))
            
            logging.info(f"Curated table records count: {curated_df.count()}")
            
            # Find records to delete (in curated but not in Oracle source)
            delete_records = curated_df.join(oracle_df, "plcy_nbr", "left_anti")
            delete_count = delete_records.count()
            
            logging.info(f"Records to delete count: {delete_count}")
            
            if delete_count > 0:
                # Get full records for soft delete
                full_delete_records = self.spark_session.sql("""
                    SELECT * FROM ta_individual_datalake_{}_ida_pds_curated_db.afp_poll_plcy_clob_ro
                    WHERE plcy_nbr IN ({})
                """.format(self.env, ",".join([f"'{row.plcy_nbr}'" for row in delete_records.collect()])))
                
                # Perform Hudi soft delete using operation column
                delete_records_with_op = full_delete_records.withColumn("operation", lit("D"))
                
                delete_records_with_op.write \
                    .format("hudi") \
                    .option("hoodie.table.name", "afp_poll_plcy_clob") \
                    .option("hoodie.datasource.write.recordkey.field", "plcy_nbr") \
                    .option("hoodie.datasource.write.table.type", "COPY_ON_WRITE") \
                    .option("hoodie.datasource.write.operation", "upsert") \
                    .mode("append") \
                    .save("s3://ta-individual-datalake-ida-{}-curated/pds-ida/afp_poll_plcy_clob/".format(self.env))
                
                logging.info(f"Successfully soft deleted {delete_count} records from datalake")
            else:
                logging.info("No records to delete")
                
        except Exception as e:
            logging.error(f"Delete operation failed: {e}")
            raise



pds_oreacle_delete_opertion.py
=======

from .secrets_conn import GetSecrets
from .custom_logging import CustomLogging
import logging
import boto3
import boto3.session
from botocore.config import Config
import pymssql
import psycopg2


class GetServerConn(GetSecrets):

    def __init__(self):
        # Initializing GetSecrets to Connect this script with utils.get_secrets
        GetSecrets.__init__(self)
        self.sdm_control_=''
        self.config_path_=''
        self._swap_db_ = ''
        dy_session = boto3.session.Session()
        dy_boto_config = Config(connect_timeout=120, read_timeout=120, retries={'max_attempts': 10})
        self.dynamodb_client = dy_session.resource('dynamodb', region_name='us-east-1', config=dy_boto_config)

    @property
    def swap_db(self):
        return self._swap_db_

    @swap_db.setter
    def swap_db(self, value):
        self._swap_db_ = value

    @property
    def config_path(self):
        return self.config_path_

    @config_path.setter
    def config_path(self,value):
        self.config_path_=value

    @property
    def sdm_control(self):
        return self.sdm_control_

    @sdm_control.setter
    def sdm_control(self,value):
        self.sdm_control_ = value

        GetSecrets.__init__(self)
        dy_session = boto3.session.Session()
        dy_boto_config = Config(connect_timeout=120, read_timeout=120, retries={'max_attempts': 10})
        self.dynamodb_client = dy_session.resource('dynamodb', region_name='us-east-1', config=dy_boto_config)

    # JDBC connection for Spark to Postgres
    def spark_postgres(self):
        try:
            logging.debug("spark_postgres :: Creating spark postgres prop Start", extra=CustomLogging.job.add_data())
            conn = dict()
            conn['url'] = 'jdbc:postgresql://' + self.secret_obj.POSTGRESSERVER.HOST + ':' \
                          + self.secret_obj.POSTGRESSERVER.PORT + '/' + self.secret_obj.POSTGRESSERVER.DB
            conn["driver"] = 'org.postgresql.Driver'
            conn['user'] =self.secret_obj.POSTGRESSERVER.UNAME
            conn['password'] = self.secret_obj.POSTGRESSERVER.PWD
            logging.debug("spark_postgres :: Created spark postgres prop End", extra=CustomLogging.job.add_data())
            return conn
        except Exception as establish_postgres:
            logging.error("spark_postgres :: Failed to create spark con prop"
                          + str(establish_postgres), extra=CustomLogging.job.add_data())

            raise establish_postgres

    # JDBC connection for Spark to SDM SqlServer
    def spark_sqlserver_sdm(self):
        try:

            logging.debug("spark_sqlserver_sdm :: Creating spark sdm prop", extra=CustomLogging.job.add_data())
            conn = dict()
            conn['url'] = 'jdbc:sqlserver://' + self.secret_obj.SDM_SQLSERVER.SERVERNAME + \
                          ':' + self.secret_obj.SDM_SQLSERVER.PORT + ';databaseName=' + self.secret_obj.SDM_SQLSERVER.DB
            conn["driver"] = 'com.microsoft.sqlserver.jdbc.SQLServerDriver'
            conn['user'] = self.secret_obj.SDM_SQLSERVER.UNAME
            conn['password'] = self.secret_obj.SDM_SQLSERVER.PWD
            logging.debug("spark_sqlserver_sdm :: Created spark sdm prop", extra=CustomLogging.job.add_data())
            return conn
        except Exception as establish_sql_sdm:
            logging.error("spark_sqlserver_sdm :: Failed to create spark con prop"
                          + str(establish_sql_sdm), extra=CustomLogging.job.add_data())

            raise establish_sql_sdm

    # JDBC connection for Spark to AWD SqlServer
    def spark_sqlserver_awd(self):
        try:

            logging.debug("spark_sqlserver_awd :: Creating spark awd prop", extra=CustomLogging.job.add_data())
            conn = dict()
            conn['url'] = 'jdbc:sqlserver://' + self.secret_obj.AWD_RDS_SQLSERVER.SERVERNAME + \
                          ':' + self.secret_obj.AWD_RDS_SQLSERVER.PORT + ';databaseName=' + self.secret_obj.AWD_RDS_SQLSERVER.DB
            conn["driver"] = 'com.microsoft.sqlserver.jdbc.SQLServerDriver'
            conn['user'] = self.secret_obj.AWD_RDS_SQLSERVER.UNAME
            conn['password'] = self.secret_obj.AWD_RDS_SQLSERVER.PWD
            logging.debug("spark_sqlserver_awd :: Created spark awd prop", extra=CustomLogging.job.add_data())
            return conn
        except Exception as establish_sql_sdm:
            logging.error("spark_sqlserver_awd :: Failed to create spark con prop"
                          + str(establish_sql_sdm), extra=CustomLogging.job.add_data())

            raise establish_sql_sdm

    # JDBC connection for Spark to AWD View SqlServer
    def spark_sqlserver_awd_vw(self):
        try:

            logging.debug("spark_sqlserver_awd_vw :: Creating spark awd prop", extra=CustomLogging.job.add_data())
            conn = dict()
            conn['url'] = 'jdbc:sqlserver://' + self.secret_obj.AWD_SQLSERVER.SERVERNAME + \
                          ':' + self.secret_obj.AWD_SQLSERVER.PORT + ';databaseName=' + self.secret_obj.AWD_SQLSERVER.DB
            conn["driver"] = 'com.microsoft.sqlserver.jdbc.SQLServerDriver'
            conn['user'] = self.secret_obj.AWD_SQLSERVER.UNAME
            conn['password'] = self.secret_obj.AWD_SQLSERVER.PWD
            logging.debug("spark_sqlserver_awd_vw :: Created spark awd prop", extra=CustomLogging.job.add_data())
            return conn
        except Exception as establish_sql_sdm:
            logging.error("spark_sqlserver_awd :: Failed to create spark con prop"
                          + str(establish_sql_sdm), extra=CustomLogging.job.add_data())

            raise establish_sql_sdm

    def spark_sqlserver_sdm_control(self):
        try:

            logging.debug("spark_sqlserver_sdm_control :: Creating spark sdm prop", extra=CustomLogging.job.add_data())
            conn = dict()
            conn['url'] = 'jdbc:sqlserver://' + self.secret_obj.SDM_CONTROL_SQLSERVER.SERVERNAME + \
                          ':' + self.secret_obj.SDM_CONTROL_SQLSERVER.PORT + ';databaseName=' + self.secret_obj.SDM_CONTROL_SQLSERVER.DB
            conn["driver"] = 'com.microsoft.sqlserver.jdbc.SQLServerDriver'
            conn['user'] = self.secret_obj.SDM_CONTROL_SQLSERVER.UNAME
            conn['password'] = self.secret_obj.SDM_CONTROL_SQLSERVER.PWD
            logging.debug("spark_sqlserver_sdm_control :: Created spark sdm prop", extra=CustomLogging.job.add_data())
            return conn
        except Exception as establish_sql_sdm:
            logging.error("spark_sqlserver_sdm_control :: Failed to create spark con prop"
                          + str(establish_sql_sdm), extra=CustomLogging.job.add_data())

            raise establish_sql_sdm


    # JDBC connection for Spark to Infomart SqlServer
    def spark_sqlserver_infomart(self):
        try:
            logging.debug("spark_sqlserver_infomart :: Creating spark infomart prop", extra=CustomLogging.job.add_data())
            conn = dict()
            conn['url'] = "jdbc:sqlserver://" + self.secret_obj.INFOMART_SQLSERVER.SERVERNAME + ":" + self.secret_obj.INFOMART_SQLSERVER.PORT + ";database=" + self.secret_obj.INFOMART_SQLSERVER.DB
            conn["driver"] = "com.microsoft.sqlserver.jdbc.SQLServerDriver"
            conn['user'] = self.secret_obj.INFOMART_SQLSERVER.UNAME
            conn['password'] = self.secret_obj.INFOMART_SQLSERVER.PWD
            logging.debug("spark_sqlserver_infomart :: Created spark infomart prop", extra=CustomLogging.job.add_data())
            return conn
        except Exception as establish_sql_inf:
            logging.error("spark_sqlserver_infomart :: Failed to create spark con prop"
                          + str(establish_sql_inf), extra=CustomLogging.job.add_data())

            raise establish_sql_inf

    # JDBC connection for Spark to Redshift
    def spark_redshift(self):
        try:
            logging.debug("spark_redshift :: Creating spark redshift prop", extra=CustomLogging.job.add_data())
            conn = dict()
            conn['url'] = 'jdbc:redshift://' + self.secret_obj.REDSHIFTSERVER.HOST + \
                          ':' + self.secret_obj.REDSHIFTSERVER.PORT + '/' + self.secret_obj.REDSHIFTSERVER.DB + '?' \
                          'ssl=true'
            conn['user'] = self.secret_obj.REDSHIFTSERVER.UNAME
            conn['password'] = self.secret_obj.REDSHIFTSERVER.PWD
            logging.debug("spark_redshift :: Created spark redshift prop", extra=CustomLogging.job.add_data())
            return conn
        except Exception as establish_redshift:
            logging.error("spark_redshift :: Failed to create spark con prop"
                          + str(establish_redshift), extra=CustomLogging.job.add_data())

            raise establish_redshift

    # JDBC connection for Spark to Oracle
    def spark_oracle(self):
        try:
            logging.debug("spark_oracle :: Creating spark oracle prop", extra=CustomLogging.job.add_data())
            conn = dict()
            conn['url'] = 'jdbc:oracle:thin:@' + self.secret_obj.PDSSERVER.HOST + \
                          ':' + self.secret_obj.PDSSERVER.PORT + '/' + self.secret_obj.PDSSERVER.DB
            conn["driver"] = 'oracle.jdbc.driver.OracleDriver'
            conn['user'] = self.secret_obj.PDSSERVER.UNAME
            conn['password'] = self.secret_obj.PDSSERVER.PWD
            logging.debug("spark_oracle :: Created spark oracle prop", extra=CustomLogging.job.add_data())
            return conn
        except Exception as establish_oracle:
            logging.error("spark_oracle :: Failed to create spark con prop"
                          + str(establish_oracle), extra=CustomLogging.job.add_data())

            raise establish_oracle

    def spark_oracle_hzn(self):
        try:
            logging.debug("spark_oracle :: Creating spark oracle prop", extra=CustomLogging.job.add_data())
            conn = dict()
            #conn['url'] = 'jdbc:oracle:thin:@' + self.secret_obj.HZNSERVER.HOST + \
            #              ':' + self.secret_obj.HZNSERVER.PORT + '/' + self.secret_obj.HZNSERVER.DB
            conn["driver"] = 'oracle.jdbc.driver.OracleDriver'
            conn['user'] = self.secret_obj.horizon.username
            conn['password'] = self.secret_obj.horizon.password
            logging.debug("spark_oracle :: Created spark oracle prop", extra=CustomLogging.job.add_data())
            return conn
        except Exception as establish_oracle:
            logging.error("spark_oracle :: Failed to create spark con prop"
                          + str(establish_oracle), extra=CustomLogging.job.add_data())

            raise establish_oracle

    def spark_pplus_db2(self):
        try:
            logging.debug("spark :: Creating spark oracle prop", extra=CustomLogging.job.add_data())
            conn = dict()
            #conn['url'] = 'jdbc:oracle:thin:@' + self.secret_obj.HZNSERVER.HOST + \
            #              ':' + self.secret_obj.HZNSERVER.PORT + '/' + self.secret_obj.HZNSERVER.DB
            conn['user'] = self.secret_obj.pplus_db2.username
            conn['password'] = self.secret_obj.pplus_db2.password
            logging.debug("spark:: Created spark oracle prop", extra=CustomLogging.job.add_data())
            return conn
        except Exception as establish_oracle:
            logging.error("spark:: Failed to create spark con prop"
                          + str(establish_oracle), extra=CustomLogging.job.add_data())

            raise establish_oracle

    def spark_oracle_phd(self):
        try:
            logging.debug("spark_oracle :: Creating spark oracle prop", extra=CustomLogging.job.add_data())
            conn = dict()
            conn['url'] = 'jdbc:oracle:thin:@' + self.secret_obj.PDSSERVER.HOST + \
                          ':' + self.secret_obj.PDSSERVER.PORT + '/' + self.secret_obj.PDSSERVER.DB
            conn["driver"] = 'oracle.jdbc.driver.OracleDriver'
            conn['user'] = self.secret_obj.phd.username
            conn['password'] = self.secret_obj.phd.password
            logging.debug("spark_oracle :: Created spark oracle prop", extra=CustomLogging.job.add_data())
            return conn
        except Exception as establish_oracle:
            logging.error("spark_oracle :: Failed to create spark con prop"
                          + str(establish_oracle), extra=CustomLogging.job.add_data())

            raise establish_oracle

    # connection for pymssql to SDM SqlServer
    def pymssql_sqlserver_sdm(self):
        try:
            logging.debug("pymssql_sqlserver_sdm :: Creating pymssql sdm prop", extra=CustomLogging.job.add_data())
            target_conn = pymssql.connect(server=self.secret_obj.SDM_SQLSERVER.SERVERNAME,
                                          user=self.secret_obj.SDM_SQLSERVER.UNAME,
                                          password=self.secret_obj.SDM_SQLSERVER.PWD,
                                          database=self.secret_obj.SDM_SQLSERVER.DB)
            return target_conn
        except Exception as establish_sdm:
            logging.error("pymssql_sqlserver_sdm :: Failed to create pymssql con prop"
                          + str(establish_sdm), extra=CustomLogging.job.add_data())

            raise establish_sdm

    # connection for pymssql to INF SqlServer
    def pymssql_sqlserver_inf(self):
        try:
            logging.debug("pymssql_sqlserver_inf :: Creating pymssql inf prop", extra=CustomLogging.job.add_data())
            target_conn = pymssql.connect(server=self.secret_obj.INFOMART_SQLSERVER.SERVERNAME,
                                          user=self.secret_obj.INFOMART_SQLSERVER.UNAME,
                                          password=self.secret_obj.INFOMART_SQLSERVER.PWD,
                                          database=self.secret_obj.INFOMART_SQLSERVER.DB)
            return target_conn
        except Exception as establish_inf:
            logging.error("pymssql_sqlserver_inf :: Failed to create pymssql inf prop"
                          + str(establish_inf), extra=CustomLogging.job.add_data())

            raise establish_inf

    # connection for psycopg to postgres
    def psycopg_postgres(self):
        try:
            logging.debug("psycopg_postgres :: Creating postgres conn Start", extra=CustomLogging.job.add_data())
            target_conn = psycopg2.connect(database=self.secret_obj.POSTGRESSERVER.DB,
                                           host=self.secret_obj.POSTGRESSERVER.HOST,
                                           port=self.secret_obj.POSTGRESSERVER.PORT,
                                           user=self.secret_obj.POSTGRESSERVER.UNAME,
                                           password=self.secret_obj.POSTGRESSERVER.PWD)
            logging.debug("psycopg_postgres :: Created postgres conn End", extra=CustomLogging.job.add_data())
            return target_conn
        except Exception as establish_postgres:
            logging.error("psycopg_postgres :: Failed to access " + self.secret_obj.POSTGRESSERVER.HOST + " failed: " +
                          str(establish_postgres), extra=CustomLogging.job.add_data())
            raise establish_postgres

    # connection for psycopg to redshift
    def psycopg_redshift(self):
        try:
            logging.debug("psycopg_redshift :: Creating redshift conn Start", extra=CustomLogging.job.add_data())
            target_conn = psycopg2.connect(database=self.secret_obj.REDSHIFTSERVER.DB,
                                           host=self.secret_obj.REDSHIFTSERVER.HOST,
                                           port=self.secret_obj.REDSHIFTSERVER.PORT,
                                           user=self.secret_obj.REDSHIFTSERVER.UNAME,
                                           password=self.secret_obj.REDSHIFTSERVER.PWD,sslmode='require')
            logging.debug("psycopg_redshift :: Created redshift conn End", extra=CustomLogging.job.add_data())
            return target_conn
        except Exception as establish_redshift:
            logging.error("psycopg_redshift :: Failed to access " + self.secret_obj.POSTGRESSERVER.HOST + " failed: " +
                          str(establish_redshift), extra=CustomLogging.job.add_data())
            raise establish_redshift

    def control_table(self):
        try:
            logging.debug("control_table :: Connecting to dynamodb table " +
                          str(self.secret_obj.dynamo_db.control),
                          extra=CustomLogging.job.add_data())
            logging.debug("control_table :: Connected to dynamodb table " +
                          str(self.secret_obj.dynamo_db.control),
                          extra=CustomLogging.job.add_data())
            return self.dynamodb_client.Table(self.secret_obj.dynamo_db.control)
        except Exception as establish_postgres:
            logging.error("control_table :: Failed to connect to dynamo control table"
                          + str(establish_postgres), extra=CustomLogging.job.add_data())
            raise establish_postgres

    # Boto3 Connection to DynamoDB Load Status Table with respect to the current env
    def load_status_table(self):
        try:
            logging.debug("load_status_table :: Connecting to dynamodb table " +
                          str(self.secret_obj.dynamo_db.load_status), extra=CustomLogging.job.add_data())
            logging.debug("load_status_table :: Connected to dynamodb table " +
                          str(self.secret_obj.dynamo_db.load_status), extra=CustomLogging.job.add_data())
            return self.dynamodb_client.Table(self.secret_obj.dynamo_db.load_status)
        except Exception as establish_postgres:  # pragma: no cover
            logging.error("load_status_table :: Failed to connect to dynamo load status table"
                          + str(establish_postgres), extra=CustomLogging.job.add_data())
            raise establish_postgres

server_conn.py

==========

Log events 

Actions
Start tailing
Create metric filter
You can use the filter bar below to search for and match terms, phrases, or values in your log events. Learn more about filter patterns 

Clear
1m
30m
1h
12h

Custom

UTC timezone
Display

Timestamp
	
Message

Timestamp
	
Message

No older events at this moment. 
Retry
2025-10-15T17:28:39.458Z
{
    "message": "{\"level\": \"INFO\", \"@timestamp\": \"2025-10-15 12:28:39.456800\", \"message\": \"IDA Module Detected\", \"caller\": \"redshift_dynamodb.py::<module>\"}",
    "time": "2025-10-15T17:28:39+00:00"
}

{"message":"{\"level\": \"INFO\", \"@timestamp\": \"2025-10-15 12:28:39.456800\", \"message\": \"IDA Module Detected\", \"caller\": \"redshift_dynamodb.py::<module>\"}","time":"2025-10-15T17:28:39+00:00"}
2025-10-15T17:28:39.458Z
{
    "message": "{\"level\": \"INFO\", \"@timestamp\": \"2025-10-15 12:28:39.457074\", \"message\": \"Not Bestow Module\", \"caller\": \"redshift_dynamodb.py::<module>\"}",
    "time": "2025-10-15T17:28:39+00:00"
}

{"message":"{\"level\": \"INFO\", \"@timestamp\": \"2025-10-15 12:28:39.457074\", \"message\": \"Not Bestow Module\", \"caller\": \"redshift_dynamodb.py::<module>\"}","time":"2025-10-15T17:28:39+00:00"}
2025-10-15T17:28:39.458Z
{
    "message": "{\"level\": \"INFO\", \"@timestamp\": \"2025-10-15 12:28:39.457848\", \"message\": \"MRAS Module Detected\", \"caller\": \"redshift_dynamodb.py::<module>\"}",
    "time": "2025-10-15T17:28:39+00:00"
}

{"message":"{\"level\": \"INFO\", \"@timestamp\": \"2025-10-15 12:28:39.457848\", \"message\": \"MRAS Module Detected\", \"caller\": \"redshift_dynamodb.py::<module>\"}","time":"2025-10-15T17:28:39+00:00"}
2025-10-15T17:28:39.459Z
{
    "message": "{\"level\": \"INFO\", \"@timestamp\": \"2025-10-15 12:28:39.458162\", \"message\": \"Not Common Module\", \"caller\": \"redshift_dynamodb.py::<module>\"}",
    "time": "2025-10-15T17:28:39+00:00"
}

{"message":"{\"level\": \"INFO\", \"@timestamp\": \"2025-10-15 12:28:39.458162\", \"message\": \"Not Common Module\", \"caller\": \"redshift_dynamodb.py::<module>\"}","time":"2025-10-15T17:28:39+00:00"}
2025-10-15T17:28:39.459Z
{
    "message": "{\"level\": \"INFO\", \"@timestamp\": \"2025-10-15 12:28:39.458860\", \"message\": \"Agent Sync Module Detected\", \"caller\": \"redshift_dynamodb.py::<module>\"}",
    "time": "2025-10-15T17:28:39+00:00"
}

{"message":"{\"level\": \"INFO\", \"@timestamp\": \"2025-10-15 12:28:39.458860\", \"message\": \"Agent Sync Module Detected\", \"caller\": \"redshift_dynamodb.py::<module>\"}","time":"2025-10-15T17:28:39+00:00"}
2025-10-15T17:28:39.803Z
{
    "message": "{\"level\": \"INFO\", \"@timestamp\": \"2025-10-15 12:28:39.802028\", \"message\": \"init framework None\", \"caller\": \"__init__.py::init\"}",
    "time": "2025-10-15T17:28:39+00:00"
}

{"message":"{\"level\": \"INFO\", \"@timestamp\": \"2025-10-15 12:28:39.802028\", \"message\": \"init framework None\", \"caller\": \"__init__.py::init\"}","time":"2025-10-15T17:28:39+00:00"}
2025-10-15T17:28:39.804Z
{
    "message": "PDS_CUSTOM_DELETE",
    "time": "2025-10-15T17:28:39+00:00"
}

{"message":"PDS_CUSTOM_DELETE","time":"2025-10-15T17:28:39+00:00"}
2025-10-15T17:28:39.804Z
{
    "message": "DELETE",
    "time": "2025-10-15T17:28:39+00:00"
}

{"message":"DELETE","time":"2025-10-15T17:28:39+00:00"}
2025-10-15T17:28:39.804Z
{
    "message": "{\"level\": \"INFO\", \"@timestamp\": \"2025-10-15 12:28:39.803675\", \"message\": \"Entering PDS data load main\", \"caller\": \"streaming_main.py::<module>\"}",
    "time": "2025-10-15T17:28:39+00:00"
}

{"message":"{\"level\": \"INFO\", \"@timestamp\": \"2025-10-15 12:28:39.803675\", \"message\": \"Entering PDS data load main\", \"caller\": \"streaming_main.py::<module>\"}","time":"2025-10-15T17:28:39+00:00"}
2025-10-15T17:28:39.804Z
{
    "message": "Entering PDS data load main",
    "time": "2025-10-15T17:28:39+00:00"
}

{"message":"Entering PDS data load main","time":"2025-10-15T17:28:39+00:00"}
2025-10-15T17:28:39.879Z
{
    "message": "{\"level\": \"INFO\", \"@timestamp\": \"2025-10-15 12:28:39.878362\", \"message\": \"generate_spark_context :: Generating Spark Context Start\", \"caller\": \"get_spark_context.py::generate_spark_context\"}",
    "time": "2025-10-15T17:28:39+00:00"
}

{"message":"{\"level\": \"INFO\", \"@timestamp\": \"2025-10-15 12:28:39.878362\", \"message\": \"generate_spark_context :: Generating Spark Context Start\", \"caller\": \"get_spark_context.py::generate_spark_context\"}","time":"2025-10-15T17:28:39+00:00"}
2025-10-15T17:29:12.454Z
{
    "message": "{\"level\": \"INFO\", \"@timestamp\": \"2025-10-15 12:29:12.453452\", \"message\": \"generate_spark_context :: Generating Spark Context End\", \"caller\": \"get_spark_context.py::generate_spark_context\"}",
    "time": "2025-10-15T17:29:12+00:00"
}

{"message":"{\"level\": \"INFO\", \"@timestamp\": \"2025-10-15 12:29:12.453452\", \"message\": \"generate_spark_context :: Generating Spark Context End\", \"caller\": \"get_spark_context.py::generate_spark_context\"}","time":"2025-10-15T17:29:12+00:00"}
2025-10-15T17:29:12.454Z
{
    "message": "DELETE",
    "time": "2025-10-15T17:29:12+00:00"
}

{"message":"DELETE","time":"2025-10-15T17:29:12+00:00"}
2025-10-15T17:29:12.528Z
{
    "message": "{\"level\": \"INFO\", \"@timestamp\": \"2025-10-15 12:29:12.527335\", \"message\": \"generate_spark_context :: Generating Spark Context Start\", \"caller\": \"get_spark_context.py::generate_spark_context\"}",
    "time": "2025-10-15T17:29:12+00:00"
}

{"message":"{\"level\": \"INFO\", \"@timestamp\": \"2025-10-15 12:29:12.527335\", \"message\": \"generate_spark_context :: Generating Spark Context Start\", \"caller\": \"get_spark_context.py::generate_spark_context\"}","time":"2025-10-15T17:29:12+00:00"}
2025-10-15T17:29:12.645Z
{
    "message": "{\"level\": \"INFO\", \"@timestamp\": \"2025-10-15 12:29:12.644351\", \"message\": \"generate_spark_context :: Generating Spark Context End\", \"caller\": \"get_spark_context.py::generate_spark_context\"}",
    "time": "2025-10-15T17:29:12+00:00"
}

{"message":"{\"level\": \"INFO\", \"@timestamp\": \"2025-10-15 12:29:12.644351\", \"message\": \"generate_spark_context :: Generating Spark Context End\", \"caller\": \"get_spark_context.py::generate_spark_context\"}","time":"2025-10-15T17:29:12+00:00"}
2025-10-15T17:29:12.645Z
{
    "message": "{\"level\": \"INFO\", \"@timestamp\": \"2025-10-15 12:29:12.644776\", \"message\": \"generate_spark_context :: Generating Spark Context Start\", \"caller\": \"get_spark_context.py::generate_spark_context\"}",
    "time": "2025-10-15T17:29:12+00:00"
}

{"message":"{\"level\": \"INFO\", \"@timestamp\": \"2025-10-15 12:29:12.644776\", \"message\": \"generate_spark_context :: Generating Spark Context Start\", \"caller\": \"get_spark_context.py::generate_spark_context\"}","time":"2025-10-15T17:29:12+00:00"}
2025-10-15T17:29:12.749Z
{
    "message": "{\"level\": \"INFO\", \"@timestamp\": \"2025-10-15 12:29:12.746265\", \"message\": \"generate_spark_context :: Generating Spark Context End\", \"caller\": \"get_spark_context.py::generate_spark_context\"}",
    "time": "2025-10-15T17:29:12+00:00"
}

{"message":"{\"level\": \"INFO\", \"@timestamp\": \"2025-10-15 12:29:12.746265\", \"message\": \"generate_spark_context :: Generating Spark Context End\", \"caller\": \"get_spark_context.py::generate_spark_context\"}","time":"2025-10-15T17:29:12+00:00"}
2025-10-15T17:29:12.749Z
{
    "message": "{\"level\": \"INFO\", \"@timestamp\": \"2025-10-15 12:29:12.746778\", \"message\": \"pds_load_main :: Load PDS to Redshift for table   Start\", \"caller\": \"pds_dataload.py::pds_load_main\"}",
    "time": "2025-10-15T17:29:12+00:00"
}

{"message":"{\"level\": \"INFO\", \"@timestamp\": \"2025-10-15 12:29:12.746778\", \"message\": \"pds_load_main :: Load PDS to Redshift for table Start\", \"caller\": \"pds_dataload.py::pds_load_main\"}","time":"2025-10-15T17:29:12+00:00"}
2025-10-15T17:29:30.390Z
{
    "message": "{\"level\": \"INFO\", \"@timestamp\": \"2025-10-15 12:29:30.343974\", \"message\": \"Thread Started for table: REFD_CO_LU_MVIEW\", \"caller\": \"pds_dataload.py::perform\"}",
    "time": "2025-10-15T17:29:30+00:00"
}

{"message":"{\"level\": \"INFO\", \"@timestamp\": \"2025-10-15 12:29:30.343974\", \"message\": \"Thread Started for table: REFD_CO_LU_MVIEW\", \"caller\": \"pds_dataload.py::perform\"}","time":"2025-10-15T17:29:30+00:00"}
2025-10-15T17:29:30.390Z
{
    "message": "{\"level\": \"INFO\", \"@timestamp\": \"2025-10-15 12:29:30.344195\", \"message\": \"properties for table: REFD_CO_LU_MVIEW\", \"caller\": \"pds_dataload.py::perform\"}",
    "time": "2025-10-15T17:29:30+00:00"
}

{"message":"{\"level\": \"INFO\", \"@timestamp\": \"2025-10-15 12:29:30.344195\", \"message\": \"properties for table: REFD_CO_LU_MVIEW\", \"caller\": \"pds_dataload.py::perform\"}","time":"2025-10-15T17:29:30+00:00"}
2025-10-15T17:29:30.390Z
{
    "message": "{\"level\": \"INFO\", \"@timestamp\": \"2025-10-15 12:29:30.344321\", \"message\": \"Running DELTA load for table: REFD_CO_LU_MVIEW\", \"caller\": \"pds_dataload.py::perform\"}",
    "time": "2025-10-15T17:29:30+00:00"
}

{"message":"{\"level\": \"INFO\", \"@timestamp\": \"2025-10-15 12:29:30.344321\", \"message\": \"Running DELTA load for table: REFD_CO_LU_MVIEW\", \"caller\": \"pds_dataload.py::perform\"}","time":"2025-10-15T17:29:30+00:00"}
2025-10-15T17:29:30.390Z
{
    "message": "{\"level\": \"INFO\", \"@timestamp\": \"2025-10-15 12:29:30.344485\", \"message\": \"read query from hive ::select max(job_run_time) from ta_individual_datalake_dev_controls_db.job_audit where lake_table_name='AFP_REFD_REFD_CO_LU_MVIEW' and job_run_time != 'None'\", \"caller\": \"read_write_db.py::pds_oracle_tmp\"}",
    "time": "2025-10-15T17:29:30+00:00"
}

{"message":"{\"level\": \"INFO\", \"@timestamp\": \"2025-10-15 12:29:30.344485\", \"message\": \"read query from hive ::select max(job_run_time) from ta_individual_datalake_dev_controls_db.job_audit where lake_table_name='AFP_REFD_REFD_CO_LU_MVIEW' and job_run_time != 'None'\", \"caller\": \"read_write_db.py::pds_oracle_tmp\"}","time":"2025-10-15T17:29:30+00:00"}
2025-10-15T17:31:11.035Z
{
    "message": "None",
    "time": "2025-10-15T17:31:11+00:00"
}

{"message":"None","time":"2025-10-15T17:31:11+00:00"}
2025-10-15T17:31:11.035Z
{
    "message": "(select * from AFP_REFD.REFD_CO_LU_MVIEW where LST_UPDT_DT >to_timestamp('None', 'yyyy-mm-dd hh24:mi:ss.ff6')) pds",
    "time": "2025-10-15T17:31:11+00:00"
}

{"message":"(select * from AFP_REFD.REFD_CO_LU_MVIEW where LST_UPDT_DT >to_timestamp('None', 'yyyy-mm-dd hh24:mi:ss.ff6')) pds","time":"2025-10-15T17:31:11+00:00"}
2025-10-15T17:31:11.035Z
{
    "message": "{\"level\": \"INFO\", \"@timestamp\": \"2025-10-15 12:31:11.025567\", \"message\": \"read_oracle_without_partition :: Running Oracle Query without partition for table (select * from AFP_REFD.REFD_CO_LU_MVIEW where LST_UPDT_DT >to_timestamp('None', 'yyyy-mm-dd hh24:mi:ss.ff6')) pds  Start\", \"caller\": \"read_write_db.py::read_oracle_without_partition\"}",
    "time": "2025-10-15T17:31:11+00:00"
}

{"message":"{\"level\": \"INFO\", \"@timestamp\": \"2025-10-15 12:31:11.025567\", \"message\": \"read_oracle_without_partition :: Running Oracle Query without partition for table (select * from AFP_REFD.REFD_CO_LU_MVIEW where LST_UPDT_DT >to_timestamp('None', 'yyyy-mm-dd hh24:mi:ss.ff6')) pds Start\", \"caller\": \"read_write_db.py::read_oracle_without_partition\"}","time":"2025-10-15T17:31:11+00:00"}
2025-10-15T17:31:11.048Z
{
    "message": "{\"level\": \"INFO\", \"@timestamp\": \"2025-10-15 12:31:11.046568\", \"message\": \"fetch_obj :: Accessing Secrets ta-individual-apps-dev-common to Generate Secrets Obj\", \"caller\": \"secrets_conn.py::fetch_objs\"}",
    "time": "2025-10-15T17:31:11+00:00"
}

{"message":"{\"level\": \"INFO\", \"@timestamp\": \"2025-10-15 12:31:11.046568\", \"message\": \"fetch_obj :: Accessing Secrets ta-individual-apps-dev-common to Generate Secrets Obj\", \"caller\": \"secrets_conn.py::fetch_objs\"}","time":"2025-10-15T17:31:11+00:00"}
2025-10-15T17:31:12.968Z
{
    "message": "{\"level\": \"INFO\", \"@timestamp\": \"2025-10-15 12:31:12.967600\", \"message\": \"read_oracle_without_partition :: Running Oracle Query without partition for table   End\", \"caller\": \"read_write_db.py::read_oracle_without_partition\"}",
    "time": "2025-10-15T17:31:12+00:00"
}

{"message":"{\"level\": \"INFO\", \"@timestamp\": \"2025-10-15 12:31:12.967600\", \"message\": \"read_oracle_without_partition :: Running Oracle Query without partition for table End\", \"caller\": \"read_write_db.py::read_oracle_without_partition\"}","time":"2025-10-15T17:31:12+00:00"}
2025-10-15T17:31:13.274Z
{
    "message": "select count(*) from AFP_REFD.REFD_CO_LU_MVIEW where LST_UPDT_DT >to_timestamp('None', 'yyyy-mm-dd hh24:mi:ss.ff6')",
    "time": "2025-10-15T17:31:13+00:00"
}

{"message":"select count(*) from AFP_REFD.REFD_CO_LU_MVIEW where LST_UPDT_DT >to_timestamp('None', 'yyyy-mm-dd hh24:mi:ss.ff6')","time":"2025-10-15T17:31:13+00:00"}
2025-10-15T17:31:13.274Z
{
    "message": "{\"level\": \"INFO\", \"@timestamp\": \"2025-10-15 12:31:13.257160\", \"message\": \"get_count_from_oracle_pds :: Get total count from PDS oracle server for table   Start\", \"caller\": \"read_write_db.py::get_count_from_oracle_pds\"}",
    "time": "2025-10-15T17:31:13+00:00"
}

{"message":"{\"level\": \"INFO\", \"@timestamp\": \"2025-10-15 12:31:13.257160\", \"message\": \"get_count_from_oracle_pds :: Get total count from PDS oracle server for table Start\", \"caller\": \"read_write_db.py::get_count_from_oracle_pds\"}","time":"2025-10-15T17:31:13+00:00"}
2025-10-15T17:31:13.730Z
{
    "message": "{\"level\": \"ERROR\", \"@timestamp\": \"2025-10-15 12:31:13.728872\", \"message\": \"get_count_from_oracle_pds :: While Getting total count from PDS oracle server for table   : An error occurred while calling o903.load.\\n: java.sql.SQLDataException: ORA-01841: (full) year must be between -4713 and +9999, and not be 0\\n\\n\\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:494)\\n\\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:446)\\n\\tat oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:1054)\\n\\tat oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:623)\\n\\tat oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:252)\\n\\tat oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:612)\\n\\tat oracle.jdbc.driver.T4CPreparedStatement.doOall8(T4CPreparedStatement.java:226)\\n\\tat oracle.jdbc.driver.T4CPreparedStatement.doOall8(T4CPreparedStatement.java:59)\\n\\tat oracle.jdbc.driver.T4CPreparedStatement.executeForDescribe(T4CPreparedStatement.java:747)\\n\\tat oracle.jdbc.driver.OracleStatement.executeMaybeDescribe(OracleStatement.java:904)\\n\\tat oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1082)\\n\\tat oracle.jdbc.driver.OraclePreparedStatement.executeInternal(OraclePreparedStatement.java:3780)\\n\\tat oracle.jdbc.driver.T4CPreparedStatement.executeInternal(T4CPreparedStatement.java:1343)\\n\\tat oracle.jdbc.driver.OraclePreparedStatement.executeQuery(OraclePreparedStatement.java:3822)\\n\\tat oracle.jdbc.driver.OraclePreparedStatementWrapper.executeQuery(OraclePreparedStatementWrapper.java:1165)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:61)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:226)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:35)\\n\\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:354)\\n\\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:326)\\n\\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:308)\\n\\tat scala.Option.getOrElse(Option.scala:189)\\n\\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:308)\\n\\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:226)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\\n\\tat java.lang.Thread.run(Thread.java:750)\\nCaused by: Error : 1841, Position : 94, Sql = SELECT * FROM (select count(*) from AFP_REFD.REFD_CO_LU_MVIEW where LST_UPDT_DT >to_timestamp('None', 'yyyy-mm-dd hh24:mi:ss.ff6')) SPARK_GEN_SUBQ_0 WHERE 1=0, OriginalSql = SELECT * FROM (select count(*) from AFP_REFD.REFD_CO_LU_MVIEW where LST_UPDT_DT >to_timestamp('None', 'yyyy-mm-dd hh24:mi:ss.ff6')) SPARK_GEN_SUBQ_0 WHERE 1=0, Error Msg = ORA-01841: (full) year must be between -4713 and +9999, and not be 0\\n\\n\\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:498)\\n\\t... 34 more\\n\", \"caller\": \"read_write_db.py::get_count_from_oracle_pds\"}",
    "time": "2025-10-15T17:31:13+00:00"
}

{"message":"{\"level\": \"ERROR\", \"@timestamp\": \"2025-10-15 12:31:13.728872\", \"message\": \"get_count_from_oracle_pds :: While Getting total count from PDS oracle server for table : An error occurred while calling o903.load.\\n: java.sql.SQLDataException: ORA-01841: (full) year must be between -4713 and +9999, and not be 0\\n\\n\\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:494)\\n\\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:446)\\n\\tat oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:1054)\\n\\tat oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:623)\\n\\tat oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:252)\\n\\tat oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:612)\\n\\tat oracle.jdbc.driver.T4CPreparedStatement.doOall8(T4CPreparedStatement.java:226)\\n\\tat oracle.jdbc.driver.T4CPreparedStatement.doOall8(T4CPreparedStatement.java:59)\\n\\tat oracle.jdbc.driver.T4CPreparedStatement.executeForDescribe(T4CPreparedStatement.java:747)\\n\\tat oracle.jdbc.driver.OracleStatement.executeMaybeDescribe(OracleStatement.java:904)\\n\\tat oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1082)\\n\\tat oracle.jdbc.driver.OraclePreparedStatement.executeInternal(OraclePreparedStatement.java:3780)\\n\\tat oracle.jdbc.driver.T4CPreparedStatement.executeInternal(T4CPreparedStatement.java:1343)\\n\\tat oracle.jdbc.driver.OraclePreparedStatement.executeQuery(OraclePreparedStatement.java:3822)\\n\\tat oracle.jdbc.driver.OraclePreparedStatementWrapper.executeQuery(OraclePreparedStatementWrapper.java:1165)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:61)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:226)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:35)\\n\\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:354)\\n\\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:326)\\n\\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:308)\\n\\tat scala.Option.getOrElse(Option.scala:189)\\n\\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:308)\\n\\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:226)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\\n\\tat java.lang.Thread.run(Thread.java:750)\\nCaused by: Error : 1841, Position : 94, Sql = SELECT * FROM (select count(*) from AFP_REFD.REFD_CO_LU_MVIEW where LST_UPDT_DT >to_timestamp('None', 'yyyy-mm-dd hh24:mi:ss.ff6')) SPARK_GEN_SUBQ_0 WHERE 1=0, OriginalSql = SELECT * FROM (select count(*) from AFP_REFD.REFD_CO_LU_MVIEW where LST_UPDT_DT >to_timestamp('None', 'yyyy-mm-dd hh24:mi:ss.ff6')) SPARK_GEN_SUBQ_0 WHERE 1=0, Error Msg = ORA-01841: (full) year must be between -4713 and +9999, and not be 0\\n\\n\\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:498)\\n\\t... 34 more\\n\", \"caller\": \"read_write_db.py::get_count_from_oracle_pds\"}","time":"2025-10-15T17:31:13+00:00"}
2025-10-15T17:31:13.730Z
{
    "message": "{\"level\": \"ERROR\", \"@timestamp\": \"2025-10-15 12:31:13.729354\", \"message\": \"Error during DELTA\", \"caller\": \"pds_dataload.py::perform\"}",
    "time": "2025-10-15T17:31:13+00:00"
}

{"message":"{\"level\": \"ERROR\", \"@timestamp\": \"2025-10-15 12:31:13.729354\", \"message\": \"Error during DELTA\", \"caller\": \"pds_dataload.py::perform\"}","time":"2025-10-15T17:31:13+00:00"}
2025-10-15T17:31:13.730Z
{
    "message": "{\"level\": \"ERROR\", \"@timestamp\": \"2025-10-15 12:31:13.729638\", \"message\": \"error in perform function for ntable REFD_CO_LU_MVIEW\", \"caller\": \"pds_dataload.py::perform\"}",
    "time": "2025-10-15T17:31:13+00:00"
}

{"message":"{\"level\": \"ERROR\", \"@timestamp\": \"2025-10-15 12:31:13.729638\", \"message\": \"error in perform function for ntable REFD_CO_LU_MVIEW\", \"caller\": \"pds_dataload.py::perform\"}","time":"2025-10-15T17:31:13+00:00"}
2025-10-15T17:31:13.730Z
{
    "message": "{\"level\": \"ERROR\", \"@timestamp\": \"2025-10-15 12:31:13.730115\", \"message\": \"pds_load_main :: Failed to load PDS to Redshift for table : An error occurred while calling o903.load.\\n: java.sql.SQLDataException: ORA-01841: (full) year must be between -4713 and +9999, and not be 0\\n\\n\\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:494)\\n\\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:446)\\n\\tat oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:1054)\\n\\tat oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:623)\\n\\tat oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:252)\\n\\tat oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:612)\\n\\tat oracle.jdbc.driver.T4CPreparedStatement.doOall8(T4CPreparedStatement.java:226)\\n\\tat oracle.jdbc.driver.T4CPreparedStatement.doOall8(T4CPreparedStatement.java:59)\\n\\tat oracle.jdbc.driver.T4CPreparedStatement.executeForDescribe(T4CPreparedStatement.java:747)\\n\\tat oracle.jdbc.driver.OracleStatement.executeMaybeDescribe(OracleStatement.java:904)\\n\\tat oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1082)\\n\\tat oracle.jdbc.driver.OraclePreparedStatement.executeInternal(OraclePreparedStatement.java:3780)\\n\\tat oracle.jdbc.driver.T4CPreparedStatement.executeInternal(T4CPreparedStatement.java:1343)\\n\\tat oracle.jdbc.driver.OraclePreparedStatement.executeQuery(OraclePreparedStatement.java:3822)\\n\\tat oracle.jdbc.driver.OraclePreparedStatementWrapper.executeQuery(OraclePreparedStatementWrapper.java:1165)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:61)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:226)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:35)\\n\\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:354)\\n\\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:326)\\n\\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:308)\\n\\tat scala.Option.getOrElse(Option.scala:189)\\n\\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:308)\\n\\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:226)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\\n\\tat java.lang.Thread.run(Thread.java:750)\\nCaused by: Error : 1841, Position : 94, Sql = SELECT * FROM (select count(*) from AFP_REFD.REFD_CO_LU_MVIEW where LST_UPDT_DT >to_timestamp('None', 'yyyy-mm-dd hh24:mi:ss.ff6')) SPARK_GEN_SUBQ_0 WHERE 1=0, OriginalSql = SELECT * FROM (select count(*) from AFP_REFD.REFD_CO_LU_MVIEW where LST_UPDT_DT >to_timestamp('None', 'yyyy-mm-dd hh24:mi:ss.ff6')) SPARK_GEN_SUBQ_0 WHERE 1=0, Error Msg = ORA-01841: (full) year must be between -4713 and +9999, and not be 0\\n\\n\\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:498)\\n\\t... 34 more\\n\", \"caller\": \"pds_dataload.py::pds_load_main\"}",
    "time": "2025-10-15T17:31:13+00:00"
}

{"message":"{\"level\": \"ERROR\", \"@timestamp\": \"2025-10-15 12:31:13.730115\", \"message\": \"pds_load_main :: Failed to load PDS to Redshift for table : An error occurred while calling o903.load.\\n: java.sql.SQLDataException: ORA-01841: (full) year must be between -4713 and +9999, and not be 0\\n\\n\\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:494)\\n\\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:446)\\n\\tat oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:1054)\\n\\tat oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:623)\\n\\tat oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:252)\\n\\tat oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:612)\\n\\tat oracle.jdbc.driver.T4CPreparedStatement.doOall8(T4CPreparedStatement.java:226)\\n\\tat oracle.jdbc.driver.T4CPreparedStatement.doOall8(T4CPreparedStatement.java:59)\\n\\tat oracle.jdbc.driver.T4CPreparedStatement.executeForDescribe(T4CPreparedStatement.java:747)\\n\\tat oracle.jdbc.driver.OracleStatement.executeMaybeDescribe(OracleStatement.java:904)\\n\\tat oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1082)\\n\\tat oracle.jdbc.driver.OraclePreparedStatement.executeInternal(OraclePreparedStatement.java:3780)\\n\\tat oracle.jdbc.driver.T4CPreparedStatement.executeInternal(T4CPreparedStatement.java:1343)\\n\\tat oracle.jdbc.driver.OraclePreparedStatement.executeQuery(OraclePreparedStatement.java:3822)\\n\\tat oracle.jdbc.driver.OraclePreparedStatementWrapper.executeQuery(OraclePreparedStatementWrapper.java:1165)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:61)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:226)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:35)\\n\\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:354)\\n\\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:326)\\n\\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:308)\\n\\tat scala.Option.getOrElse(Option.scala:189)\\n\\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:308)\\n\\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:226)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\\n\\tat java.lang.Thread.run(Thread.java:750)\\nCaused by: Error : 1841, Position : 94, Sql = SELECT * FROM (select count(*) from AFP_REFD.REFD_CO_LU_MVIEW where LST_UPDT_DT >to_timestamp('None', 'yyyy-mm-dd hh24:mi:ss.ff6')) SPARK_GEN_SUBQ_0 WHERE 1=0, OriginalSql = SELECT * FROM (select count(*) from AFP_REFD.REFD_CO_LU_MVIEW where LST_UPDT_DT >to_timestamp('None', 'yyyy-mm-dd hh24:mi:ss.ff6')) SPARK_GEN_SUBQ_0 WHERE 1=0, Error Msg = ORA-01841: (full) year must be between -4713 and +9999, and not be 0\\n\\n\\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:498)\\n\\t... 34 more\\n\", \"caller\": \"pds_dataload.py::pds_load_main\"}","time":"2025-10-15T17:31:13+00:00"}
2025-10-15T17:31:13.730Z
{
    "message": "StreamingQueryException OCCURED... Rtrying the job",
    "time": "2025-10-15T17:31:13+00:00"
}

{"message":"StreamingQueryException OCCURED... Rtrying the job","time":"2025-10-15T17:31:13+00:00"}
2025-10-15T17:31:13.730Z
{
    "message": "{\"level\": \"INFO\", \"@timestamp\": \"2025-10-15 12:31:13.730380\", \"message\": \"StreamingQueryException OCCURED... Rtrying the job\", \"caller\": \"streaming_main.py::<module>\"}",
    "time": "2025-10-15T17:31:13+00:00"
}

{"message":"{\"level\": \"INFO\", \"@timestamp\": \"2025-10-15 12:31:13.730380\", \"message\": \"StreamingQueryException OCCURED... Rtrying the job\", \"caller\": \"streaming_main.py::<module>\"}","time":"2025-10-15T17:31:13+00:00"}
2025-10-15T17:31:13.738Z
{
    "message": "An error occurred while calling o903.load.",
    "time": "2025-10-15T17:31:13+00:00"
}

{"message":"An error occurred while calling o903.load.","time":"2025-10-15T17:31:13+00:00"}
2025-10-15T17:31:13.738Z
{
    "message": ": java.sql.SQLDataException: ORA-01841: (full) year must be between -4713 and +9999, and not be 0",
    "time": "2025-10-15T17:31:13+00:00"
}

{"message":": java.sql.SQLDataException: ORA-01841: (full) year must be between -4713 and +9999, and not be 0","time":"2025-10-15T17:31:13+00:00"}
2025-10-15T17:31:13.738Z
{
    "time": "2025-10-15T17:31:13+00:00"
}

{"time":"2025-10-15T17:31:13+00:00"}
2025-10-15T17:31:13.738Z
{
    "message": "\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:494)",
    "time": "2025-10-15T17:31:13+00:00"
}

{"message":"\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:494)","time":"2025-10-15T17:31:13+00:00"}
2025-10-15T17:31:13.738Z
{
    "message": "\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:446)",
    "time": "2025-10-15T17:31:13+00:00"
}

{"message":"\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:446)","time":"2025-10-15T17:31:13+00:00"}
2025-10-15T17:31:13.738Z
{
    "message": "\tat oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:1054)",
    "time": "2025-10-15T17:31:13+00:00"
}

{"message":"\tat oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:1054)","time":"2025-10-15T17:31:13+00:00"}
2025-10-15T17:31:13.738Z
{
    "message": "\tat oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:623)",
    "time": "2025-10-15T17:31:13+00:00"
}

{"message":"\tat oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:623)","time":"2025-10-15T17:31:13+00:00"}
2025-10-15T17:31:13.738Z
{
    "message": "\tat oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:252)",
    "time": "2025-10-15T17:31:13+00:00"
}

{"message":"\tat oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:252)","time":"2025-10-15T17:31:13+00:00"}
2025-10-15T17:31:13.738Z
{
    "message": "\tat oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:612)",
    "time": "2025-10-15T17:31:13+00:00"
}

{"message":"\tat oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:612)","time":"2025-10-15T17:31:13+00:00"}
2025-10-15T17:31:13.738Z
{
    "message": "\tat oracle.jdbc.driver.T4CPreparedStatement.doOall8(T4CPreparedStatement.java:226)",
    "time": "2025-10-15T17:31:13+00:00"
}

{"message":"\tat oracle.jdbc.driver.T4CPreparedStatement.doOall8(T4CPreparedStatement.java:226)","time":"2025-10-15T17:31:13+00:00"}
2025-10-15T17:31:13.738Z
{
    "message": "\tat oracle.jdbc.driver.T4CPreparedStatement.doOall8(T4CPreparedStatement.java:59)",
    "time": "2025-10-15T17:31:13+00:00"
}

{"message":"\tat oracle.jdbc.driver.T4CPreparedStatement.doOall8(T4CPreparedStatement.java:59)","time":"2025-10-15T17:31:13+00:00"}
2025-10-15T17:31:13.738Z
{
    "message": "\tat oracle.jdbc.driver.T4CPreparedStatement.executeForDescribe(T4CPreparedStatement.java:747)",
    "time": "2025-10-15T17:31:13+00:00"
}

{"message":"\tat oracle.jdbc.driver.T4CPreparedStatement.executeForDescribe(T4CPreparedStatement.java:747)","time":"2025-10-15T17:31:13+00:00"}
2025-10-15T17:31:13.738Z
{
    "message": "\tat oracle.jdbc.driver.OracleStatement.executeMaybeDescribe(OracleStatement.java:904)",
    "time": "2025-10-15T17:31:13+00:00"
}

{"message":"\tat oracle.jdbc.driver.OracleStatement.executeMaybeDescribe(OracleStatement.java:904)","time":"2025-10-15T17:31:13+00:00"}
2025-10-15T17:31:13.738Z
{
    "message": "\tat oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1082)",
    "time": "2025-10-15T17:31:13+00:00"
}

{"message":"\tat oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1082)","time":"2025-10-15T17:31:13+00:00"}
2025-10-15T17:31:13.738Z
{
    "message": "\tat oracle.jdbc.driver.OraclePreparedStatement.executeInternal(OraclePreparedStatement.java:3780)",
    "time": "2025-10-15T17:31:13+00:00"
}

{"message":"\tat oracle.jdbc.driver.OraclePreparedStatement.executeInternal(OraclePreparedStatement.java:3780)","time":"2025-10-15T17:31:13+00:00"}
2025-10-15T17:31:13.738Z
{
    "message": "\tat oracle.jdbc.driver.T4CPreparedStatement.executeInternal(T4CPreparedStatement.java:1343)",
    "time": "2025-10-15T17:31:13+00:00"
}

{"message":"\tat oracle.jdbc.driver.T4CPreparedStatement.executeInternal(T4CPreparedStatement.java:1343)","time":"2025-10-15T17:31:13+00:00"}
2025-10-15T17:31:13.738Z
{
    "message": "\tat oracle.jdbc.driver.OraclePreparedStatement.executeQuery(OraclePreparedStatement.java:3822)",
    "time": "2025-10-15T17:31:13+00:00"
}

{"message":"\tat oracle.jdbc.driver.OraclePreparedStatement.executeQuery(OraclePreparedStatement.java:3822)","time":"2025-10-15T17:31:13+00:00"}
2025-10-15T17:31:13.738Z
{
    "message": "\tat oracle.jdbc.driver.OraclePreparedStatementWrapper.executeQuery(OraclePreparedStatementWrapper.java:1165)",
    "time": "2025-10-15T17:31:13+00:00"
}

{"message":"\tat oracle.jdbc.driver.OraclePreparedStatementWrapper.executeQuery(OraclePreparedStatementWrapper.java:1165)","time":"2025-10-15T17:31:13+00:00"}
2025-10-15T17:31:13.738Z
{
    "message": "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:61)",
    "time": "2025-10-15T17:31:13+00:00"
}

{"message":"\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:61)","time":"2025-10-15T17:31:13+00:00"}
2025-10-15T17:31:13.738Z
{
    "message": "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:226)",
    "time": "2025-10-15T17:31:13+00:00"
}

{"message":"\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:226)","time":"2025-10-15T17:31:13+00:00"}
2025-10-15T17:31:13.738Z
{
    "message": "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:35)",
    "time": "2025-10-15T17:31:13+00:00"
}

{"message":"\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:35)","time":"2025-10-15T17:31:13+00:00"}
2025-10-15T17:31:13.738Z
{
    "message": "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:354)",
    "time": "2025-10-15T17:31:13+00:00"
}

{"message":"\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:354)","time":"2025-10-15T17:31:13+00:00"}
2025-10-15T17:31:13.738Z
{
    "message": "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:326)",
    "time": "2025-10-15T17:31:13+00:00"
}

{"message":"\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:326)","time":"2025-10-15T17:31:13+00:00"}
2025-10-15T17:31:13.738Z
{
    "message": "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:308)",
    "time": "2025-10-15T17:31:13+00:00"
}

{"message":"\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:308)","time":"2025-10-15T17:31:13+00:00"}
2025-10-15T17:31:13.738Z
{
    "message": "\tat scala.Option.getOrElse(Option.scala:189)",
    "time": "2025-10-15T17:31:13+00:00"
}

{"message":"\tat scala.Option.getOrElse(Option.scala:189)","time":"2025-10-15T17:31:13+00:00"}
2025-10-15T17:31:13.738Z
{
    "message": "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:308)",
    "time": "2025-10-15T17:31:13+00:00"
}

{"message":"\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:308)","time":"2025-10-15T17:31:13+00:00"}
2025-10-15T17:31:13.738Z
{
    "message": "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:226)",
    "time": "2025-10-15T17:31:13+00:00"
}

{"message":"\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:226)","time":"2025-10-15T17:31:13+00:00"}
2025-10-15T17:31:13.738Z
{
    "message": "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
    "time": "2025-10-15T17:31:13+00:00"
}

{"message":"\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)","time":"2025-10-15T17:31:13+00:00"}
2025-10-15T17:31:13.738Z
{
    "message": "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
    "time": "2025-10-15T17:31:13+00:00"
}

{"message":"\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)","time":"2025-10-15T17:31:13+00:00"}
2025-10-15T17:31:13.738Z
{
    "message": "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
    "time": "2025-10-15T17:31:13+00:00"
}

{"message":"\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","time":"2025-10-15T17:31:13+00:00"}
2025-10-15T17:31:13.738Z
{
    "message": "\tat java.lang.reflect.Method.invoke(Method.java:498)",
    "time": "2025-10-15T17:31:13+00:00"
}

{"message":"\tat java.lang.reflect.Method.invoke(Method.java:498)","time":"2025-10-15T17:31:13+00:00"}
2025-10-15T17:31:13.738Z
{
    "message": "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)",
    "time": "2025-10-15T17:31:13+00:00"
}

{"message":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)","time":"2025-10-15T17:31:13+00:00"}
2025-10-15T17:31:13.738Z
{
    "message": "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)",
    "time": "2025-10-15T17:31:13+00:00"
}

{"message":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)","time":"2025-10-15T17:31:13+00:00"}
2025-10-15T17:31:13.738Z
{
    "message": "\tat py4j.Gateway.invoke(Gateway.java:282)",
    "time": "2025-10-15T17:31:13+00:00"
}

{"message":"\tat py4j.Gateway.invoke(Gateway.java:282)","time":"2025-10-15T17:31:13+00:00"}
2025-10-15T17:31:13.738Z
{
    "message": "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)",
    "time": "2025-10-15T17:31:13+00:00"
}

{"message":"\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)","time":"2025-10-15T17:31:13+00:00"}
2025-10-15T17:31:13.738Z
{
    "message": "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)",
    "time": "2025-10-15T17:31:13+00:00"
}

{"message":"\tat py4j.commands.CallCommand.execute(CallCommand.java:79)","time":"2025-10-15T17:31:13+00:00"}
2025-10-15T17:31:13.738Z
{
    "message": "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)",
    "time": "2025-10-15T17:31:13+00:00"
}

{"message":"\tat py4j.GatewayConnection.run(GatewayConnection.java:238)","time":"2025-10-15T17:31:13+00:00"}
2025-10-15T17:31:13.738Z
{
    "message": "\tat java.lang.Thread.run(Thread.java:750)",
    "time": "2025-10-15T17:31:13+00:00"
}

{"message":"\tat java.lang.Thread.run(Thread.java:750)","time":"2025-10-15T17:31:13+00:00"}
2025-10-15T17:31:13.738Z
{
    "message": "Caused by: Error : 1841, Position : 94, Sql = SELECT * FROM (select count(*) from AFP_REFD.REFD_CO_LU_MVIEW where LST_UPDT_DT >to_timestamp('None', 'yyyy-mm-dd hh24:mi:ss.ff6')) SPARK_GEN_SUBQ_0 WHERE 1=0, OriginalSql = SELECT * FROM (select count(*) from AFP_REFD.REFD_CO_LU_MVIEW where LST_UPDT_DT >to_timestamp('None', 'yyyy-mm-dd hh24:mi:ss.ff6')) SPARK_GEN_SUBQ_0 WHERE 1=0, Error Msg = ORA-01841: (full) year must be between -4713 and +9999, and not be 0",
    "time": "2025-10-15T17:31:13+00:00"
}

{"message":"Caused by: Error : 1841, Position : 94, Sql = SELECT * FROM (select count(*) from AFP_REFD.REFD_CO_LU_MVIEW where LST_UPDT_DT >to_timestamp('None', 'yyyy-mm-dd hh24:mi:ss.ff6')) SPARK_GEN_SUBQ_0 WHERE 1=0, OriginalSql = SELECT * FROM (select count(*) from AFP_REFD.REFD_CO_LU_MVIEW where LST_UPDT_DT >to_timestamp('None', 'yyyy-mm-dd hh24:mi:ss.ff6')) SPARK_GEN_SUBQ_0 WHERE 1=0, Error Msg = ORA-01841: (full) year must be between -4713 and +9999, and not be 0","time":"2025-10-15T17:31:13+00:00"}
2025-10-15T17:31:13.738Z
{
    "time": "2025-10-15T17:31:13+00:00"
}

{"time":"2025-10-15T17:31:13+00:00"}
2025-10-15T17:31:13.738Z
{
    "message": "\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:498)",
    "time": "2025-10-15T17:31:13+00:00"
}

{"message":"\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:498)","time":"2025-10-15T17:31:13+00:00"}
2025-10-15T17:31:13.738Z
{
    "message": "\t... 34 more",
    "time": "2025-10-15T17:31:13+00:00"
}

{"message":"\t... 34 more","time":"2025-10-15T17:31:13+00:00"}
2025-10-15T17:31:13.738Z
{
    "time": "2025-10-15T17:31:13+00:00"
}

{"time":"2025-10-15T17:31:13+00:00"}
2025-10-15T17:31:13.738Z
{
    "message": "{\"level\": \"INFO\", \"@timestamp\": \"2025-10-15 12:31:13.732737\", \"message\": \"An error occurred while calling o903.load.\\n: java.sql.SQLDataException: ORA-01841: (full) year must be between -4713 and +9999, and not be 0\\n\\n\\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:494)\\n\\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:446)\\n\\tat oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:1054)\\n\\tat oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:623)\\n\\tat oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:252)\\n\\tat oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:612)\\n\\tat oracle.jdbc.driver.T4CPreparedStatement.doOall8(T4CPreparedStatement.java:226)\\n\\tat oracle.jdbc.driver.T4CPreparedStatement.doOall8(T4CPreparedStatement.java:59)\\n\\tat oracle.jdbc.driver.T4CPreparedStatement.executeForDescribe(T4CPreparedStatement.java:747)\\n\\tat oracle.jdbc.driver.OracleStatement.executeMaybeDescribe(OracleStatement.java:904)\\n\\tat oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1082)\\n\\tat oracle.jdbc.driver.OraclePreparedStatement.executeInternal(OraclePreparedStatement.java:3780)\\n\\tat oracle.jdbc.driver.T4CPreparedStatement.executeInternal(T4CPreparedStatement.java:1343)\\n\\tat oracle.jdbc.driver.OraclePreparedStatement.executeQuery(OraclePreparedStatement.java:3822)\\n\\tat oracle.jdbc.driver.OraclePreparedStatementWrapper.executeQuery(OraclePreparedStatementWrapper.java:1165)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:61)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:226)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:35)\\n\\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:354)\\n\\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:326)\\n\\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:308)\\n\\tat scala.Option.getOrElse(Option.scala:189)\\n\\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:308)\\n\\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:226)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\\n\\tat java.lang.Thread.run(Thread.java:750)\\nCaused by: Error : 1841, Position : 94, Sql = SELECT * FROM (select count(*) from AFP_REFD.REFD_CO_LU_MVIEW where LST_UPDT_DT >to_timestamp('None', 'yyyy-mm-dd hh24:mi:ss.ff6')) SPARK_GEN_SUBQ_0 WHERE 1=0, OriginalSql = SELECT * FROM (select count(*) from AFP_REFD.REFD_CO_LU_MVIEW where LST_UPDT_DT >to_timestamp('None', 'yyyy-mm-dd hh24:mi:ss.ff6')) SPARK_GEN_SUBQ_0 WHERE 1=0, Error Msg = ORA-01841: (full) year must be between -4713 and +9999, and not be 0\\n\\n\\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:498)\\n\\t... 34 more\\n\", \"caller\": \"streaming_main.py::<module>\"}",
    "time": "2025-10-15T17:31:13+00:00"
